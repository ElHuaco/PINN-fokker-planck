{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Hide tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'}\n",
    "# 0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs,\n",
    "#     and 3 to additionally filter out ERROR logs.\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "from pyDOE import lhs  #Latin Hypercube Sampling\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial p(x,t)}{\\partial t} =\n",
    "\\frac{\\partial}{\\partial x}\\left[\\frac{m\\omega^2}{\\gamma} x \\,p(x,t) + D \\frac{\\partial p(x,t)}{\\partial x}\\right]\n",
    "\\qquad A= \\frac{m\\omega^2}{\\gamma}= \\frac{k}{\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Definitions and Data Prep*\n",
    "\n",
    "## *Defining the domain and physical constants*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import constants\n",
    "\n",
    "Temperature = 300  # K\n",
    "viscosity = 1e-3  # Kg / (m * s)\n",
    "radius = 20e-10  # m\n",
    "gamma = 6 * np.pi * viscosity * radius  # Kg / s\n",
    "k = 3e-7  # Kg / s^2; 10GHz for frequency and 3e-26 Kg for mass\n",
    "A = k / gamma  # s^-1\n",
    "D = Temperature * constants.k / gamma  # m^2 / s\n",
    "print(f\"{gamma = } [Kg/s]\")\n",
    "print(f\"{k = } [Kg/s^2]\")\n",
    "print(f\"kB = {constants.k} [J/K]\")\n",
    "print(f\"{A = :5.4e} [s^-1],\\t{D = :4.4e} [m^2/s]\")\n",
    "# Change of units m -> µm\n",
    "#                 s -> ms\n",
    "# such that A ~ O(1), D ~ O(0.1)\n",
    "A = A * 1e-3  # s^-1 -> ms^-1\n",
    "D = D * 1e-3 * 1e+12  # m^2/s -> µm^2/ms\n",
    "print(f\"{A = :4.4e} [ms^-1],\\t{D = :4.4e} [µm^2/ms]\")\n",
    "# Collocation points for every position and every time\n",
    "x_lower = -0.1\n",
    "x_upper = 0.25\n",
    "N_x = 350\n",
    "dx = (x_upper - x_lower) / N_x\n",
    "x = np.linspace(x_lower, x_upper, N_x)  # µm line length\n",
    "t_lower = 0\n",
    "t_upper = 0.69 / A  # Typical Ornstein-Uhlenbeck time is ln(2) / A\n",
    "N_steps = 200\n",
    "t = np.linspace(t_lower, t_upper, N_steps)  # ms time interval\n",
    "X, T = np.meshgrid(x, t)\n",
    "# Max epochs in training\n",
    "maxiter = 7500\n",
    "# Total number of boundary conditions points\n",
    "N_bc = int(N_steps * 0.45)\n",
    "# Total number of initial condition points\n",
    "N_ic = int(N_x * 0.45)\n",
    "# Total number of collocation points\n",
    "N_f = int(N_steps * N_x * 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Test Data*\n",
    "We take the numerical solutions of the ```fplanck``` package as the test data to compare against the solution produced\n",
    "by our PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initial Condition from imported numerical solution\n",
    "real = pickle.load(open('./simulations/numerical_solution.sav', 'rb'))\n",
    "psol=np.zeros((len(x),len(t)))\n",
    "psol[:,0]=real[0]\n",
    "\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "# Domain bounds\n",
    "low_bound = np.array([x_lower, t_lower])\n",
    "up_bound = np.array([x_upper, t_upper])\n",
    "\n",
    "p = psol.flatten('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training and Validation Data*\n",
    "\n",
    "The boundary and initial conditions serve as the training data for the PINN. We also select the collocation points\n",
    "using *Latin Hypercube Sampling*.\n",
    "\n",
    "We choose points of the domain that were not chosen for the training. We select the validation set to have the same size\n",
    "as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_and_valid_data(n_bc, n_ic, n_f):\n",
    "    # Exception if 2N > Total domain size\n",
    "    \"\"\" Initial Condition\"\"\"\n",
    "\n",
    "    #Initial Condition -1 =< x =<1 and t = 0\n",
    "    all_ic_x = np.vstack((X[0, :], T[0, :])).T\n",
    "    all_ic_p = psol[:, 0].reshape(len(psol[:, 0]), 1)\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.vstack((X[:, 0], T[:, 0])).T\n",
    "    #bottomedge_p = psol[-1, :].reshape(len(psol[-1, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.vstack((X[:, -1], T[:, 0])).T\n",
    "    #topedge_p = psol[0, :].reshape(len(psol[0, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    all_bc_x = np.vstack([bottomedge_x, topedge_x])\n",
    "    # Reflecting conditions do not use the value of p\n",
    "    #all_bc_p_train = np.vstack([ bottomedge_p, topedge_p])\n",
    "\n",
    "    #choose random N_bc and N:ic points for training\n",
    "    index_bc = np.random.choice(all_bc_x.shape[0], 2 * n_bc, replace=False)\n",
    "    index_ic = np.random.choice(all_ic_x.shape[0], 2 * n_ic, replace=False)\n",
    "\n",
    "    _x_bc_train = all_bc_x[index_bc[:len(index_bc)//2], :]\n",
    "    _x_ic_train = all_ic_x[index_ic[:len(index_ic)//2], :]\n",
    "    _p_ic_train = all_ic_p[index_ic[:len(index_ic)//2], :]\n",
    "\n",
    "    _x_bc_valid = all_bc_x[index_bc[len(index_bc)//2 + 1:], :]\n",
    "    _x_ic_valid = all_ic_x[index_ic[len(index_ic)//2 + 1:], :]\n",
    "    _p_ic_valid = all_ic_p[index_ic[len(index_ic)//2 + 1:], :]\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points\n",
    "    # N_f sets of tuples(x,t)\n",
    "    all_f_x = low_bound + (up_bound - low_bound) * lhs(2, 2 * n_f)\n",
    "    _x_f_train = all_f_x[:len(all_f_x)//2, :]\n",
    "    _x_f_valid = all_f_x[len(all_f_x)//2 + 1:, :]\n",
    "    # Do we select boundary and initial points also for f calculation?\n",
    "    #   Only god knows\n",
    "    #x_f_train = np.vstack((x_f_train, x_bc_train, x_ic_train))\n",
    "\n",
    "    '''Normalization Instants'''\n",
    "    # N_steps is the global variable for the time steps of the domain\n",
    "    _x_norm_instant = np.vstack((X[int(N_steps / 2), :],\n",
    "                                T[int(N_steps / 2), :])).T\n",
    "\n",
    "    return _x_f_train, _x_bc_train, _x_ic_train, _p_ic_train, _x_norm_instant,\\\n",
    "           _x_f_valid, _x_bc_valid, _x_ic_valid, _p_ic_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating training and validation data\n",
    "x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instants,\\\n",
    "    x_f_valid, x_bc_valid, x_ic_valid, p_ic_valid = training_and_valid_data(N_bc, N_ic, N_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PINN implementation**\n",
    "\n",
    "## *Functions to save the training progress*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "extra_features = []\n",
    "print(\"Training with extra features:\", extra_features)\n",
    "additional_constraints = []\n",
    "print(\"Training with extra constraints:\", additional_constraints)\n",
    "\n",
    "def save_train_history(neural_network, name=None):\n",
    "    history_filename = \"./data/TRAIN_HISTORY_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            history_filename = history_filename + constrain + \"_\"\n",
    "    history_filename = history_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper:4.4f}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            history_filename = history_filename + \"_\" + feat\n",
    "    if name is not None:\n",
    "        history_filename = history_filename + \"_\" + name\n",
    "    print(\"Saving\", history_filename, \"...\")\n",
    "    neural_network.history[\"path\"] = history_filename\n",
    "    history = neural_network.get_training_history()\n",
    "    pd.DataFrame(history).to_csv(history_filename + \".csv\")\n",
    "\n",
    "\n",
    "def save_model_weights(neural_network, name=None):\n",
    "    model_filename = \"./models/MODEL_WEIGHTS_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            model_filename = model_filename + constrain + \"_\"\n",
    "    model_filename = model_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper:4.4f}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            model_filename = model_filename + \"_\" + feat\n",
    "    if name is not None:\n",
    "        model_filename = model_filename + \"_\" + name\n",
    "    print(\"Saving\", model_filename, \"...\")\n",
    "    np.savetxt(model_filename + \".txt\", neural_network.get_weights().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *PINN class definition*\n",
    "Initialization: ***Xavier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PINN(tf.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 do_print=True,\n",
    "                 _additional_constraints=(),\n",
    "                 a=10,\n",
    "                 d=0.1,\n",
    "                 f_regularization=1.0,\n",
    "                 activation_function='tanh'):\n",
    "        # The IC, BC, collocation points as well as p(IC points) and the domain upper/lower bounds are global variables,\n",
    "        #     because they define the domain of the learning problem for our model,\n",
    "        #     therefore they are required for our PINN class to be instantiated.\n",
    "        for _global_variable in ['x_f_train', 'x_bc_train', 'x_ic_train', 'p_ic_train', 'x_norm_instants', 'low_bound',\n",
    "                                'up_bound']:\n",
    "            if _global_variable not in globals():\n",
    "                raise ValueError(\"MissingGlobalVariable: \" + _global_variable)\n",
    "        super(PINN, self).__init__(name=\"PINN\")\n",
    "        self.layers = layers\n",
    "        self.additional_constraints = _additional_constraints\n",
    "        if activation_function == 'tanh':\n",
    "            self.activation_function = tf.nn.tanh\n",
    "        elif activation_function == 'swish':\n",
    "            self.activation_function = tf.nn.swish\n",
    "        else:\n",
    "            raise ValueError(\"ActivationFunction:\" + activation_function)\n",
    "        self.a = a\n",
    "        self.d = d\n",
    "        self.f_regularization = f_regularization\n",
    "        self.epoch = 0\n",
    "        self.do_print = do_print\n",
    "        self.save_training_after_n = 1000\n",
    "        self.history = {\"path\": \"\",\n",
    "                        \"epoch\": [],\n",
    "                        \"Total loss\": [],\n",
    "                        \"IC loss\": [],\n",
    "                        \"BC loss\": [],\n",
    "                        \"f loss\": [],\n",
    "                        \"Pr loss\": [],\n",
    "                        \"Norm loss\": [],\n",
    "                        \"Equi loss\": []}\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0  #total number of parameters\n",
    "        for i in range(len(layers) - 1):\n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i + 1]\n",
    "            #Xavier standard deviation\n",
    "            std_dv = np.sqrt((2.0 / (input_dim + output_dim)))\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype='float64') * std_dv\n",
    "            w = tf.Variable(w, trainable=True, name='w' + str(i + 1))\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype='float64'), trainable=True, name='b' + str(i + 1))\n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "            self.parameters += input_dim * output_dim + output_dim\n",
    "\n",
    "    def evaluate(self, subset):\n",
    "        layer_input = (subset - low_bound) / (up_bound - low_bound)  # Normalization\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            layer_output = tf.add(tf.matmul(layer_input, self.W[2 * i]), self.W[2 * i + 1])\n",
    "            layer_input = self.activation_function(layer_output)\n",
    "        layer_output = tf.add(tf.matmul(layer_input, self.W[-2]), self.W[-1])  # Regression: no activation to last layer\n",
    "        return layer_output\n",
    "\n",
    "    def get_weights(self):\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            w_1d = tf.reshape(self.W[2 * i], [-1])  #flatten weights\n",
    "            b_1d = tf.reshape(self.W[2 * i + 1], [-1])  #flatten biases\n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0)  #concat weights\n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0)  #concat biases\n",
    "        return parameters_1d\n",
    "\n",
    "    def set_weights(self, parameters):\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            shape_w = tf.shape(self.W[2 * i]).numpy()  # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2 * i]).numpy()  #size of the weight tensor\n",
    "            shape_b = tf.shape(self.W[2 * i + 1]).numpy()  # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2 * i + 1]).numpy()  #size of the bias tensor\n",
    "            pick_w = parameters[0:size_w]  #pick the weights\n",
    "            self.W[2 * i].assign(tf.reshape(pick_w, shape_w))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_w), 0)  #delete\n",
    "            pick_b = parameters[0:size_b]  #pick the biases\n",
    "            self.W[2 * i + 1].assign(tf.reshape(pick_b, shape_b))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_b), 0)  #delete\n",
    "\n",
    "    def set_training_history(self, path):\n",
    "        history = pd.read_csv(path)\n",
    "        self.history[\"epoch\"] = list(history[\"epoch\"])\n",
    "        if self.history[\"epoch\"] is not []:\n",
    "            self.epoch = self.history[\"epoch\"]\n",
    "        self.history[\"Total loss\"] = list(history[\"Total loss\"])\n",
    "        self.history[\"IC loss\"] = list(history[\"IC loss\"])\n",
    "        self.history[\"BC loss\"] = list(history[\"BC loss\"])\n",
    "        self.history[\"f loss\"] = list(history[\"f loss\"])\n",
    "        self.history[\"Pr loss\"] = list(history[\"Pr loss\"])\n",
    "        self.history[\"Norm loss\"] = list(history[\"Norm loss\"])\n",
    "        self.history[\"Equi loss\"] = list(history[\"Equi loss\"])\n",
    "\n",
    "    def get_training_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_epoch(self):\n",
    "        return self.epoch\n",
    "\n",
    "    def set_pde_params(self, a, d):\n",
    "        self.a = a\n",
    "        self.d = d\n",
    "\n",
    "    # Satisfy the IC\n",
    "    def loss_ic(self, x_ic, p_ic):\n",
    "        # Relative MSE\n",
    "        return tf.reduce_mean(tf.square(p_ic - self.evaluate(x_ic))) / tf.reduce_sum(tf.square(p_ic))\n",
    "\n",
    "    # Satisfy the reflecting boundary\n",
    "    def loss_bc(self, boundary_points):\n",
    "        variable_bc = tf.Variable(boundary_points, dtype='float64', trainable=False)\n",
    "        x_bc = variable_bc[:, 0:1]\n",
    "        t_bc = variable_bc[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_bc)\n",
    "            tape.watch(t_bc)\n",
    "            tensor_bc = tf.stack([x_bc[:, 0], t_bc[:, 0]], axis=1)\n",
    "            output_p_bc = self.evaluate(tensor_bc)\n",
    "        p_x = tape.gradient(output_p_bc, x_bc)  #more efficient out of the context\n",
    "        del tape\n",
    "        flux = -1 * (self.a * x_bc * output_p_bc + self.d * p_x)\n",
    "        return tf.reduce_mean(tf.square(flux))  # MSE_bc\n",
    "\n",
    "    # Satisfy the PDE at the collocation points\n",
    "    def loss_pde(self, collocation_points):\n",
    "        variable_collocation = tf.Variable(collocation_points, dtype='float64', trainable=False)\n",
    "        x_f = variable_collocation[:, 0:1]\n",
    "        t_f = variable_collocation[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_f)\n",
    "            tape.watch(t_f)\n",
    "            tensor_collocation = tf.stack([x_f[:, 0], t_f[:, 0]], axis=1)\n",
    "            output_p_collocation = self.evaluate(tensor_collocation)\n",
    "            p_x = tape.gradient(output_p_collocation, x_f)  #inside the context bc we need it for higher derivative\n",
    "        p_t = tape.gradient(output_p_collocation, t_f)\n",
    "        p_xx = tape.gradient(p_x, x_f)\n",
    "        del tape\n",
    "        f = p_t - self.a * output_p_collocation - self.a * x_f * p_x - self.d * p_xx\n",
    "        return tf.reduce_mean(tf.square(f))  # MSE_f\n",
    "\n",
    "    # Satisfy |probability| = 1 at some instants\n",
    "    def loss_norm(self):\n",
    "        o = self.evaluate(x_norm_instants)\n",
    "        return tf.abs(tf.reduce_sum(o) * dx - 1.0)  # ME |norm - 1|\n",
    "\n",
    "    # Must be Boltzmann distributed at t >> 1 with ß * m * w ^ 2 = A / D\n",
    "    def loss_equi(self):\n",
    "        # Typical time is ln(2) / A ≈ 0.69 / A\n",
    "        t_large = 10 * (0.69 / self.a) * np.ones(256).reshape(256, 1)\n",
    "        x_domain = np.linspace(low_bound[0], up_bound[0], 256).reshape(256, 1)\n",
    "        x_at_large_t = tf.stack([x_domain[:, 0], t_large[:, 0]], axis=1)\n",
    "        output = self.evaluate(x_at_large_t)\n",
    "        boltzmann_dist = tf.exp(-1 * (self.a / (2 * self.d)) * x_domain ** 2)\n",
    "        z = tf.reduce_sum(boltzmann_dist) * dx\n",
    "        boltzmann_dist = boltzmann_dist / z\n",
    "        # L2 norm (Boltzmann_dist - output)\n",
    "        return tf.reduce_mean(tf.square(boltzmann_dist - output) * dx)\n",
    "\n",
    "    # Satisfy p > 0 at IC and the collocation points\n",
    "    def loss_prob(self, x_ic, x_f):\n",
    "        o1 = self.evaluate(x_ic)\n",
    "        o2 = self.evaluate(x_f)\n",
    "        negatives = tf.where(tf.greater_equal(o1, 0.),\n",
    "                             tf.zeros_like(o1),\n",
    "                             o1)\n",
    "        loss_pr = tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at IC\n",
    "        negatives = tf.where(tf.greater_equal(o2, 0.),\n",
    "                             tf.zeros_like(o2),\n",
    "                             o2)\n",
    "        loss_pr = loss_pr + tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at collocation\n",
    "        return loss_pr\n",
    "\n",
    "    def loss(self, x_ic, p_ic, x_bc, x_f):\n",
    "        loss_ic = self.loss_ic(x_ic, p_ic)\n",
    "        loss_bc = self.loss_bc(x_bc)\n",
    "        loss_f = self.loss_pde(x_f)\n",
    "        loss_prob = self.loss_prob(x_ic, x_f)\n",
    "        loss_norm = self.loss_norm()\n",
    "        loss_equi = self.loss_equi()\n",
    "        loss = loss_ic + loss_bc + self.f_regularization * loss_f\n",
    "        if \"prob\" in self.additional_constraints:\n",
    "            loss = loss + loss_prob\n",
    "        if \"norm\" in self.additional_constraints:\n",
    "            loss = loss + loss_norm\n",
    "        if \"equi\" in self.additional_constraints:\n",
    "            loss = loss + loss_equi\n",
    "        return loss, loss_ic, loss_bc, loss_f, loss_prob, loss_norm, loss_equi\n",
    "\n",
    "    def optimizerfunc(self, parameters):\n",
    "        self.set_weights(parameters)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            total_loss, loss_ic, loss_bc, loss_f, loss_pr, loss_norm, loss_equi = self.loss(x_ic_train, p_ic_train,\n",
    "                                                                                            x_bc_train, x_f_train)\n",
    "            grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.epoch += 1 # This has been an iteration of the training process.\n",
    "        if self.do_print:\n",
    "            tf.print(f\"epoch: {self.epoch}\", f\"- Total: {total_loss:5.4e}\", f\"IC: {loss_ic:5.4e}\",\n",
    "                     f\"BC: {loss_bc:5.4e}\", f\"f: {loss_f:5.4e}\", f\"Norm: {loss_norm:5.4e}\", f\"equi: {loss_equi:5.4e}\")\n",
    "        del tape\n",
    "        grads_1d = []  #flatten grads\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            grads_w_1d = tf.reshape(grads[2 * i], [-1])  #flatten weights\n",
    "            grads_b_1d = tf.reshape(grads[2 * i + 1], [-1])  #flatten biases\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0)  #concat grad_weights\n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0)  #concat grad_biases\n",
    "        self.history[\"epoch\"].append(self.epoch)\n",
    "        self.history[\"Total loss\"].append(float(total_loss))\n",
    "        self.history[\"IC loss\"].append(float(loss_ic))\n",
    "        self.history[\"BC loss\"].append(float(loss_bc))\n",
    "        self.history[\"f loss\"].append(float(loss_f))\n",
    "        self.history[\"Pr loss\"].append(float(loss_pr))\n",
    "        self.history[\"Norm loss\"].append(float(loss_norm))\n",
    "        self.history[\"Equi loss\"].append(float(loss_equi))\n",
    "        if self.epoch % self.save_training_after_n == 0:\n",
    "            save_train_history(self)\n",
    "            save_model_weights(self)\n",
    "        return total_loss.numpy(), grads_1d.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Grid Search with TensordBoard*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "for global_variable in ['x_f_valid', 'x_bc_valid', 'x_ic_valid', 'p_ic_valid', 'maxiter', 'A', 'D']:\n",
    "    if global_variable not in globals():\n",
    "        raise ValueError(\"MissingGlobalVariable: \" + global_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *Hyperparameters to tune*\n",
    "- Hidden Layers: $(20, 20, ...)$ and $(80, 70, ..., 10)$\n",
    "- Activation function: $\\tanh x$ or $x \\text{S}(x)$\n",
    "- PDE loss regularization: $(10^k)_{k=-3}^{1}$\n",
    "- Type of training: standard training or transfer training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HP_ARCHI_TYPE = hp.HParam('Architecture Type', hp.Discrete(['uniform', 'decreasing']))\n",
    "HP_NUM_LAYERS = hp.HParam('Number of Layers', hp.Discrete([2, 4, 6, 8]))\n",
    "HP_ACTIV_FUNC = hp.HParam('Activation', hp.Discrete(['tanh', 'swish']))\n",
    "HP_REGU_PDE_L = hp.HParam('MSE(f) Regularization', hp.Discrete([0.001, 0.1, 1., 10.]))\n",
    "HP_LEARN_TYPE = hp.HParam('Learning Type', hp.Discrete(['standard', 'transfer']))\n",
    "METRIC = 'validation loss'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_LAYERS, HP_ARCHI_TYPE, HP_ACTIV_FUNC, HP_REGU_PDE_L, HP_LEARN_TYPE],\n",
    "    metrics=[hp.Metric(METRIC, display_name='1/Validation Loss')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model training for a Hyperparameter choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def standard_train(hyperparams):\n",
    "    # Defining the PINN for the Hyperparameter choice\n",
    "    our_layers = np.array([2])\n",
    "    if hyperparams[HP_ARCHI_TYPE] == 'uniform':\n",
    "        our_layers = np.append(our_layers, np.repeat(20, hyperparams[HP_NUM_LAYERS]))\n",
    "    elif hyperparams[HP_ARCHI_TYPE] == 'decreasing':\n",
    "        our_layers = np.append(our_layers, np.arange(start=10, stop=10*(hyperparams[HP_NUM_LAYERS]+1), step=10)[::-1])\n",
    "    else:\n",
    "        raise ValueError(\"ArchitectureType:\" + hyperparams[HP_ARCHI_TYPE])\n",
    "    our_layers = np.append(our_layers, np.array([1]))\n",
    "    pinn = PINN(our_layers,\n",
    "                f_regularization=hyperparams[HP_REGU_PDE_L],\n",
    "                activation_function=hyperparams[HP_ACTIV_FUNC],\n",
    "                a=A,\n",
    "                d=D,\n",
    "                _additional_constraints=additional_constraints)\n",
    "    # Training the PINN\n",
    "    init_params = pinn.get_weights().numpy()\n",
    "    results = scipy.optimize.minimize(fun=pinn.optimizerfunc,\n",
    "                                      x0=init_params,\n",
    "                                      args=(),\n",
    "                                      method='L-BFGS-B',\n",
    "                                      jac=True, #fun is assumed to return the gradient along with the objective function\n",
    "                                      options={'disp': None,\n",
    "                                               'maxcor': 200,\n",
    "                                               'ftol': 1 * np.finfo(float).eps,\n",
    "                                               'gtol': 5e-8,\n",
    "                                               'maxfun': 50000,\n",
    "                                               'maxiter': max(maxiter - pinn.get_epoch(), 0),\n",
    "                                               'iprint': -1,  #print update every 50 iterations\n",
    "                                               'maxls': 50})\n",
    "    # Measuring the validation set loss\n",
    "    pinn.set_weights(results.x)\n",
    "    validation_loss = pinn.loss(x_ic_valid, p_ic_valid, x_bc_valid, x_f_valid)[0]\n",
    "    save_model_weights(pinn, name=f\"GridSearch_VL{validation_loss:4.4e}\")\n",
    "    return validation_loss ** -1\n",
    "\n",
    "\n",
    "def transfer_train(hyperparams):\n",
    "    # Defining the PINN for the Hyperparameter choice\n",
    "    our_layers = np.array([2])\n",
    "    if hyperparams[HP_ARCHI_TYPE] == 'uniform':\n",
    "        our_layers = np.append(our_layers, np.repeat(20, hyperparams[HP_NUM_LAYERS]))\n",
    "    elif hyperparams[HP_ARCHI_TYPE] == 'decreasing':\n",
    "        our_layers = np.append(our_layers, np.arange(start=10, stop=10*(hyperparams[HP_NUM_LAYERS]+1), step=10)[::-1])\n",
    "    else:\n",
    "        raise ValueError(\"ArchitectureType:\" + hyperparams[HP_ARCHI_TYPE])\n",
    "    our_layers = np.append(our_layers, np.array([1]))\n",
    "    a = np.linspace(A/100, A, num=4)\n",
    "    d = np.linspace(D/100, D, num=4)\n",
    "    pinn = PINN(our_layers,\n",
    "                f_regularization=hyperparams[HP_REGU_PDE_L],\n",
    "                activation_function=hyperparams[HP_ACTIV_FUNC],\n",
    "                a=a[0],\n",
    "                d=d[0],\n",
    "                _additional_constraints=additional_constraints)\n",
    "    results = []\n",
    "    previous_pinn_weigths = pinn.get_weights().numpy()\n",
    "    for i in range(0, len(a)):\n",
    "        # Transfer learning -> Initialize with the weights of the previous PINN trained for smaller PDE parameters\n",
    "        pinn.set_pde_params(a[i], d[i])\n",
    "        pinn.set_weights(previous_pinn_weigths)\n",
    "        init_params = previous_pinn_weigths\n",
    "        results = scipy.optimize.minimize(fun=pinn.optimizerfunc,\n",
    "                                          x0=init_params,\n",
    "                                          args=(),\n",
    "                                          method='L-BFGS-B',\n",
    "                                          jac=True, #fun is assumed to return the gradient along with the objective function\n",
    "                                          options={'disp': None,\n",
    "                                                   'maxcor': 200,\n",
    "                                                   'ftol': 1 * np.finfo(float).eps,\n",
    "                                                   'gtol': 5e-8,\n",
    "                                                   'maxfun': 50000,\n",
    "                                                   'maxiter': max(maxiter - pinn.get_epoch(), 0),\n",
    "                                                   'iprint': -1,  #print update every 50 iterations\n",
    "                                                   'maxls': 50})\n",
    "        previous_pinn_weigths = results.x\n",
    "    # Measuring the validation set loss\n",
    "    pinn.set_weights(results.x)\n",
    "    validation_loss = pinn.loss(x_ic_valid, p_ic_valid, x_bc_valid, x_f_valid)[0]\n",
    "    save_model_weights(pinn, name=f\"GridSearch_VL{validation_loss:4.4e}\")\n",
    "    return validation_loss ** -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run(run_dir, hyperparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hyperparams)  # record the values used in this trial\n",
    "    if hyperparams[HP_LEARN_TYPE] == 'standard':\n",
    "        validation_loss = standard_train(hyperparams)\n",
    "    elif hyperparams[HP_LEARN_TYPE] == 'transfer':\n",
    "        validation_loss = transfer_train(hyperparams)\n",
    "    else:\n",
    "        raise ValueError(\"LearningType:\" + hyperparams[HP_LEARN_TYPE])\n",
    "    tf.summary.scalar(METRIC, validation_loss, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## GridSearch proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for archi_type in HP_ARCHI_TYPE.domain.values:\n",
    "    for num_layers in HP_NUM_LAYERS.domain.values:\n",
    "        for activ_func in HP_ACTIV_FUNC.domain.values:\n",
    "            for regu_pde_l in HP_REGU_PDE_L.domain.values:\n",
    "                for learn_type in HP_LEARN_TYPE.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_ARCHI_TYPE: archi_type,\n",
    "                        HP_NUM_LAYERS: num_layers,\n",
    "                        HP_ACTIV_FUNC: activ_func,\n",
    "                        HP_REGU_PDE_L: regu_pde_l,\n",
    "                        HP_LEARN_TYPE: learn_type\n",
    "                    }\n",
    "                    run_name = \"run-%d\" % session_num\n",
    "                    print('--- Starting trial: %s' % run_name)\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('logs/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
