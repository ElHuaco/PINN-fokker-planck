{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Hide tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'}\n",
    "# 0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs,\n",
    "#     and 3 to additionally filter out ERROR logs.\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "from pyDOE import lhs  #Latin Hypercube Sampling\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\frac{\\partial p(x,t)}{\\partial t} =\n",
    "\\frac{\\partial}{\\partial x}\\left[\\frac{m\\omega^2}{\\gamma} x \\,p(x,t) + D \\frac{\\partial p(x,t)}{\\partial x}\\right]\n",
    "\\qquad A= \\frac{m\\omega^2}{\\gamma}= \\frac{k}{\\gamma}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 3.769911184307752e-11 [Kg/s]\n",
      "k = 3e-07 [Kg/s^2]\n",
      "kB = 1.380649e-23 [J/K]\n",
      "A = 7.9577e+03 [s^-1],\tD = 1.0987e-10 [m^2/s]\n",
      "A = 7.9577e+00 [ms^-1],\tD = 1.0987e-01 [µm^2/ms]\n"
     ]
    }
   ],
   "source": [
    "from scipy import constants\n",
    "\n",
    "Temperature = 300  # K\n",
    "viscosity = 1e-3  # Kg / (m * s)\n",
    "radius = 20e-10  # m\n",
    "gamma = 6 * np.pi * viscosity * radius  # Kg / s\n",
    "k = 3e-7  # Kg / s^2; 10GHz for frequency and 3e-26 Kg for mass\n",
    "A = k / gamma  # s^-1\n",
    "D = Temperature * constants.k / gamma  # m^2 / s\n",
    "print(f\"{gamma = } [Kg/s]\")\n",
    "print(f\"{k = } [Kg/s^2]\")\n",
    "print(f\"kB = {constants.k} [J/K]\")\n",
    "print(f\"{A = :5.4e} [s^-1],\\t{D = :4.4e} [m^2/s]\")\n",
    "# Change of units m -> µm\n",
    "#                 s -> ms\n",
    "# such that A ~ O(1), D ~ O(0.1)\n",
    "A = A * 1e-3  # s^-1 -> ms^-1\n",
    "D = D * 1e-3 * 1e+12  # m^2/s -> µm^2/ms\n",
    "print(f\"{A = :4.4e} [ms^-1],\\t{D = :4.4e} [µm^2/ms]\")\n",
    "# Collocation points for every position and every time\n",
    "x_lower = -0.1\n",
    "x_upper = 0.25\n",
    "x_steps = 350\n",
    "dx = (x_upper - x_lower) / x_steps\n",
    "x = np.linspace(x_lower, x_upper, x_steps)  # µm line length\n",
    "t_lower = 0\n",
    "t_upper = 0.69 / A  # Typical Ornstein-Uhlenbeck time is ln(2) / A\n",
    "N_steps = 200\n",
    "t = np.linspace(t_lower, t_upper, N_steps)  # ms time interval\n",
    "X, T = np.meshgrid(x, t)\n",
    "# Initial Condition from imported numerical solution\n",
    "real = pickle.load(open('./simulations/numerical_solution.sav', 'rb'))\n",
    "psol=np.zeros((len(x),len(t)))\n",
    "psol[:,0]=real[0]\n",
    "# Max epochs in training\n",
    "maxiter = 10000\n",
    "# Total number of boundary conditions points\n",
    "N_bc = int(N_steps * 0.45)\n",
    "# Total number of initial condition points\n",
    "N_ic = int(x_steps * 0.45)\n",
    "# Total number of collocation points\n",
    "N_f = int(N_steps * x_steps * 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Test Data*\n",
    "We take the numerical solutions of the ```fplanck``` package as the test data to compare against the solution produced\n",
    "by the PINN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "# Domain bounds\n",
    "low_bound = np.array([x_lower, t_lower])\n",
    "up_bound = np.array([x_upper, t_upper])\n",
    "\n",
    "p = psol.flatten('F')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Training and Validation Data*\n",
    "\n",
    "The boundary and initial conditions serve as the training data for the PINN. We also select the collocation points\n",
    "using *Latin Hypercube Sampling*.\n",
    "\n",
    "We choose points of the domain that were not chosen for the training. We select the validation set to have the same size\n",
    "as the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def training_and_valid_data(n_bc, n_ic, n_f):\n",
    "    # Exception if 2N > Total domain size\n",
    "    \"\"\" Initial Condition\"\"\"\n",
    "\n",
    "    #Initial Condition -1 =< x =<1 and t = 0\n",
    "    all_ic_x = np.vstack((X[0, :], T[0, :])).T\n",
    "    all_ic_p = psol[:, 0].reshape(len(psol[:, 0]), 1)\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.vstack((X[:, 0], T[:, 0])).T\n",
    "    #bottomedge_p = psol[-1, :].reshape(len(psol[-1, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.vstack((X[:, -1], T[:, 0])).T\n",
    "    #topedge_p = psol[0, :].reshape(len(psol[0, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    all_bc_x = np.vstack([bottomedge_x, topedge_x])\n",
    "    # Reflecting conditions do not use the value of p\n",
    "    #all_bc_p_train = np.vstack([ bottomedge_p, topedge_p])\n",
    "\n",
    "    #choose random N_bc and N:ic points for training\n",
    "    index_bc = np.random.choice(all_bc_x.shape[0], 2 * n_bc, replace=False)\n",
    "    index_ic = np.random.choice(all_ic_x.shape[0], 2 * n_ic, replace=False)\n",
    "\n",
    "    _x_bc_train = all_bc_x[index_bc[:len(index_bc)//2], :]\n",
    "    _x_ic_train = all_ic_x[index_ic[:len(index_ic)//2], :]\n",
    "    _p_ic_train = all_ic_p[index_ic[:len(index_ic)//2], :]\n",
    "\n",
    "    _x_bc_valid = all_bc_x[index_bc[len(index_bc)//2 + 1:], :]\n",
    "    _x_ic_valid = all_ic_x[index_ic[len(index_ic)//2 + 1:], :]\n",
    "    _p_ic_valid = all_ic_p[index_ic[len(index_ic)//2 + 1:], :]\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points\n",
    "    # N_f sets of tuples(x,t)\n",
    "    all_f_x = low_bound + (up_bound - low_bound) * lhs(2, 2 * n_f)\n",
    "    _x_f_train = all_f_x[:len(all_f_x)//2, :]\n",
    "    _x_f_valid = all_f_x[len(all_f_x)//2 + 1:, :]\n",
    "    # Do we select boundary and initial points also for f calculation?\n",
    "    #   Only god knows\n",
    "    #x_f_train = np.vstack((x_f_train, x_bc_train, x_ic_train))\n",
    "\n",
    "    '''Normalization Instants'''\n",
    "    # N_steps is the global variable for the time steps of the domain\n",
    "    _x_norm_instant = np.vstack((X[int(N_steps / 2), :],\n",
    "                                T[int(N_steps / 2), :])).T\n",
    "\n",
    "    return _x_f_train, _x_bc_train, _x_ic_train, _p_ic_train, _x_norm_instant,\\\n",
    "           _x_f_valid, _x_bc_valid, _x_ic_valid, _p_ic_valid\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **PINN implementation**\n",
    "\n",
    "Initialization: ***Xavier***\n",
    "\n",
    "## *Hyperparameters to tune*\n",
    "- Layers: $((20)_{i=2}^{j})_{j=2}^{6}$ and $(160, 80, 40, 20, 10)$\n",
    "- Activation function: $\\tanh x$ or $x \\text{S}(x)$\n",
    "- PDE loss regularization: $(10^k)_{k=-3}^{1}$\n",
    "- Type of training: standard training or transfer training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with extra features: ['GridSearch', 'xLower-0.1', 'xUpper0.25', 'micrometers_miliseconds', 'k3e-7', 'r1e-20']\n",
      "Training with extra constraints: []\n"
     ]
    }
   ],
   "source": [
    "extra_features = [\"GridSearch\", \"xLower-0.1\", \"xUpper0.25\", \"micrometers_miliseconds\", \"k3e-7\", \"r1e-20\"]\n",
    "print(\"Training with extra features:\", extra_features)\n",
    "additional_constraints = []\n",
    "print(\"Training with extra constraints:\", additional_constraints)\n",
    "\n",
    "def save_train_history(neural_network):\n",
    "    history_filename = \"./data/TRAIN_HISTORY_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            history_filename = history_filename + constrain + \"_\"\n",
    "    history_filename = history_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            history_filename = history_filename + \"_\" + feat\n",
    "    print(\"Saving\", history_filename, \"...\")\n",
    "    neural_network.history[\"path\"] = history_filename\n",
    "    history = neural_network.get_training_history()\n",
    "    pd.DataFrame(history).to_csv(history_filename + \".csv\")\n",
    "\n",
    "\n",
    "def save_model_weights(neural_network):\n",
    "    model_filename = \"./models/MODEL_WEIGHTS_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            model_filename = model_filename + constrain + \"_\"\n",
    "    model_filename = model_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            model_filename = model_filename + \"_\" + feat\n",
    "    print(\"Saving\", model_filename, \"...\")\n",
    "    np.savetxt(model_filename + \".txt\", neural_network.get_weights().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class PINN(tf.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 do_print=True,\n",
    "                 _additional_constraints=(),\n",
    "                 a=10,\n",
    "                 d=0.1,\n",
    "                 f_regularization=1.0,\n",
    "                 activation_function=tf.nn.tanh):\n",
    "        # The IC, BC, collocation points as well as p(IC points) and the domain upper/lower bounds are global variables,\n",
    "        #     because they define the domain of the learning problem for our model,\n",
    "        #     therefore they are required for our PINN class to be instantiated.\n",
    "        for global_variable in ['x_f_train', 'x_bc_train', 'x_ic_train', 'p_ic_train', 'x_norm_instants', 'low_bound',\n",
    "                                'up_bound']:\n",
    "            if global_variable not in globals():\n",
    "                raise ValueError(\"MissingGlobalVariable: \" + global_variable)\n",
    "        super(PINN, self).__init__(name=\"PINN\")\n",
    "        self.layers = layers\n",
    "        self.additional_constraints = _additional_constraints\n",
    "        self.activation_function = activation_function\n",
    "        self.a = a\n",
    "        self.d = d\n",
    "        self.f_regularization = f_regularization\n",
    "        self.epoch = 0\n",
    "        self.do_print = do_print\n",
    "        self.save_training_after_n = 1000\n",
    "        self.history = {\"path\": \"\",\n",
    "                        \"epoch\": [],\n",
    "                        \"Total loss\": [],\n",
    "                        \"IC loss\": [],\n",
    "                        \"BC loss\": [],\n",
    "                        \"f loss\": [],\n",
    "                        \"Pr loss\": [],\n",
    "                        \"Norm loss\": [],\n",
    "                        \"Equi loss\": []}\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0  #total number of parameters\n",
    "        for i in range(len(layers) - 1):\n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i + 1]\n",
    "            #Xavier standard deviation\n",
    "            std_dv = np.sqrt((2.0 / (input_dim + output_dim)))\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype='float64') * std_dv\n",
    "            w = tf.Variable(w, trainable=True, name='w' + str(i + 1))\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype='float64'), trainable=True, name='b' + str(i + 1))\n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "            self.parameters += input_dim * output_dim + output_dim\n",
    "\n",
    "    def evaluate(self, subset):\n",
    "        layer_input = (subset - low_bound) / (up_bound - low_bound)  # Normalization\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            layer_output = tf.add(tf.matmul(layer_input, self.W[2 * i]), self.W[2 * i + 1])\n",
    "            layer_input = self.activation_function(layer_output)\n",
    "        layer_output = tf.add(tf.matmul(layer_input, self.W[-2]), self.W[-1])  # Regression: no activation to last layer\n",
    "        return layer_output\n",
    "\n",
    "    def get_weights(self):\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            w_1d = tf.reshape(self.W[2 * i], [-1])  #flatten weights\n",
    "            b_1d = tf.reshape(self.W[2 * i + 1], [-1])  #flatten biases\n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0)  #concat weights\n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0)  #concat biases\n",
    "        return parameters_1d\n",
    "\n",
    "    def set_weights(self, parameters):\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            shape_w = tf.shape(self.W[2 * i]).numpy()  # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2 * i]).numpy()  #size of the weight tensor\n",
    "            shape_b = tf.shape(self.W[2 * i + 1]).numpy()  # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2 * i + 1]).numpy()  #size of the bias tensor\n",
    "            pick_w = parameters[0:size_w]  #pick the weights\n",
    "            self.W[2 * i].assign(tf.reshape(pick_w, shape_w))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_w), 0)  #delete\n",
    "            pick_b = parameters[0:size_b]  #pick the biases\n",
    "            self.W[2 * i + 1].assign(tf.reshape(pick_b, shape_b))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_b), 0)  #delete\n",
    "\n",
    "    def set_training_history(self, path):\n",
    "        history = pd.read_csv(path)\n",
    "        self.history[\"epoch\"] = list(history[\"epoch\"])\n",
    "        if self.history[\"epoch\"] is not []:\n",
    "            self.epoch = self.history[\"epoch\"]\n",
    "        self.history[\"Total loss\"] = list(history[\"Total loss\"])\n",
    "        self.history[\"IC loss\"] = list(history[\"IC loss\"])\n",
    "        self.history[\"BC loss\"] = list(history[\"BC loss\"])\n",
    "        self.history[\"f loss\"] = list(history[\"f loss\"])\n",
    "        self.history[\"Pr loss\"] = list(history[\"Pr loss\"])\n",
    "        self.history[\"Norm loss\"] = list(history[\"Norm loss\"])\n",
    "        self.history[\"Equi loss\"] = list(history[\"Equi loss\"])\n",
    "\n",
    "    def get_training_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_epoch(self):\n",
    "        return self.epoch\n",
    "\n",
    "    # Satisfy the IC\n",
    "    def loss_ic(self):\n",
    "        # Relative MSE\n",
    "        return tf.reduce_mean(tf.square(p_ic_train - self.evaluate(x_ic_train))) / tf.reduce_sum(tf.square(p_ic_train))\n",
    "\n",
    "    # Satisfy the reflecting boundary\n",
    "    def loss_bc(self):\n",
    "        variable_bc = tf.Variable(x_bc_train, dtype='float64', trainable=False)\n",
    "        x_bc = variable_bc[:, 0:1]\n",
    "        t_bc = variable_bc[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_bc)\n",
    "            tape.watch(t_bc)\n",
    "            tensor_bc = tf.stack([x_bc[:, 0], t_bc[:, 0]], axis=1)\n",
    "            output_p_bc = self.evaluate(tensor_bc)\n",
    "        p_x = tape.gradient(output_p_bc, x_bc)  #more efficient out of the context\n",
    "        del tape\n",
    "        flux = -1 * (self.a * x_bc * output_p_bc + self.d * p_x)\n",
    "        return tf.reduce_mean(tf.square(flux))  # MSE_bc\n",
    "\n",
    "    # Satisfy the PDE at the collocation points\n",
    "    def loss_pde(self):\n",
    "        collocation_points = x_f_train\n",
    "        variable_collocation = tf.Variable(collocation_points, dtype='float64', trainable=False)\n",
    "        x_f = variable_collocation[:, 0:1]\n",
    "        t_f = variable_collocation[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_f)\n",
    "            tape.watch(t_f)\n",
    "            tensor_collocation = tf.stack([x_f[:, 0], t_f[:, 0]], axis=1)\n",
    "            output_p_collocation = self.evaluate(tensor_collocation)\n",
    "            p_x = tape.gradient(output_p_collocation, x_f)  #inside the context bc we need it for higher derivative\n",
    "        p_t = tape.gradient(output_p_collocation, t_f)\n",
    "        p_xx = tape.gradient(p_x, x_f)\n",
    "        del tape\n",
    "        f = p_t - self.a * output_p_collocation - self.a * x_f * p_x - self.d * p_xx\n",
    "        return tf.reduce_mean(tf.square(f))  # MSE_f\n",
    "\n",
    "    # Satisfy |probability| = 1 at some instants\n",
    "    def loss_norm(self):\n",
    "        o = self.evaluate(x_norm_instants)\n",
    "        return tf.abs(tf.reduce_sum(o) * dx - 1.0)  # ME |norm - 1|\n",
    "\n",
    "    # Must be Boltzmann distributed at t >> 1 with ß * m * w ^ 2 = A / D\n",
    "    def loss_equi(self):\n",
    "        # Typical time is ln(2) / A ≈ 0.69 / A\n",
    "        t_large = 10 * (0.69 / self.a) * np.ones(256).reshape(256, 1)\n",
    "        x_domain = np.linspace(low_bound[0], up_bound[0], 256).reshape(256, 1)\n",
    "        x_at_large_t = tf.stack([x_domain[:, 0], t_large[:, 0]], axis=1)\n",
    "        output = self.evaluate(x_at_large_t)\n",
    "        boltzmann_dist = tf.exp(-1 * (self.a / (2 * self.d)) * x_domain ** 2)\n",
    "        z = tf.reduce_sum(boltzmann_dist) * dx\n",
    "        boltzmann_dist = boltzmann_dist / z\n",
    "        # L2 norm (Boltzmann_dist - output)\n",
    "        return tf.reduce_mean(tf.square(boltzmann_dist - output) * dx)\n",
    "\n",
    "    # Satisfy p > 0 at IC and the collocation points\n",
    "    def loss_prob(self):\n",
    "        o1 = self.evaluate(x_ic_train)\n",
    "        o2 = self.evaluate(x_f_train)\n",
    "        negatives = tf.where(tf.greater_equal(o1, 0.),\n",
    "                             tf.zeros_like(o1),\n",
    "                             o1)\n",
    "        loss_pr = tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at IC\n",
    "        negatives = tf.where(tf.greater_equal(o2, 0.),\n",
    "                             tf.zeros_like(o2),\n",
    "                             o2)\n",
    "        loss_pr = loss_pr + tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at collocation\n",
    "        return loss_pr\n",
    "\n",
    "    def loss(self):\n",
    "        loss_ic = self.loss_ic()\n",
    "        loss_bc = self.loss_bc()\n",
    "        loss_f = self.loss_pde()\n",
    "        loss_prob = self.loss_prob()\n",
    "        loss_norm = self.loss_norm()\n",
    "        loss_equi = self.loss_equi()\n",
    "        loss = loss_ic + loss_bc + self.f_regularization * loss_f\n",
    "        if \"prob\" in self.additional_constraints:\n",
    "            loss = loss + loss_prob\n",
    "        if \"norm\" in self.additional_constraints:\n",
    "            loss = loss + loss_norm\n",
    "        if \"equi\" in self.additional_constraints:\n",
    "            loss = loss + loss_equi\n",
    "        return loss, loss_ic, loss_bc, loss_f, loss_prob, loss_norm, loss_equi\n",
    "\n",
    "    def optimizerfunc(self, parameters):\n",
    "        self.set_weights(parameters)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            total_loss, loss_ic, loss_bc, loss_f, loss_pr, loss_norm, loss_equi = self.loss()\n",
    "            grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.epoch += 1 # This has been an iteration of the training process.\n",
    "        if self.do_print:\n",
    "            tf.print(f\"epoch: {self.epoch}\", f\"- Total: {total_loss:5.4e}\", f\"IC: {loss_ic:5.4e}\",\n",
    "                     f\"BC: {loss_bc:5.4e}\", f\"f: {loss_f:5.4e}\", f\"Norm: {loss_norm:5.4e}\", f\"equi: {loss_equi:5.4e}\")\n",
    "        del tape\n",
    "        grads_1d = []  #flatten grads\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            grads_w_1d = tf.reshape(grads[2 * i], [-1])  #flatten weights\n",
    "            grads_b_1d = tf.reshape(grads[2 * i + 1], [-1])  #flatten biases\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0)  #concat grad_weights\n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0)  #concat grad_biases\n",
    "        self.history[\"epoch\"].append(self.epoch)\n",
    "        self.history[\"Total loss\"].append(float(total_loss))\n",
    "        self.history[\"IC loss\"].append(float(loss_ic))\n",
    "        self.history[\"BC loss\"].append(float(loss_bc))\n",
    "        self.history[\"f loss\"].append(float(loss_f))\n",
    "        self.history[\"Pr loss\"].append(float(loss_pr))\n",
    "        self.history[\"Norm loss\"].append(float(loss_norm))\n",
    "        self.history[\"Equi loss\"].append(float(loss_equi))\n",
    "        if self.epoch % self.save_training_after_n == 0:\n",
    "            save_train_history(self)\n",
    "            save_model_weights(self)\n",
    "        return total_loss.numpy(), grads_1d.numpy()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *Grid Search with TensordBoard*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Hyperparameters\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training function for a Hyperparameter set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GridSearch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22292264  0.        ]\n",
      " [ 0.20587393  0.        ]\n",
      " [ 0.12464183  0.        ]\n",
      " [ 0.04340974  0.        ]\n",
      " [ 0.14971347  0.        ]\n",
      " [ 0.03338109  0.        ]\n",
      " [-0.00673352  0.        ]\n",
      " [-0.01275072  0.        ]\n",
      " [ 0.22593123  0.        ]\n",
      " [ 0.22793696  0.        ]\n",
      " [-0.02378223  0.        ]\n",
      " [-0.00873926  0.        ]\n",
      " [-0.03681948  0.        ]\n",
      " [ 0.05644699  0.        ]\n",
      " [ 0.15573066  0.        ]\n",
      " [ 0.13868195  0.        ]\n",
      " [-0.05888252  0.        ]\n",
      " [-0.09899713  0.        ]\n",
      " [ 0.17077364  0.        ]\n",
      " [ 0.02936963  0.        ]\n",
      " [ 0.03037249  0.        ]\n",
      " [ 0.21891117  0.        ]\n",
      " [ 0.13667622  0.        ]\n",
      " [ 0.20186246  0.        ]\n",
      " [-0.047851    0.        ]\n",
      " [ 0.00028653  0.        ]\n",
      " [ 0.04641834  0.        ]\n",
      " [ 0.19283668  0.        ]\n",
      " [ 0.11260745  0.        ]\n",
      " [ 0.09555874  0.        ]\n",
      " [-0.00974212  0.        ]\n",
      " [ 0.00730659  0.        ]\n",
      " [ 0.10859599  0.        ]\n",
      " [-0.09097421  0.        ]\n",
      " [ 0.0213467   0.        ]\n",
      " [-0.01375358  0.        ]\n",
      " [ 0.03438395  0.        ]\n",
      " [-0.06991404  0.        ]\n",
      " [ 0.15673352  0.        ]\n",
      " [ 0.09154728  0.        ]\n",
      " [-0.01977077  0.        ]\n",
      " [-0.07091691  0.        ]\n",
      " [-0.00071633  0.        ]\n",
      " [ 0.13767908  0.        ]\n",
      " [-0.03481375  0.        ]\n",
      " [ 0.24799427  0.        ]\n",
      " [ 0.21189112  0.        ]\n",
      " [-0.08295129  0.        ]\n",
      " [-0.0769341   0.        ]\n",
      " [-0.0247851   0.        ]\n",
      " [-0.04484241  0.        ]\n",
      " [ 0.08452722  0.        ]\n",
      " [ 0.1286533   0.        ]\n",
      " [ 0.08051576  0.        ]\n",
      " [-0.08094556  0.        ]\n",
      " [-0.02979943  0.        ]\n",
      " [ 0.06246418  0.        ]\n",
      " [-0.04183381  0.        ]\n",
      " [ 0.13968481  0.        ]\n",
      " [ 0.02034384  0.        ]\n",
      " [ 0.18882521  0.        ]\n",
      " [ 0.07048711  0.        ]\n",
      " [ 0.21590258  0.        ]\n",
      " [-0.08595989  0.        ]\n",
      " [ 0.15071633  0.        ]\n",
      " [ 0.04140401  0.        ]\n",
      " [ 0.0243553   0.        ]\n",
      " [-0.07191977  0.        ]\n",
      " [-0.0217765   0.        ]\n",
      " [ 0.06848138  0.        ]\n",
      " [ 0.13166189  0.        ]\n",
      " [-0.06891117  0.        ]\n",
      " [-0.04283668  0.        ]\n",
      " [-0.04383954  0.        ]\n",
      " [ 0.06446991  0.        ]\n",
      " [ 0.08553009  0.        ]\n",
      " [ 0.15773639  0.        ]\n",
      " [ 0.18782235  0.        ]\n",
      " [ 0.11160458  0.        ]\n",
      " [-0.08997135  0.        ]\n",
      " [-0.07893983  0.        ]\n",
      " [ 0.02636103  0.        ]\n",
      " [ 0.22893983  0.        ]\n",
      " [ 0.19083095  0.        ]\n",
      " [ 0.21790831  0.        ]\n",
      " [ 0.20988539  0.        ]\n",
      " [-0.02578797  0.        ]\n",
      " [ 0.0734957   0.        ]\n",
      " [-0.08495702  0.        ]\n",
      " [-0.06590258  0.        ]\n",
      " [ 0.00830946  0.        ]\n",
      " [ 0.16074499  0.        ]\n",
      " [-0.02879656  0.        ]\n",
      " [ 0.10558739  0.        ]\n",
      " [ 0.11561605  0.        ]\n",
      " [ 0.24197708  0.        ]\n",
      " [ 0.14269341  0.        ]\n",
      " [ 0.07249284  0.        ]\n",
      " [ 0.18381089  0.        ]\n",
      " [ 0.04040115  0.        ]\n",
      " [ 0.21489971  0.        ]\n",
      " [ 0.07951289  0.        ]\n",
      " [ 0.03839542  0.        ]\n",
      " [-0.07593123  0.        ]\n",
      " [ 0.18180516  0.        ]\n",
      " [ 0.17979943  0.        ]\n",
      " [-0.03381089  0.        ]\n",
      " [-0.04684814  0.        ]\n",
      " [ 0.18681948  0.        ]\n",
      " [ 0.10458453  0.        ]\n",
      " [ 0.01031519  0.        ]\n",
      " [ 0.13567335  0.        ]\n",
      " [ 0.15873926  0.        ]\n",
      " [ 0.01733524  0.        ]\n",
      " [ 0.08151862  0.        ]\n",
      " [ 0.05744986  0.        ]\n",
      " [ 0.19383954  0.        ]\n",
      " [-0.08896848  0.        ]\n",
      " [ 0.14068768  0.        ]\n",
      " [ 0.02335244  0.        ]\n",
      " [ 0.00530086  0.        ]\n",
      " [ 0.0965616   0.        ]\n",
      " [-0.03882521  0.        ]\n",
      " [ 0.12363897  0.        ]\n",
      " [ 0.05143266  0.        ]\n",
      " [ 0.20487106  0.        ]\n",
      " [ 0.22191977  0.        ]\n",
      " [-0.00472779  0.        ]\n",
      " [-0.08395415  0.        ]\n",
      " [ 0.17879656  0.        ]\n",
      " [ 0.1487106   0.        ]\n",
      " [ 0.03638968  0.        ]\n",
      " [-0.09398281  0.        ]\n",
      " [ 0.24097421  0.        ]\n",
      " [ 0.09255014  0.        ]\n",
      " [-0.0277937   0.        ]\n",
      " [ 0.01131805  0.        ]\n",
      " [-0.07492837  0.        ]\n",
      " [ 0.21690544  0.        ]\n",
      " [ 0.07550143  0.        ]\n",
      " [ 0.11361032  0.        ]\n",
      " [-0.01174785  0.        ]\n",
      " [ 0.03739255  0.        ]\n",
      " [ 0.06547278  0.        ]\n",
      " [ 0.10057307  0.        ]\n",
      " [-0.01575931  0.        ]\n",
      " [ 0.01633238  0.        ]\n",
      " [-0.1         0.        ]\n",
      " [-0.04885387  0.        ]\n",
      " [ 0.0534384   0.        ]\n",
      " [-0.05988539  0.        ]\n",
      " [ 0.13467049  0.        ]\n",
      " [ 0.15974212  0.        ]\n",
      " [-0.03280802  0.        ]\n",
      " [ 0.05845272  0.        ]\n",
      " [ 0.10358166  0.        ]\n",
      " [ 0.14469914  0.        ]]\n",
      "[[ 0.19484241  0.        ]\n",
      " [ 0.07449857  0.        ]\n",
      " [-0.05587393  0.        ]\n",
      " [ 0.15272206  0.        ]\n",
      " [-0.09598854  0.        ]\n",
      " [ 0.16275072  0.        ]\n",
      " [ 0.08853868  0.        ]\n",
      " [ 0.01432665  0.        ]\n",
      " [ 0.23094556  0.        ]\n",
      " [ 0.18481375  0.        ]\n",
      " [ 0.23194842  0.        ]\n",
      " [ 0.0995702   0.        ]\n",
      " [ 0.05444126  0.        ]\n",
      " [ 0.23295129  0.        ]\n",
      " [ 0.0273639   0.        ]\n",
      " [-0.03080229  0.        ]\n",
      " [-0.03782235  0.        ]\n",
      " [ 0.01934097  0.        ]\n",
      " [ 0.23696275  0.        ]\n",
      " [ 0.11962751  0.        ]\n",
      " [-0.00372493  0.        ]\n",
      " [ 0.24899713  0.        ]\n",
      " [-0.01475645  0.        ]\n",
      " [ 0.07148997  0.        ]\n",
      " [ 0.18581662  0.        ]\n",
      " [ 0.17578797  0.        ]\n",
      " [-0.07292264  0.        ]\n",
      " [-0.03982808  0.        ]\n",
      " [-0.06189112  0.        ]\n",
      " [ 0.06647564  0.        ]\n",
      " [ 0.17378223  0.        ]\n",
      " [ 0.2469914   0.        ]\n",
      " [ 0.09455587  0.        ]\n",
      " [ 0.16676218  0.        ]\n",
      " [ 0.01833811  0.        ]\n",
      " [-0.07994269  0.        ]\n",
      " [ 0.01332378  0.        ]\n",
      " [ 0.1256447   0.        ]\n",
      " [-0.06690544  0.        ]\n",
      " [ 0.09355301  0.        ]\n",
      " [ 0.11862464  0.        ]\n",
      " [ 0.23595989  0.        ]\n",
      " [ 0.11461318  0.        ]\n",
      " [ 0.16475645  0.        ]\n",
      " [ 0.04240688  0.        ]\n",
      " [ 0.10759312  0.        ]\n",
      " [-0.05186246  0.        ]\n",
      " [-0.06289398  0.        ]\n",
      " [ 0.19584527  0.        ]\n",
      " [-0.03180516  0.        ]\n",
      " [ 0.09054441  0.        ]\n",
      " [ 0.18280802  0.        ]\n",
      " [ 0.11762178  0.        ]\n",
      " [ 0.1747851   0.        ]\n",
      " [ 0.21389685  0.        ]\n",
      " [ 0.00429799  0.        ]\n",
      " [ 0.11060172  0.        ]\n",
      " [ 0.06346705  0.        ]\n",
      " [-0.05386819  0.        ]\n",
      " [ 0.10959885  0.        ]\n",
      " [ 0.14570201  0.        ]\n",
      " [ 0.04942693  0.        ]\n",
      " [-0.00272206  0.        ]\n",
      " [ 0.16375358  0.        ]\n",
      " [ 0.13266476  0.        ]\n",
      " [ 0.2239255   0.        ]\n",
      " [-0.08796562  0.        ]\n",
      " [-0.05487106  0.        ]\n",
      " [ 0.11661891  0.        ]\n",
      " [-0.06389685  0.        ]\n",
      " [ 0.0012894   0.        ]\n",
      " [ 0.07851003  0.        ]\n",
      " [ 0.08753582  0.        ]\n",
      " [ 0.16575931  0.        ]\n",
      " [ 0.1517192   0.        ]\n",
      " [ 0.03538682  0.        ]\n",
      " [ 0.20286533  0.        ]\n",
      " [ 0.14369628  0.        ]\n",
      " [ 0.04441261  0.        ]\n",
      " [-0.05787966  0.        ]\n",
      " [ 0.23896848  0.        ]\n",
      " [-0.06489971  0.        ]\n",
      " [ 0.18080229  0.        ]\n",
      " [-0.09799427  0.        ]\n",
      " [ 0.17277937  0.        ]\n",
      " [-0.08696275  0.        ]\n",
      " [ 0.08252149  0.        ]\n",
      " [ 0.24398281  0.        ]\n",
      " [ 0.04541547  0.        ]\n",
      " [ 0.08954155  0.        ]\n",
      " [ 0.02234957  0.        ]\n",
      " [ 0.03137536  0.        ]\n",
      " [ 0.12063037  0.        ]\n",
      " [-0.03581662  0.        ]\n",
      " [ 0.1025788   0.        ]\n",
      " [ 0.06045845  0.        ]\n",
      " [ 0.20687679  0.        ]\n",
      " [ 0.00630372  0.        ]\n",
      " [-0.0508596   0.        ]\n",
      " [-0.06790831  0.        ]\n",
      " [ 0.08352436  0.        ]\n",
      " [ 0.06747851  0.        ]\n",
      " [ 0.23395415  0.        ]\n",
      " [-0.01776504  0.        ]\n",
      " [ 0.08653295  0.        ]\n",
      " [ 0.24297994  0.        ]\n",
      " [ 0.21088825  0.        ]\n",
      " [ 0.10659026  0.        ]\n",
      " [-0.08194842  0.        ]\n",
      " [ 0.06146132  0.        ]\n",
      " [-0.0969914   0.        ]\n",
      " [ 0.12163324  0.        ]\n",
      " [ 0.25        0.        ]\n",
      " [ 0.20386819  0.        ]\n",
      " [-0.01676218  0.        ]\n",
      " [ 0.21991404  0.        ]\n",
      " [ 0.05945559  0.        ]\n",
      " [ 0.16876791  0.        ]\n",
      " [-0.09498567  0.        ]\n",
      " [ 0.22492837  0.        ]\n",
      " [ 0.01532951  0.        ]\n",
      " [-0.06088825  0.        ]\n",
      " [ 0.20888252  0.        ]\n",
      " [-0.00773639  0.        ]\n",
      " [-0.01074499  0.        ]\n",
      " [ 0.16977077  0.        ]\n",
      " [ 0.20787966  0.        ]\n",
      " [ 0.03939828  0.        ]\n",
      " [-0.07793696  0.        ]\n",
      " [ 0.1717765   0.        ]\n",
      " [ 0.12965616  0.        ]\n",
      " [ 0.23495702  0.        ]\n",
      " [-0.04985673  0.        ]\n",
      " [ 0.197851    0.        ]\n",
      " [ 0.04842407  0.        ]\n",
      " [ 0.14670487  0.        ]\n",
      " [ 0.10157593  0.        ]\n",
      " [ 0.2008596   0.        ]\n",
      " [ 0.19885387  0.        ]\n",
      " [ 0.16174785  0.        ]\n",
      " [ 0.15372493  0.        ]\n",
      " [ 0.03237822  0.        ]\n",
      " [ 0.12664756  0.        ]\n",
      " [ 0.09756447  0.        ]\n",
      " [ 0.1226361   0.        ]\n",
      " [ 0.14770774  0.        ]\n",
      " [ 0.1777937   0.        ]\n",
      " [ 0.0474212   0.        ]\n",
      " [-0.0739255   0.        ]\n",
      " [-0.09297994  0.        ]\n",
      " [ 0.23997135  0.        ]\n",
      " [ 0.00229226  0.        ]\n",
      " [ 0.12765043  0.        ]\n",
      " [ 0.16776504  0.        ]\n",
      " [-0.05286533  0.        ]\n",
      " [ 0.05243553  0.        ]]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_7517/2037744420.py:6: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  print(x_ic_train[x_ic_train == x_ic_valid])\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nour_layers = np.array([2, 20, 20, 20, 20, 20, 20, 20, 20, 1])  #8 hidden layers\\npinn = PINN(our_layers,\\n            a=A,\\n            d=D,\\n            _additional_constraints=additional_constraints)\\nsave_train_history(pinn)\\nsave_model_weights(pinn)\\ninit_params = pinn.get_weights().numpy()\\n'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instants,\\\n",
    "    x_f_valid, x_bc_valid, x_ic_valid, p_ic_valid = training_and_valid_data(N_bc, N_ic, N_f)\n",
    "\"\"\"\n",
    "our_layers = np.array([2, 20, 20, 20, 20, 20, 20, 20, 20, 1])  #8 hidden layers\n",
    "pinn = PINN(our_layers,\n",
    "            a=A,\n",
    "            d=D,\n",
    "            _additional_constraints=additional_constraints)\n",
    "save_train_history(pinn)\n",
    "save_model_weights(pinn)\n",
    "init_params = pinn.get_weights().numpy()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (3615800182.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_7517/3615800182.py\"\u001B[0;36m, line \u001B[0;32m23\u001B[0m\n\u001B[0;31m    \"\"\"\u001B[0m\n\u001B[0m       ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# train the model with Scipy L-BFGS optimizer\n",
    "results = scipy.optimize.minimize(fun=pinn.optimizerfunc,\n",
    "                                  x0=init_params,\n",
    "                                  args=(),\n",
    "                                  method='L-BFGS-B',\n",
    "                                  jac=True, #fun is assumed to return the gradient along with the objective function\n",
    "                                  options={'disp': None,\n",
    "                                           'maxcor': 200,\n",
    "                                           'ftol': 1 * np.finfo(float).eps,\n",
    "                                           #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                           'gtol': 5e-8,\n",
    "                                           'maxfun': 50000,\n",
    "                                           'maxiter': max(maxiter - pinn.get_epoch(), 0),\n",
    "                                           'iprint': -1,  #print update every 50 iterations\n",
    "                                           'maxls': 50})\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.2f' % elapsed)\n",
    "print(results)\n",
    "pinn.set_weights(results.x)\n",
    "save_train_history(pinn)\n",
    "save_model_weights(pinn)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}