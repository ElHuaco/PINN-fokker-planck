{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Hide tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'}\n",
    "# 0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs,\n",
    "#     and 3 to additionally filter out ERROR logs.\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "from pyDOE import lhs  #Latin Hypercube Sampling\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\frac{\\partial p(x,t)}{\\partial t} =\n",
    "\\frac{\\partial}{\\partial x}\\left[\\frac{m\\omega^2}{\\gamma} x \\,p(x,t) + D \\frac{\\partial p(x,t)}{\\partial x}\\right]\n",
    "\\qquad A= \\frac{m\\omega^2}{\\gamma}= \\frac{k}{\\gamma}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *Definitions and Data Prep*\n",
    "\n",
    "## *Defining the domain and physical constants*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 3.769911184307752e-11 [Kg/s]\n",
      "k = 3e-07 [Kg/s^2]\n",
      "kB = 1.380649e-23 [J/K]\n",
      "A = 7.9577e+03 [s^-1],\tD = 1.0987e-10 [m^2/s]\n",
      "A = 7.9577e+00 [ms^-1],\tD = 1.0987e-01 [µm^2/ms]\n"
     ]
    }
   ],
   "source": [
    "from scipy import constants\n",
    "\n",
    "Temperature = 300  # K\n",
    "viscosity = 1e-3  # Kg / (m * s)\n",
    "radius = 20e-10  # m\n",
    "gamma = 6 * np.pi * viscosity * radius  # Kg / s\n",
    "k = 3e-7  # Kg / s^2; 10GHz for frequency and 3e-26 Kg for mass\n",
    "A = k / gamma  # s^-1\n",
    "D = Temperature * constants.k / gamma  # m^2 / s\n",
    "print(f\"{gamma = } [Kg/s]\")\n",
    "print(f\"{k = } [Kg/s^2]\")\n",
    "print(f\"kB = {constants.k} [J/K]\")\n",
    "print(f\"{A = :5.4e} [s^-1],\\t{D = :4.4e} [m^2/s]\")\n",
    "# Change of units m -> µm\n",
    "#                 s -> ms\n",
    "# such that A ~ O(1), D ~ O(0.1)\n",
    "A = A * 1e-3  # s^-1 -> ms^-1\n",
    "D = D * 1e-3 * 1e+12  # m^2/s -> µm^2/ms\n",
    "print(f\"{A = :4.4e} [ms^-1],\\t{D = :4.4e} [µm^2/ms]\")\n",
    "# Collocation points for every position and every time\n",
    "x_lower = -0.1\n",
    "x_upper = 0.25\n",
    "N_x = 350\n",
    "dx = (x_upper - x_lower) / N_x\n",
    "x = np.linspace(x_lower, x_upper, N_x)  # µm line length\n",
    "t_lower = 0\n",
    "t_upper = 0.69 / A  # Typical Ornstein-Uhlenbeck time is ln(2) / A\n",
    "N_steps = 200\n",
    "t = np.linspace(t_lower, t_upper, N_steps)  # ms time interval\n",
    "X, T = np.meshgrid(x, t)\n",
    "# Max epochs in training\n",
    "maxiter = 10000\n",
    "# Total number of boundary conditions points\n",
    "N_bc = int(N_steps * 0.45)\n",
    "# Total number of initial condition points\n",
    "N_ic = int(N_x * 0.45)\n",
    "# Total number of collocation points\n",
    "N_f = int(N_steps * N_x * 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Test Data*\n",
    "We take the numerical solutions of the ```fplanck``` package as the test data to compare against the solution produced\n",
    "by our PINN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# Initial Condition from imported numerical solution\n",
    "real = pickle.load(open('./simulations/numerical_solution.sav', 'rb'))\n",
    "psol=np.zeros((len(x),len(t)))\n",
    "psol[:,0]=real[0]\n",
    "\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "# Domain bounds\n",
    "low_bound = np.array([x_lower, t_lower])\n",
    "up_bound = np.array([x_upper, t_upper])\n",
    "\n",
    "p = psol.flatten('F')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Training and Validation Data*\n",
    "\n",
    "The boundary and initial conditions serve as the training data for the PINN. We also select the collocation points\n",
    "using *Latin Hypercube Sampling*.\n",
    "\n",
    "We choose points of the domain that were not chosen for the training. We select the validation set to have the same size\n",
    "as the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def training_and_valid_data(n_bc, n_ic, n_f):\n",
    "    # Exception if 2N > Total domain size\n",
    "    \"\"\" Initial Condition\"\"\"\n",
    "\n",
    "    #Initial Condition -1 =< x =<1 and t = 0\n",
    "    all_ic_x = np.vstack((X[0, :], T[0, :])).T\n",
    "    all_ic_p = psol[:, 0].reshape(len(psol[:, 0]), 1)\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.vstack((X[:, 0], T[:, 0])).T\n",
    "    #bottomedge_p = psol[-1, :].reshape(len(psol[-1, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.vstack((X[:, -1], T[:, 0])).T\n",
    "    #topedge_p = psol[0, :].reshape(len(psol[0, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    all_bc_x = np.vstack([bottomedge_x, topedge_x])\n",
    "    # Reflecting conditions do not use the value of p\n",
    "    #all_bc_p_train = np.vstack([ bottomedge_p, topedge_p])\n",
    "\n",
    "    #choose random N_bc and N:ic points for training\n",
    "    index_bc = np.random.choice(all_bc_x.shape[0], 2 * n_bc, replace=False)\n",
    "    index_ic = np.random.choice(all_ic_x.shape[0], 2 * n_ic, replace=False)\n",
    "\n",
    "    _x_bc_train = all_bc_x[index_bc[:len(index_bc)//2], :]\n",
    "    _x_ic_train = all_ic_x[index_ic[:len(index_ic)//2], :]\n",
    "    _p_ic_train = all_ic_p[index_ic[:len(index_ic)//2], :]\n",
    "\n",
    "    _x_bc_valid = all_bc_x[index_bc[len(index_bc)//2 + 1:], :]\n",
    "    _x_ic_valid = all_ic_x[index_ic[len(index_ic)//2 + 1:], :]\n",
    "    _p_ic_valid = all_ic_p[index_ic[len(index_ic)//2 + 1:], :]\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points\n",
    "    # N_f sets of tuples(x,t)\n",
    "    all_f_x = low_bound + (up_bound - low_bound) * lhs(2, 2 * n_f)\n",
    "    _x_f_train = all_f_x[:len(all_f_x)//2, :]\n",
    "    _x_f_valid = all_f_x[len(all_f_x)//2 + 1:, :]\n",
    "    # Do we select boundary and initial points also for f calculation?\n",
    "    #   Only god knows\n",
    "    #x_f_train = np.vstack((x_f_train, x_bc_train, x_ic_train))\n",
    "\n",
    "    '''Normalization Instants'''\n",
    "    # N_steps is the global variable for the time steps of the domain\n",
    "    _x_norm_instant = np.vstack((X[int(N_steps / 2), :],\n",
    "                                T[int(N_steps / 2), :])).T\n",
    "\n",
    "    return _x_f_train, _x_bc_train, _x_ic_train, _p_ic_train, _x_norm_instant,\\\n",
    "           _x_f_valid, _x_bc_valid, _x_ic_valid, _p_ic_valid\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Instantiating training and validation data\n",
    "x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instants,\\\n",
    "    x_f_valid, x_bc_valid, x_ic_valid, p_ic_valid = training_and_valid_data(N_bc, N_ic, N_f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **PINN implementation**\n",
    "\n",
    "## *Functions to save the training progress*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with extra features: []\n",
      "Training with extra constraints: []\n"
     ]
    }
   ],
   "source": [
    "extra_features = []\n",
    "print(\"Training with extra features:\", extra_features)\n",
    "additional_constraints = []\n",
    "print(\"Training with extra constraints:\", additional_constraints)\n",
    "\n",
    "def save_train_history(neural_network, name=None):\n",
    "    history_filename = \"./data/TRAIN_HISTORY_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            history_filename = history_filename + constrain + \"_\"\n",
    "    history_filename = history_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper:4.4f}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            history_filename = history_filename + \"_\" + feat\n",
    "    if name is not None:\n",
    "        history_filename = history_filename + \"_\" + name\n",
    "    print(\"Saving\", history_filename, \"...\")\n",
    "    neural_network.history[\"path\"] = history_filename\n",
    "    history = neural_network.get_training_history()\n",
    "    pd.DataFrame(history).to_csv(history_filename + \".csv\")\n",
    "\n",
    "\n",
    "def save_model_weights(neural_network, name=None):\n",
    "    model_filename = \"./models/MODEL_WEIGHTS_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            model_filename = model_filename + constrain + \"_\"\n",
    "    model_filename = model_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper:4.4f}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            model_filename = model_filename + \"_\" + feat\n",
    "    if name is not None:\n",
    "        model_filename = model_filename + \"_\" + name\n",
    "    print(\"Saving\", model_filename, \"...\")\n",
    "    np.savetxt(model_filename + \".txt\", neural_network.get_weights().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *PINN class definition*\n",
    "Initialization: ***Xavier***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class PINN(tf.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 do_print=True,\n",
    "                 _additional_constraints=(),\n",
    "                 a=10,\n",
    "                 d=0.1,\n",
    "                 f_regularization=1.0,\n",
    "                 activation_function='tanh'):\n",
    "        # The IC, BC, collocation points as well as p(IC points) and the domain upper/lower bounds are global variables,\n",
    "        #     because they define the domain of the learning problem for our model,\n",
    "        #     therefore they are required for our PINN class to be instantiated.\n",
    "        for _global_variable in ['x_f_train', 'x_bc_train', 'x_ic_train', 'p_ic_train', 'x_norm_instants', 'low_bound',\n",
    "                                'up_bound']:\n",
    "            if _global_variable not in globals():\n",
    "                raise ValueError(\"MissingGlobalVariable: \" + _global_variable)\n",
    "        super(PINN, self).__init__(name=\"PINN\")\n",
    "        self.layers = layers\n",
    "        self.additional_constraints = _additional_constraints\n",
    "        if activation_function == 'tanh':\n",
    "            self.activation_function = tf.nn.tanh\n",
    "        elif activation_function == 'swish':\n",
    "            self.activation_function = tf.nn.swish\n",
    "        else:\n",
    "            raise ValueError(\"ActivationFunction:\" + activation_function)\n",
    "        self.a = a\n",
    "        self.d = d\n",
    "        self.f_regularization = f_regularization\n",
    "        self.epoch = 0\n",
    "        self.do_print = do_print\n",
    "        self.save_training_after_n = 1000\n",
    "        self.history = {\"path\": \"\",\n",
    "                        \"epoch\": [],\n",
    "                        \"Total loss\": [],\n",
    "                        \"IC loss\": [],\n",
    "                        \"BC loss\": [],\n",
    "                        \"f loss\": [],\n",
    "                        \"Pr loss\": [],\n",
    "                        \"Norm loss\": [],\n",
    "                        \"Equi loss\": []}\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0  #total number of parameters\n",
    "        for i in range(len(layers) - 1):\n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i + 1]\n",
    "            #Xavier standard deviation\n",
    "            std_dv = np.sqrt((2.0 / (input_dim + output_dim)))\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype='float64') * std_dv\n",
    "            w = tf.Variable(w, trainable=True, name='w' + str(i + 1))\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype='float64'), trainable=True, name='b' + str(i + 1))\n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "            self.parameters += input_dim * output_dim + output_dim\n",
    "\n",
    "    def evaluate(self, subset):\n",
    "        layer_input = (subset - low_bound) / (up_bound - low_bound)  # Normalization\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            layer_output = tf.add(tf.matmul(layer_input, self.W[2 * i]), self.W[2 * i + 1])\n",
    "            layer_input = self.activation_function(layer_output)\n",
    "        layer_output = tf.add(tf.matmul(layer_input, self.W[-2]), self.W[-1])  # Regression: no activation to last layer\n",
    "        return layer_output\n",
    "\n",
    "    def get_weights(self):\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            w_1d = tf.reshape(self.W[2 * i], [-1])  #flatten weights\n",
    "            b_1d = tf.reshape(self.W[2 * i + 1], [-1])  #flatten biases\n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0)  #concat weights\n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0)  #concat biases\n",
    "        return parameters_1d\n",
    "\n",
    "    def set_weights(self, parameters):\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            shape_w = tf.shape(self.W[2 * i]).numpy()  # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2 * i]).numpy()  #size of the weight tensor\n",
    "            shape_b = tf.shape(self.W[2 * i + 1]).numpy()  # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2 * i + 1]).numpy()  #size of the bias tensor\n",
    "            pick_w = parameters[0:size_w]  #pick the weights\n",
    "            self.W[2 * i].assign(tf.reshape(pick_w, shape_w))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_w), 0)  #delete\n",
    "            pick_b = parameters[0:size_b]  #pick the biases\n",
    "            self.W[2 * i + 1].assign(tf.reshape(pick_b, shape_b))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_b), 0)  #delete\n",
    "\n",
    "    def set_training_history(self, path):\n",
    "        history = pd.read_csv(path)\n",
    "        self.history[\"epoch\"] = list(history[\"epoch\"])\n",
    "        if self.history[\"epoch\"] is not []:\n",
    "            self.epoch = self.history[\"epoch\"]\n",
    "        self.history[\"Total loss\"] = list(history[\"Total loss\"])\n",
    "        self.history[\"IC loss\"] = list(history[\"IC loss\"])\n",
    "        self.history[\"BC loss\"] = list(history[\"BC loss\"])\n",
    "        self.history[\"f loss\"] = list(history[\"f loss\"])\n",
    "        self.history[\"Pr loss\"] = list(history[\"Pr loss\"])\n",
    "        self.history[\"Norm loss\"] = list(history[\"Norm loss\"])\n",
    "        self.history[\"Equi loss\"] = list(history[\"Equi loss\"])\n",
    "\n",
    "    def get_training_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_epoch(self):\n",
    "        return self.epoch\n",
    "\n",
    "    def set_pde_params(self, a, d):\n",
    "        self.a = a\n",
    "        self.d = d\n",
    "\n",
    "    # Satisfy the IC\n",
    "    def loss_ic(self, x_ic, p_ic):\n",
    "        # Relative MSE\n",
    "        return tf.reduce_mean(tf.square(p_ic - self.evaluate(x_ic))) / tf.reduce_sum(tf.square(p_ic))\n",
    "\n",
    "    # Satisfy the reflecting boundary\n",
    "    def loss_bc(self, boundary_points):\n",
    "        variable_bc = tf.Variable(boundary_points, dtype='float64', trainable=False)\n",
    "        x_bc = variable_bc[:, 0:1]\n",
    "        t_bc = variable_bc[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_bc)\n",
    "            tape.watch(t_bc)\n",
    "            tensor_bc = tf.stack([x_bc[:, 0], t_bc[:, 0]], axis=1)\n",
    "            output_p_bc = self.evaluate(tensor_bc)\n",
    "        p_x = tape.gradient(output_p_bc, x_bc)  #more efficient out of the context\n",
    "        del tape\n",
    "        flux = -1 * (self.a * x_bc * output_p_bc + self.d * p_x)\n",
    "        return tf.reduce_mean(tf.square(flux))  # MSE_bc\n",
    "\n",
    "    # Satisfy the PDE at the collocation points\n",
    "    def loss_pde(self, collocation_points):\n",
    "        variable_collocation = tf.Variable(collocation_points, dtype='float64', trainable=False)\n",
    "        x_f = variable_collocation[:, 0:1]\n",
    "        t_f = variable_collocation[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_f)\n",
    "            tape.watch(t_f)\n",
    "            tensor_collocation = tf.stack([x_f[:, 0], t_f[:, 0]], axis=1)\n",
    "            output_p_collocation = self.evaluate(tensor_collocation)\n",
    "            p_x = tape.gradient(output_p_collocation, x_f)  #inside the context bc we need it for higher derivative\n",
    "        p_t = tape.gradient(output_p_collocation, t_f)\n",
    "        p_xx = tape.gradient(p_x, x_f)\n",
    "        del tape\n",
    "        f = p_t - self.a * output_p_collocation - self.a * x_f * p_x - self.d * p_xx\n",
    "        return tf.reduce_mean(tf.square(f))  # MSE_f\n",
    "\n",
    "    # Satisfy |probability| = 1 at some instants\n",
    "    def loss_norm(self):\n",
    "        o = self.evaluate(x_norm_instants)\n",
    "        return tf.abs(tf.reduce_sum(o) * dx - 1.0)  # ME |norm - 1|\n",
    "\n",
    "    # Must be Boltzmann distributed at t >> 1 with ß * m * w ^ 2 = A / D\n",
    "    def loss_equi(self):\n",
    "        # Typical time is ln(2) / A ≈ 0.69 / A\n",
    "        t_large = 10 * (0.69 / self.a) * np.ones(256).reshape(256, 1)\n",
    "        x_domain = np.linspace(low_bound[0], up_bound[0], 256).reshape(256, 1)\n",
    "        x_at_large_t = tf.stack([x_domain[:, 0], t_large[:, 0]], axis=1)\n",
    "        output = self.evaluate(x_at_large_t)\n",
    "        boltzmann_dist = tf.exp(-1 * (self.a / (2 * self.d)) * x_domain ** 2)\n",
    "        z = tf.reduce_sum(boltzmann_dist) * dx\n",
    "        boltzmann_dist = boltzmann_dist / z\n",
    "        # L2 norm (Boltzmann_dist - output)\n",
    "        return tf.reduce_mean(tf.square(boltzmann_dist - output) * dx)\n",
    "\n",
    "    # Satisfy p > 0 at IC and the collocation points\n",
    "    def loss_prob(self, x_ic, x_f):\n",
    "        o1 = self.evaluate(x_ic)\n",
    "        o2 = self.evaluate(x_f)\n",
    "        negatives = tf.where(tf.greater_equal(o1, 0.),\n",
    "                             tf.zeros_like(o1),\n",
    "                             o1)\n",
    "        loss_pr = tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at IC\n",
    "        negatives = tf.where(tf.greater_equal(o2, 0.),\n",
    "                             tf.zeros_like(o2),\n",
    "                             o2)\n",
    "        loss_pr = loss_pr + tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at collocation\n",
    "        return loss_pr\n",
    "\n",
    "    def loss(self, x_ic, p_ic, x_bc, x_f):\n",
    "        loss_ic = self.loss_ic(x_ic, p_ic)\n",
    "        loss_bc = self.loss_bc(x_bc)\n",
    "        loss_f = self.loss_pde(x_f)\n",
    "        loss_prob = self.loss_prob(x_ic, x_f)\n",
    "        loss_norm = self.loss_norm()\n",
    "        loss_equi = self.loss_equi()\n",
    "        loss = loss_ic + loss_bc + self.f_regularization * loss_f\n",
    "        if \"prob\" in self.additional_constraints:\n",
    "            loss = loss + loss_prob\n",
    "        if \"norm\" in self.additional_constraints:\n",
    "            loss = loss + loss_norm\n",
    "        if \"equi\" in self.additional_constraints:\n",
    "            loss = loss + loss_equi\n",
    "        return loss, loss_ic, loss_bc, loss_f, loss_prob, loss_norm, loss_equi\n",
    "\n",
    "    def optimizerfunc(self, parameters):\n",
    "        self.set_weights(parameters)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            total_loss, loss_ic, loss_bc, loss_f, loss_pr, loss_norm, loss_equi = self.loss(x_ic_train, p_ic_train,\n",
    "                                                                                            x_bc_train, x_f_train)\n",
    "            grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.epoch += 1 # This has been an iteration of the training process.\n",
    "        if self.do_print:\n",
    "            tf.print(f\"epoch: {self.epoch}\", f\"- Total: {total_loss:5.4e}\", f\"IC: {loss_ic:5.4e}\",\n",
    "                     f\"BC: {loss_bc:5.4e}\", f\"f: {loss_f:5.4e}\", f\"Norm: {loss_norm:5.4e}\", f\"equi: {loss_equi:5.4e}\")\n",
    "        del tape\n",
    "        grads_1d = []  #flatten grads\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            grads_w_1d = tf.reshape(grads[2 * i], [-1])  #flatten weights\n",
    "            grads_b_1d = tf.reshape(grads[2 * i + 1], [-1])  #flatten biases\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0)  #concat grad_weights\n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0)  #concat grad_biases\n",
    "        self.history[\"epoch\"].append(self.epoch)\n",
    "        self.history[\"Total loss\"].append(float(total_loss))\n",
    "        self.history[\"IC loss\"].append(float(loss_ic))\n",
    "        self.history[\"BC loss\"].append(float(loss_bc))\n",
    "        self.history[\"f loss\"].append(float(loss_f))\n",
    "        self.history[\"Pr loss\"].append(float(loss_pr))\n",
    "        self.history[\"Norm loss\"].append(float(loss_norm))\n",
    "        self.history[\"Equi loss\"].append(float(loss_equi))\n",
    "        if self.epoch % self.save_training_after_n == 0:\n",
    "            save_train_history(self)\n",
    "            save_model_weights(self)\n",
    "        return total_loss.numpy(), grads_1d.numpy()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *Grid Search with TensordBoard*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "for global_variable in ['x_f_valid', 'x_bc_valid', 'x_ic_valid', 'p_ic_valid', 'maxiter', 'A', 'D']:\n",
    "    if global_variable not in globals():\n",
    "        raise ValueError(\"MissingGlobalVariable: \" + global_variable)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Hyperparameters to tune*\n",
    "- Layers: $((20)_{i=2}^{j})_{j=2}^{6}$ and $(160, 80, 40, 20, 10)$\n",
    "- Activation function: $\\tanh x$ or $x \\text{S}(x)$\n",
    "- PDE loss regularization: $(10^k)_{k=-3}^{1}$\n",
    "- Type of training: standard training or transfer training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "HP_ARCHI_TYPE = hp.HParam('Architecture Type', hp.Discrete(['uniform', 'decreasing']))\n",
    "HP_NUM_LAYERS = hp.HParam('Number of Layers', hp.Discrete([2, 4, 6, 8]))\n",
    "HP_ACTIV_FUNC = hp.HParam('Activation', hp.Discrete(['tanh', 'swish']))\n",
    "HP_REGU_PDE_L = hp.HParam('MSE(f) Regularization', hp.Discrete([0.001, 0.1, 1., 10.]))\n",
    "HP_LEARN_TYPE = hp.HParam('Learning Type', hp.Discrete(['standard', 'transfer']))\n",
    "METRIC = 'validation loss'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_LAYERS, HP_ARCHI_TYPE, HP_ACTIV_FUNC, HP_REGU_PDE_L, HP_LEARN_TYPE],\n",
    "    metrics=[hp.Metric(METRIC, display_name='1/Validation Loss')],\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training for a Hyperparameter choice"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def standard_train(hyperparams):\n",
    "    # Defining the PINN for the Hyperparameter choice\n",
    "    our_layers = np.array([2])\n",
    "    if hyperparams[HP_ARCHI_TYPE] == 'uniform':\n",
    "        our_layers = np.append(our_layers, np.repeat(20, hyperparams[HP_NUM_LAYERS]))\n",
    "    elif hyperparams[HP_ARCHI_TYPE] == 'decreasing':\n",
    "        our_layers = np.append(our_layers, np.arange(start=10, stop=10*(hyperparams[HP_NUM_LAYERS]+1), step=10)[::-1])\n",
    "    else:\n",
    "        raise ValueError(\"ArchitectureType:\" + hyperparams[HP_ARCHI_TYPE])\n",
    "    our_layers = np.append(our_layers, np.array([1]))\n",
    "    pinn = PINN(our_layers,\n",
    "                f_regularization=hyperparams[HP_REGU_PDE_L],\n",
    "                activation_function=hyperparams[HP_ACTIV_FUNC],\n",
    "                a=A,\n",
    "                d=D,\n",
    "                _additional_constraints=additional_constraints)\n",
    "    # Training the PINN\n",
    "    init_params = pinn.get_weights().numpy()\n",
    "    results = scipy.optimize.minimize(fun=pinn.optimizerfunc,\n",
    "                                      x0=init_params,\n",
    "                                      args=(),\n",
    "                                      method='L-BFGS-B',\n",
    "                                      jac=True, #fun is assumed to return the gradient along with the objective function\n",
    "                                      options={'disp': None,\n",
    "                                               'maxcor': 200,\n",
    "                                               'ftol': 1 * np.finfo(float).eps,\n",
    "                                               'gtol': 5e-8,\n",
    "                                               'maxfun': 50000,\n",
    "                                               'maxiter': max(maxiter - pinn.get_epoch(), 0),\n",
    "                                               'iprint': -1,  #print update every 50 iterations\n",
    "                                               'maxls': 50})\n",
    "    # Measuring the validation set loss\n",
    "    pinn.set_weights(results.x)\n",
    "    validation_loss = pinn.loss(x_ic_valid, p_ic_valid, x_bc_valid, x_f_valid)[0]\n",
    "    save_model_weights(pinn, name=f\"GridSearch_VL{validation_loss:4.4e}\")\n",
    "    return validation_loss ** -1\n",
    "\n",
    "\n",
    "def transfer_train(hyperparams):\n",
    "    # Defining the PINN for the Hyperparameter choice\n",
    "    our_layers = np.array([2])\n",
    "    if hyperparams[HP_ARCHI_TYPE] == 'uniform':\n",
    "        our_layers = np.append(our_layers, np.repeat(20, hyperparams[HP_NUM_LAYERS]))\n",
    "    elif hyperparams[HP_ARCHI_TYPE] == 'decreasing':\n",
    "        our_layers = np.append(our_layers, np.arange(start=10, stop=10*(hyperparams[HP_NUM_LAYERS]+1), step=10)[::-1])\n",
    "    else:\n",
    "        raise ValueError(\"ArchitectureType:\" + hyperparams[HP_ARCHI_TYPE])\n",
    "    our_layers = np.append(our_layers, np.array([1]))\n",
    "    a = np.linspace(A/100, A, num=4)\n",
    "    d = np.linspace(D/100, D, num=4)\n",
    "    pinn = PINN(our_layers,\n",
    "                f_regularization=hyperparams[HP_REGU_PDE_L],\n",
    "                activation_function=hyperparams[HP_ACTIV_FUNC],\n",
    "                a=a[0],\n",
    "                d=d[0],\n",
    "                _additional_constraints=additional_constraints)\n",
    "    results = []\n",
    "    previous_pinn_weigths = pinn.get_weights().numpy()\n",
    "    for i in range(0, len(a)):\n",
    "        # Transfer learning -> Initialize with the weights of the previous PINN trained for smaller PDE parameters\n",
    "        pinn.set_pde_params(a[i], d[i])\n",
    "        pinn.set_weights(previous_pinn_weigths)\n",
    "        init_params = previous_pinn_weigths\n",
    "        results = scipy.optimize.minimize(fun=pinn.optimizerfunc,\n",
    "                                          x0=init_params,\n",
    "                                          args=(),\n",
    "                                          method='L-BFGS-B',\n",
    "                                          jac=True, #fun is assumed to return the gradient along with the objective function\n",
    "                                          options={'disp': None,\n",
    "                                                   'maxcor': 200,\n",
    "                                                   'ftol': 1 * np.finfo(float).eps,\n",
    "                                                   'gtol': 5e-8,\n",
    "                                                   'maxfun': 50000,\n",
    "                                                   'maxiter': max(maxiter - pinn.get_epoch(), 0),\n",
    "                                                   'iprint': -1,  #print update every 50 iterations\n",
    "                                                   'maxls': 50})\n",
    "        previous_pinn_weigths = results.x\n",
    "    # Measuring the validation set loss\n",
    "    pinn.set_weights(results.x)\n",
    "    validation_loss = pinn.loss(x_ic_valid, p_ic_valid, x_bc_valid, x_f_valid)[0]\n",
    "    save_model_weights(pinn, name=f\"GridSearch_VL{validation_loss:4.4e}\")\n",
    "    return validation_loss ** -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run(run_dir, hyperparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hyperparams)  # record the values used in this trial\n",
    "    if hyperparams[HP_LEARN_TYPE] == 'standard':\n",
    "        validation_loss = standard_train(hyperparams)\n",
    "    elif hyperparams[HP_LEARN_TYPE] == 'transfer':\n",
    "        validation_loss = transfer_train(hyperparams)\n",
    "    else:\n",
    "        raise ValueError(\"LearningType:\" + hyperparams[HP_LEARN_TYPE])\n",
    "    tf.summary.scalar(METRIC, validation_loss, step=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GridSearch proper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 0.001, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 9.8150e-02 IC: 9.5736e-02 BC: 2.3850e-03 f: 2.9656e-02 Norm: 1.0084e+00 equi: 1.6160e-02\n",
      "epoch: 2 - Total: 4.1010e+02 IC: 4.0677e+02 BC: 3.2292e+00 f: 1.0410e+02 Norm: 5.8662e-01 equi: 9.2500e-03\n",
      "epoch: 3 - Total: 2.2477e-02 IC: 2.1605e-02 BC: 8.3363e-04 f: 3.7708e-02 Norm: 1.0027e+00 equi: 1.6051e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.2826e-02 ...\n",
      "--- Starting trial: run-1\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 0.001, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 6.0943e-01 IC: 6.0749e-01 BC: 6.6978e-07 f: 1.9362e+00 Norm: 9.8906e-01 equi: 1.5378e+01\n",
      "epoch: 2 - Total: 4.0533e+02 IC: 4.0533e+02 BC: 3.3091e-04 f: 5.7672e-01 Norm: 5.6442e-01 equi: 7.9796e+00\n",
      "epoch: 3 - Total: 1.3729e-01 IC: 1.3541e-01 BC: 8.4702e-07 f: 1.8713e+00 Norm: 9.7467e-01 equi: 1.5115e+01\n",
      "epoch: 4 - Total: 1.3798e-01 IC: 1.3541e-01 BC: 9.7916e-04 f: 1.5816e+00 Norm: 9.7467e-01 equi: 5.5524e-02\n",
      "epoch: 5 - Total: 1.1062e+02 IC: 1.1056e+02 BC: 4.8331e-02 f: 1.4159e+01 Norm: 1.1723e+00 equi: 2.9606e-02\n",
      "epoch: 6 - Total: 1.2219e-01 IC: 1.1970e-01 BC: 8.4393e-04 f: 1.6438e+00 Norm: 9.7693e-01 equi: 5.5117e-02\n",
      "epoch: 7 - Total: 1.2443e-01 IC: 1.1970e-01 BC: 3.2771e-03 f: 1.4499e+00 Norm: 9.7693e-01 equi: 3.1028e-02\n",
      "epoch: 8 - Total: 3.3178e+02 IC: 3.3039e+02 BC: 1.3603e+00 f: 2.6240e+01 Norm: 6.0042e-01 equi: 1.3881e-02\n",
      "epoch: 9 - Total: 1.1148e-01 IC: 1.0642e-01 BC: 3.6982e-03 f: 1.3546e+00 Norm: 9.7448e-01 equi: 3.0895e-02\n",
      "epoch: 10 - Total: 1.1586e-01 IC: 1.0642e-01 BC: 8.2385e-03 f: 1.1968e+00 Norm: 9.7448e-01 equi: 2.4108e-02\n",
      "epoch: 11 - Total: 1.8633e+02 IC: 1.8547e+02 BC: 7.9759e-01 f: 5.8418e+01 Norm: 1.2410e+00 equi: 2.4898e-02\n",
      "epoch: 12 - Total: 1.0702e-01 IC: 9.8389e-02 BC: 7.3583e-03 f: 1.2687e+00 Norm: 9.7625e-01 equi: 2.4113e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.1563e-01 ...\n",
      "--- Starting trial: run-2\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 0.1, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 1.0824e+00 IC: 7.8151e-01 BC: 8.5744e-03 f: 2.9236e+00 Norm: 9.8740e-01 equi: 8.1394e-03\n",
      "epoch: 2 - Total: 4.0187e+02 IC: 3.8971e+02 BC: 3.5387e+00 f: 8.6222e+01 Norm: 5.7601e-01 equi: 5.5965e-03\n",
      "epoch: 3 - Total: 3.0550e-01 IC: 1.3167e-01 BC: 1.7793e-02 f: 1.5604e+00 Norm: 9.6989e-01 equi: 8.0135e-03\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL3.1320e-01 ...\n",
      "--- Starting trial: run-3\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 0.1, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 3.0328e+00 IC: 2.9787e+00 BC: 5.7802e-06 f: 5.4045e-01 Norm: 9.8003e-01 equi: 2.1666e+00\n",
      "epoch: 2 - Total: 3.6317e+02 IC: 3.6317e+02 BC: 2.5216e-04 f: 1.0139e-02 Norm: 1.3987e+00 equi: 3.7136e-01\n",
      "epoch: 3 - Total: 6.8195e-01 IC: 6.3546e-01 BC: 2.3946e-06 f: 4.6489e-01 Norm: 1.0114e+00 equi: 1.9877e+00\n",
      "epoch: 4 - Total: 6.9576e-01 IC: 6.3546e-01 BC: 2.7681e-03 f: 5.7529e-01 Norm: 1.0114e+00 equi: 3.1476e-02\n",
      "epoch: 5 - Total: 4.2757e+00 IC: 4.2212e+00 BC: 1.0089e-02 f: 4.4427e-01 Norm: 9.5946e-01 equi: 4.1326e-03\n",
      "epoch: 6 - Total: 1.1494e-01 IC: 1.0427e-01 BC: 1.6611e-04 f: 1.0502e-01 Norm: 9.9757e-01 equi: 1.5990e-02\n",
      "epoch: 7 - Total: 1.1952e-01 IC: 1.0427e-01 BC: 6.4505e-04 f: 1.4597e-01 Norm: 9.9757e-01 equi: 2.0098e-02\n",
      "epoch: 8 - Total: 4.0675e+02 IC: 4.0119e+02 BC: 1.2829e+00 f: 4.2798e+01 Norm: 1.4126e+00 equi: 2.5524e-02\n",
      "epoch: 9 - Total: 1.8538e-02 IC: 9.9835e-03 BC: 1.6105e-04 f: 8.3938e-02 Norm: 1.0040e+00 equi: 2.0171e-02\n",
      "epoch: 10 - Total: 1.9984e-02 IC: 9.9835e-03 BC: 3.5876e-04 f: 9.6412e-02 Norm: 1.0040e+00 equi: 2.1919e-02\n",
      "epoch: 11 - Total: 2.4212e+01 IC: 2.1341e+01 BC: 3.1592e-01 f: 2.5543e+01 Norm: 1.0316e+00 equi: 4.1544e-03\n",
      "epoch: 12 - Total: 1.5915e-02 IC: 9.6784e-03 BC: 1.5063e-04 f: 6.0861e-02 Norm: 1.0043e+00 equi: 2.1334e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.7497e-02 ...\n",
      "--- Starting trial: run-4\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 1.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 2.7318e+00 IC: 7.8227e-01 BC: 9.4782e-02 f: 1.8547e+00 Norm: 9.4347e-01 equi: 7.8905e-03\n",
      "epoch: 2 - Total: 4.4964e+01 IC: 4.3737e+01 BC: 5.0969e-01 f: 7.1804e-01 Norm: 1.1773e+00 equi: 9.8080e-02\n",
      "epoch: 3 - Total: 1.5725e+00 IC: 2.4696e-01 BC: 2.2039e-02 f: 1.3035e+00 Norm: 9.8178e-01 equi: 1.6009e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.5983e+00 ...\n",
      "--- Starting trial: run-5\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 1.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 7.5806e+00 IC: 7.4172e-02 BC: 3.6947e-06 f: 7.5064e+00 Norm: 1.0481e+00 equi: 1.0866e+02\n",
      "epoch: 2 - Total: 2.0257e+01 IC: 1.7560e+01 BC: 2.2625e-05 f: 2.6968e+00 Norm: 8.8657e-01 equi: 7.9290e+00\n",
      "epoch: 3 - Total: 2.4285e+00 IC: 1.5728e+00 BC: 4.9913e-07 f: 8.5569e-01 Norm: 9.8889e-01 equi: 1.3771e+01\n",
      "epoch: 4 - Total: 2.6362e+00 IC: 1.5728e+00 BC: 5.7700e-04 f: 1.0629e+00 Norm: 9.8889e-01 equi: 5.1288e-02\n",
      "epoch: 5 - Total: 3.9621e+02 IC: 3.8458e+02 BC: 3.3793e-01 f: 1.1298e+01 Norm: 1.4075e+00 equi: 2.6677e-02\n",
      "epoch: 6 - Total: 6.0156e-01 IC: 3.4212e-02 BC: 4.3932e-04 f: 5.6691e-01 Norm: 1.0169e+00 equi: 4.8854e-02\n",
      "epoch: 7 - Total: 4.8223e-01 IC: 3.4212e-02 BC: 1.7060e-03 f: 4.4631e-01 Norm: 1.0169e+00 equi: 3.1017e-02\n",
      "epoch: 8 - Total: 5.3016e+01 IC: 3.1388e+01 BC: 3.5377e-02 f: 2.1592e+01 Norm: 1.0582e+00 equi: 3.5088e-03\n",
      "epoch: 9 - Total: 3.5290e-01 IC: 1.7386e-01 BC: 1.9730e-03 f: 1.7707e-01 Norm: 1.0184e+00 equi: 2.7280e-02\n",
      "epoch: 10 - Total: 3.0736e-01 IC: 1.7386e-01 BC: 4.3952e-03 f: 1.2911e-01 Norm: 1.0184e+00 equi: 2.3207e-02\n",
      "epoch: 11 - Total: 5.2823e+02 IC: 4.2757e+02 BC: 3.4357e+00 f: 9.7217e+01 Norm: 5.6612e-01 equi: 1.0633e-02\n",
      "epoch: 12 - Total: 2.1970e-01 IC: 2.8110e-02 BC: 1.9303e-03 f: 1.8966e-01 Norm: 1.0126e+00 equi: 2.2997e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.2147e-01 ...\n",
      "--- Starting trial: run-6\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 10.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 2.2928e+01 IC: 6.8715e-01 BC: 1.4028e-02 f: 2.2227e+00 Norm: 1.0068e+00 equi: 1.9891e-02\n",
      "epoch: 2 - Total: 1.5015e+03 IC: 3.7996e+02 BC: 2.6420e+00 f: 1.1189e+02 Norm: 1.3796e+00 equi: 8.7999e-03\n",
      "epoch: 3 - Total: 4.1732e+00 IC: 1.7764e+00 BC: 2.4927e-02 f: 2.3718e-01 Norm: 1.0442e+00 equi: 1.8106e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL4.1878e+00 ...\n",
      "--- Starting trial: run-7\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'swish', 'MSE(f) Regularization': 10.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 2.3574e+01 IC: 5.0117e+00 BC: 1.5833e-05 f: 1.8563e+00 Norm: 1.0551e+00 equi: 1.9314e+01\n",
      "epoch: 2 - Total: 5.9745e+02 IC: 2.6247e+01 BC: 6.0978e-05 f: 5.7120e+01 Norm: 7.8989e-01 equi: 1.4077e+03\n",
      "epoch: 3 - Total: 2.4331e+00 IC: 1.7308e+00 BC: 6.5929e-06 f: 7.0230e-02 Norm: 1.0105e+00 equi: 2.3603e+00\n",
      "epoch: 4 - Total: 4.0729e+00 IC: 1.7308e+00 BC: 7.6214e-03 f: 2.3345e-01 Norm: 1.0105e+00 equi: 1.0700e-02\n",
      "epoch: 5 - Total: 8.4314e+02 IC: 1.8628e+02 BC: 7.7448e-02 f: 6.5678e+01 Norm: 8.0529e-01 equi: 5.3365e-01\n",
      "epoch: 6 - Total: 2.2484e+00 IC: 1.0971e+00 BC: 5.8904e-03 f: 1.1455e-01 Norm: 1.0014e+00 equi: 1.8461e-02\n",
      "epoch: 7 - Total: 5.1025e+00 IC: 1.0971e+00 BC: 2.2874e-02 f: 3.9826e-01 Norm: 1.0014e+00 equi: 2.0404e-02\n",
      "epoch: 8 - Total: 5.7333e+02 IC: 1.3658e+02 BC: 4.4609e-01 f: 4.3631e+01 Norm: 7.9839e-01 equi: 7.5927e-02\n",
      "epoch: 9 - Total: 4.5298e+00 IC: 9.9080e-01 BC: 1.7709e-02 f: 3.5213e-01 Norm: 9.9556e-01 equi: 2.1878e-02\n",
      "epoch: 10 - Total: 8.6946e+00 IC: 9.9080e-01 BC: 3.9449e-02 f: 7.6643e-01 Norm: 9.9556e-01 equi: 2.2499e-02\n",
      "epoch: 11 - Total: 3.3054e+01 IC: 1.2705e+01 BC: 5.9340e-02 f: 2.0290e+00 Norm: 1.0710e+00 equi: 1.6848e-02\n",
      "epoch: 12 - Total: 2.6158e+00 IC: 9.3482e-01 BC: 1.4221e-02 f: 1.6668e-01 Norm: 1.0188e+00 equi: 2.0563e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.6674e+00 ...\n",
      "--- Starting trial: run-8\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 0.001, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 1.3073e+01 IC: 1.2876e+01 BC: 1.8741e-01 f: 1.0068e+01 Norm: 1.0420e+00 equi: 1.9031e-02\n",
      "epoch: 2 - Total: 4.7753e+02 IC: 4.7362e+02 BC: 3.8006e+00 f: 1.0820e+02 Norm: 5.4181e-01 equi: 1.1511e-02\n",
      "epoch: 3 - Total: 1.8566e+00 IC: 1.8027e+00 BC: 5.1217e-02 f: 2.6715e+00 Norm: 9.7499e-01 equi: 1.7986e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.9834e+00 ...\n",
      "--- Starting trial: run-9\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 0.001, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 6.3830e+00 IC: 6.3381e+00 BC: 5.2011e-05 f: 4.4942e+01 Norm: 1.1614e+00 equi: 2.6485e-02\n",
      "epoch: 2 - Total: 9.2425e+02 IC: 9.2422e+02 BC: 6.7676e-04 f: 3.0799e+01 Norm: 4.4059e-01 equi: 1.6302e-02\n",
      "epoch: 3 - Total: 8.8691e-01 IC: 8.4103e-01 BC: 2.6780e-05 f: 4.5849e+01 Norm: 1.1153e+00 equi: 2.5799e-02\n",
      "epoch: 4 - Total: 9.0864e-01 IC: 8.4103e-01 BC: 3.0958e-02 f: 3.6644e+01 Norm: 1.1153e+00 equi: 2.5859e-02\n",
      "epoch: 5 - Total: 2.8226e+02 IC: 2.8168e+02 BC: 5.3707e-01 f: 5.1432e+01 Norm: 7.4219e-01 equi: 2.0018e-02\n",
      "epoch: 6 - Total: 6.8995e-01 IC: 6.2850e-01 BC: 2.4460e-02 f: 3.6992e+01 Norm: 1.1061e+00 equi: 2.5710e-02\n",
      "epoch: 7 - Total: 7.5368e-01 IC: 6.2850e-01 BC: 9.4983e-02 f: 3.0197e+01 Norm: 1.1061e+00 equi: 2.6180e-02\n",
      "epoch: 8 - Total: 8.5579e+02 IC: 8.5258e+02 BC: 3.1559e+00 f: 5.0397e+01 Norm: 1.6660e+00 equi: 3.4921e-02\n",
      "epoch: 9 - Total: 6.0354e-01 IC: 4.6956e-01 BC: 1.0520e-01 f: 2.8780e+01 Norm: 1.1132e+00 equi: 2.6274e-02\n",
      "epoch: 10 - Total: 7.2714e-01 IC: 4.6956e-01 BC: 2.3436e-01 f: 2.3224e+01 Norm: 1.1132e+00 equi: 2.6804e-02\n",
      "epoch: 11 - Total: 3.0353e+02 IC: 2.9822e+02 BC: 5.1642e+00 f: 1.5110e+02 Norm: 6.9939e-01 equi: 1.7683e-02\n",
      "epoch: 12 - Total: 5.9024e-01 IC: 3.7375e-01 BC: 1.9241e-01 f: 2.4086e+01 Norm: 1.1053e+00 equi: 2.6612e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL6.8027e-01 ...\n",
      "--- Starting trial: run-10\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 0.1, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 1.1357e+00 IC: 1.0107e+00 BC: 4.9474e-02 f: 7.5611e-01 Norm: 1.0445e+00 equi: 2.3587e-02\n",
      "epoch: 2 - Total: 1.7210e+03 IC: 1.6595e+03 BC: 1.4321e+01 f: 4.7119e+02 Norm: 1.7696e-01 equi: 1.5659e-02\n",
      "epoch: 3 - Total: 2.7568e-01 IC: 1.2379e-01 BC: 2.1047e-02 f: 1.3085e+00 Norm: 1.0279e+00 equi: 2.3439e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.9055e-01 ...\n",
      "--- Starting trial: run-11\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 0.1, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 3.7475e+01 IC: 3.7404e+01 BC: 7.4352e-05 f: 7.1402e-01 Norm: 1.1006e+00 equi: 2.0948e-02\n",
      "epoch: 2 - Total: 5.3118e+02 IC: 5.3049e+02 BC: 3.7681e-04 f: 6.8502e+00 Norm: 5.5208e-01 equi: 2.0719e-02\n",
      "epoch: 3 - Total: 5.1140e+00 IC: 5.1102e+00 BC: 1.9329e-05 f: 3.7834e-02 Norm: 9.9528e-01 equi: 2.1359e-02\n",
      "epoch: 4 - Total: 5.1953e+00 IC: 5.1102e+00 BC: 2.2345e-02 f: 6.2805e-01 Norm: 9.9528e-01 equi: 1.9051e-02\n",
      "epoch: 5 - Total: 4.6002e+01 IC: 4.2382e+01 BC: 9.7108e-02 f: 3.5222e+01 Norm: 9.6950e-01 equi: 2.6838e-02\n",
      "epoch: 6 - Total: 2.1156e+00 IC: 2.0193e+00 BC: 2.7370e-03 f: 9.3581e-01 Norm: 9.9001e-01 equi: 2.0643e-02\n",
      "epoch: 7 - Total: 2.1433e+00 IC: 2.0193e+00 BC: 1.0628e-02 f: 1.1331e+00 Norm: 9.9001e-01 equi: 1.9821e-02\n",
      "epoch: 8 - Total: 8.0445e+02 IC: 7.9027e+02 BC: 2.7081e+00 f: 1.1468e+02 Norm: 1.5589e+00 equi: 2.5751e-02\n",
      "epoch: 9 - Total: 9.2849e-01 IC: 8.5415e-01 BC: 1.8235e-02 f: 5.6105e-01 Norm: 1.0101e+00 equi: 2.0000e-02\n",
      "epoch: 10 - Total: 9.7279e-01 IC: 8.5415e-01 BC: 4.0622e-02 f: 7.8019e-01 Norm: 1.0101e+00 equi: 1.9621e-02\n",
      "epoch: 11 - Total: 3.1562e+01 IC: 2.7135e+01 BC: 8.3273e-01 f: 3.5945e+01 Norm: 1.0770e+00 equi: 2.4969e-02\n",
      "epoch: 12 - Total: 9.4757e-02 IC: 2.0609e-02 BC: 6.8076e-03 f: 6.7340e-01 Norm: 1.0198e+00 equi: 2.0215e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL9.8199e-02 ...\n",
      "--- Starting trial: run-12\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 1.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 1.0092e+01 IC: 7.0735e+00 BC: 2.5648e-01 f: 2.7625e+00 Norm: 1.0988e+00 equi: 3.1802e-02\n",
      "epoch: 2 - Total: 1.8156e+03 IC: 1.4199e+03 BC: 1.3390e+01 f: 3.8229e+02 Norm: 2.2739e-01 equi: 1.8793e-02\n",
      "epoch: 3 - Total: 5.7961e+00 IC: 1.2895e+00 BC: 1.0510e-01 f: 4.4015e+00 Norm: 1.0618e+00 equi: 3.1302e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL6.0019e+00 ...\n",
      "--- Starting trial: run-13\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 1.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 2.5282e+01 IC: 2.1394e+01 BC: 5.3371e-05 f: 3.8881e+00 Norm: 9.1662e-01 equi: 2.4324e-02\n",
      "epoch: 2 - Total: 1.3475e+03 IC: 1.3009e+03 BC: 9.1370e-04 f: 4.6612e+01 Norm: 1.6785e+00 equi: 2.3745e-02\n",
      "epoch: 3 - Total: 3.9143e+00 IC: 2.4500e+00 BC: 1.5507e-05 f: 1.4643e+00 Norm: 9.9081e-01 equi: 2.4345e-02\n",
      "epoch: 4 - Total: 4.1762e+00 IC: 2.4500e+00 BC: 1.7927e-02 f: 1.7082e+00 Norm: 9.9081e-01 equi: 2.3870e-02\n",
      "epoch: 5 - Total: 6.0640e+02 IC: 4.6736e+02 BC: 6.5349e-01 f: 1.3839e+02 Norm: 1.3045e+00 equi: 1.2876e-02\n",
      "epoch: 6 - Total: 3.3474e+00 IC: 2.0478e+00 BC: 1.1078e-02 f: 1.2886e+00 Norm: 9.9781e-01 equi: 2.3495e-02\n",
      "epoch: 7 - Total: 3.8397e+00 IC: 2.0478e+00 BC: 4.3020e-02 f: 1.7489e+00 Norm: 9.9781e-01 equi: 2.3493e-02\n",
      "epoch: 8 - Total: 1.0095e+03 IC: 8.5545e+02 BC: 1.7443e+00 f: 1.5229e+02 Norm: 4.5424e-01 equi: 1.8342e-02\n",
      "epoch: 9 - Total: 3.1526e+00 IC: 1.2892e+00 BC: 4.7332e-02 f: 1.8160e+00 Norm: 9.8471e-01 equi: 2.3387e-02\n",
      "epoch: 10 - Total: 4.2049e+00 IC: 1.2892e+00 BC: 1.0544e-01 f: 2.8102e+00 Norm: 9.8471e-01 equi: 2.3363e-02\n",
      "epoch: 11 - Total: 1.2215e+03 IC: 9.0354e+02 BC: 8.9596e+00 f: 3.0905e+02 Norm: 1.5706e+00 equi: 2.1544e-02\n",
      "epoch: 12 - Total: 3.3328e+00 IC: 1.3567e+00 BC: 6.1235e-02 f: 1.9149e+00 Norm: 9.9781e-01 equi: 2.3312e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL3.3828e+00 ...\n",
      "--- Starting trial: run-14\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 10.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 2.5913e+02 IC: 6.0287e+00 BC: 8.5509e-02 f: 2.5301e+01 Norm: 1.0370e+00 equi: 2.6015e-02\n",
      "epoch: 2 - Total: 2.3908e+03 IC: 4.9823e+02 BC: 3.2600e+00 f: 1.8893e+02 Norm: 1.4038e+00 equi: 1.7115e-02\n",
      "epoch: 3 - Total: 2.5916e+01 IC: 1.3435e+01 BC: 1.6997e-01 f: 1.2311e+00 Norm: 1.1211e+00 equi: 2.2860e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.6131e+01 ...\n",
      "--- Starting trial: run-15\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 2, 'Activation': 'tanh', 'MSE(f) Regularization': 10.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 5.5595e+01 IC: 5.4747e+01 BC: 1.0843e-04 f: 8.4762e-02 Norm: 8.6301e-01 equi: 1.8550e-02\n",
      "epoch: 2 - Total: 1.1746e+03 IC: 1.0557e+03 BC: 7.0899e-04 f: 1.1881e+01 Norm: 1.6326e+00 equi: 1.5629e-02\n",
      "epoch: 3 - Total: 9.1366e+00 IC: 6.9846e+00 BC: 2.9835e-05 f: 2.1519e-01 Norm: 9.8383e-01 equi: 1.8539e-02\n",
      "epoch: 4 - Total: 1.2445e+01 IC: 6.9846e+00 BC: 3.4489e-02 f: 5.4261e-01 Norm: 9.8383e-01 equi: 1.7802e-02\n",
      "epoch: 5 - Total: 1.1198e+03 IC: 1.0311e+02 BC: 4.8486e-01 f: 1.0162e+02 Norm: 1.4008e+00 equi: 4.5779e-02\n",
      "epoch: 6 - Total: 1.0144e+01 IC: 5.3442e+00 BC: 2.4219e-02 f: 4.7756e-01 Norm: 1.0011e+00 equi: 1.8402e-02\n",
      "epoch: 7 - Total: 2.3071e+01 IC: 5.3442e+00 BC: 9.4046e-02 f: 1.7633e+00 Norm: 1.0011e+00 equi: 1.7853e-02\n",
      "epoch: 8 - Total: 3.4465e+03 IC: 4.3032e+02 BC: 1.6583e+00 f: 3.0145e+02 Norm: 1.2247e+00 equi: 5.4240e-03\n",
      "epoch: 9 - Total: 1.7408e+01 IC: 4.4981e+00 BC: 6.4354e-02 f: 1.2845e+00 Norm: 1.0095e+00 equi: 1.7130e-02\n",
      "epoch: 10 - Total: 3.2306e+01 IC: 4.4981e+00 BC: 1.4336e-01 f: 2.7665e+00 Norm: 1.0095e+00 equi: 1.6755e-02\n",
      "epoch: 11 - Total: 5.5035e+02 IC: 9.1954e+01 BC: 1.1987e+00 f: 4.5720e+01 Norm: 9.1216e-01 equi: 1.7620e-02\n",
      "epoch: 12 - Total: 4.8612e+00 IC: 6.5701e-01 BC: 1.3939e-02 f: 4.1903e-01 Norm: 9.9391e-01 equi: 1.7133e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL4.8964e+00 ...\n",
      "--- Starting trial: run-16\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 0.001, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 2.1620e-02 IC: 1.8174e-02 BC: 3.4059e-03 f: 3.9820e-02 Norm: 9.9037e-01 equi: 1.6972e-02\n",
      "epoch: 2 - Total: 6.1277e+02 IC: 6.0822e+02 BC: 4.4110e+00 f: 1.4067e+02 Norm: 1.5093e+00 equi: 3.1149e-02\n",
      "epoch: 3 - Total: 8.6421e-03 IC: 6.1409e-03 BC: 2.4405e-03 f: 6.0638e-02 Norm: 9.9277e-01 equi: 1.7025e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.1823e-02 ...\n",
      "--- Starting trial: run-17\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 0.001, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 5.2270e-03 IC: 5.2265e-03 BC: 3.8151e-08 f: 5.4991e-04 Norm: 9.9820e-01 equi: 7.8586e+00\n",
      "epoch: 2 - Total: 4.9089e+02 IC: 4.9088e+02 BC: 3.6572e-04 f: 2.2907e-03 Norm: 1.4621e+00 equi: 7.3318e+00\n",
      "epoch: 3 - Total: 2.9594e-03 IC: 2.9588e-03 BC: 2.6970e-08 f: 5.4973e-04 Norm: 9.9916e-01 equi: 7.8575e+00\n",
      "epoch: 4 - Total: 2.9905e-03 IC: 2.9588e-03 BC: 3.1177e-05 f: 5.6358e-04 Norm: 9.9916e-01 equi: 4.1170e-02\n",
      "epoch: 5 - Total: 3.2002e+02 IC: 3.1971e+02 BC: 3.0928e-01 f: 9.0630e+00 Norm: 1.3716e+00 equi: 4.6636e-02\n",
      "epoch: 6 - Total: 2.9859e-03 IC: 2.9548e-03 BC: 3.0609e-05 f: 5.5371e-04 Norm: 9.9921e-01 equi: 4.1171e-02\n",
      "epoch: 7 - Total: 3.0755e-03 IC: 2.9548e-03 BC: 1.1886e-04 f: 1.9193e-03 Norm: 9.9921e-01 equi: 2.3526e-02\n",
      "epoch: 8 - Total: 4.6238e+01 IC: 4.6127e+01 BC: 1.0674e-01 f: 3.8265e+00 Norm: 8.5895e-01 equi: 1.8252e-02\n",
      "epoch: 9 - Total: 3.0623e-03 IC: 2.9402e-03 BC: 1.2015e-04 f: 1.9529e-03 Norm: 9.9913e-01 equi: 2.3524e-02\n",
      "epoch: 10 - Total: 3.2126e-03 IC: 2.9402e-03 BC: 2.6765e-04 f: 4.7165e-03 Norm: 9.9913e-01 equi: 1.9455e-02\n",
      "epoch: 11 - Total: 4.4974e+02 IC: 4.4612e+02 BC: 3.5096e+00 f: 1.0238e+02 Norm: 1.4412e+00 equi: 3.2741e-02\n",
      "epoch: 12 - Total: 3.1959e-03 IC: 2.9323e-03 BC: 2.5906e-04 f: 4.5395e-03 Norm: 9.9922e-01 equi: 1.9457e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL4.1636e-03 ...\n",
      "--- Starting trial: run-18\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 0.1, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 1.2552e-02 IC: 9.2122e-03 BC: 1.1604e-03 f: 2.1790e-02 Norm: 9.9332e-01 equi: 1.6120e-02\n",
      "epoch: 2 - Total: 3.9959e+02 IC: 3.8781e+02 BC: 2.7876e+00 f: 8.9988e+01 Norm: 1.4061e+00 equi: 2.6660e-02\n",
      "epoch: 3 - Total: 7.3627e-03 IC: 3.3953e-03 BC: 8.1741e-04 f: 3.1500e-02 Norm: 9.9479e-01 equi: 1.6152e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL8.8167e-03 ...\n",
      "--- Starting trial: run-19\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 0.1, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 5.0846e-02 IC: 4.4390e-02 BC: 2.4900e-07 f: 6.4558e-02 Norm: 9.9175e-01 equi: 9.4059e+00\n",
      "epoch: 2 - Total: 3.8304e+02 IC: 3.8304e+02 BC: 2.8031e-04 f: 1.7955e-02 Norm: 1.4069e+00 equi: 6.9228e+00\n",
      "epoch: 3 - Total: 1.6590e-02 IC: 1.0193e-02 BC: 1.2832e-07 f: 6.3968e-02 Norm: 9.9567e-01 equi: 9.3818e+00\n",
      "epoch: 4 - Total: 1.4937e-02 IC: 1.0193e-02 BC: 1.4834e-04 f: 4.5953e-02 Norm: 9.9567e-01 equi: 3.8836e-03\n",
      "epoch: 5 - Total: 1.9574e+02 IC: 1.9481e+02 BC: 1.4508e-01 f: 7.8500e+00 Norm: 7.1792e-01 equi: 1.9185e-02\n",
      "epoch: 6 - Total: 1.4834e-02 IC: 1.0180e-02 BC: 1.5312e-04 f: 4.5017e-02 Norm: 9.9547e-01 equi: 3.8893e-03\n",
      "epoch: 7 - Total: 1.4100e-02 IC: 1.0180e-02 BC: 5.9460e-04 f: 3.3258e-02 Norm: 9.9547e-01 equi: 1.0459e-02\n",
      "epoch: 8 - Total: 1.5988e+02 IC: 1.5791e+02 BC: 5.7590e-01 f: 1.3946e+01 Norm: 1.2669e+00 equi: 2.9264e-02\n",
      "epoch: 9 - Total: 1.4006e-02 IC: 1.0029e-02 BC: 5.6742e-04 f: 3.4095e-02 Norm: 9.9569e-01 equi: 1.0470e-02\n",
      "epoch: 10 - Total: 1.4215e-02 IC: 1.0029e-02 BC: 1.2640e-03 f: 2.9217e-02 Norm: 9.9569e-01 equi: 1.4324e-02\n",
      "epoch: 11 - Total: 2.1040e+02 IC: 2.0438e+02 BC: 1.3562e+00 f: 4.6635e+01 Norm: 7.0520e-01 equi: 1.3563e-02\n",
      "epoch: 12 - Total: 1.4114e-02 IC: 1.0009e-02 BC: 1.3053e-03 f: 2.7994e-02 Norm: 9.9549e-01 equi: 1.4324e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.7278e-02 ...\n",
      "--- Starting trial: run-20\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 1.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 5.8496e-02 IC: 4.6531e-02 BC: 1.3214e-03 f: 1.0643e-02 Norm: 1.0062e+00 equi: 2.0891e-02\n",
      "epoch: 2 - Total: 4.3882e+02 IC: 3.5362e+02 BC: 2.5793e+00 f: 8.2621e+01 Norm: 6.1039e-01 equi: 1.3030e-02\n",
      "epoch: 3 - Total: 3.8752e-02 IC: 1.7630e-02 BC: 7.0987e-04 f: 2.0412e-02 Norm: 1.0035e+00 equi: 2.0832e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL3.9718e-02 ...\n",
      "--- Starting trial: run-21\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 1.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 4.2349e-02 IC: 8.8490e-03 BC: 3.0290e-08 f: 3.3500e-02 Norm: 1.0033e+00 equi: 4.9183e+00\n",
      "epoch: 2 - Total: 3.8529e+02 IC: 3.8495e+02 BC: 2.9842e-04 f: 3.4275e-01 Norm: 5.8119e-01 equi: 5.8462e+01\n",
      "epoch: 3 - Total: 3.7646e-02 IC: 4.9975e-03 BC: 1.4662e-08 f: 3.2648e-02 Norm: 1.0018e+00 equi: 5.0037e+00\n",
      "epoch: 4 - Total: 3.2055e-02 IC: 4.9975e-03 BC: 1.6950e-05 f: 2.7040e-02 Norm: 1.0018e+00 equi: 5.8195e-03\n",
      "epoch: 5 - Total: 3.2907e+02 IC: 3.1751e+02 BC: 2.6020e-01 f: 1.1295e+01 Norm: 1.3634e+00 equi: 3.1221e-02\n",
      "epoch: 6 - Total: 3.0598e-02 IC: 5.9537e-03 BC: 2.5082e-05 f: 2.4619e-02 Norm: 1.0026e+00 equi: 5.7694e-03\n",
      "epoch: 7 - Total: 2.4846e-02 IC: 5.9537e-03 BC: 9.7397e-05 f: 1.8794e-02 Norm: 1.0026e+00 equi: 1.0337e-02\n",
      "epoch: 8 - Total: 1.7317e+02 IC: 1.4912e+02 BC: 4.3241e-01 f: 2.3615e+01 Norm: 1.2374e+00 equi: 1.4682e-02\n",
      "epoch: 9 - Total: 2.4388e-02 IC: 7.6032e-03 BC: 1.1626e-04 f: 1.6669e-02 Norm: 1.0030e+00 equi: 1.0279e-02\n",
      "epoch: 10 - Total: 2.0012e-02 IC: 7.6032e-03 BC: 2.5899e-04 f: 1.2149e-02 Norm: 1.0030e+00 equi: 1.3077e-02\n",
      "epoch: 11 - Total: 2.3361e+02 IC: 2.0028e+02 BC: 1.7248e+00 f: 3.1610e+01 Norm: 6.8214e-01 equi: 7.0921e-03\n",
      "epoch: 12 - Total: 1.9803e-02 IC: 6.4094e-03 BC: 2.2391e-04 f: 1.3170e-02 Norm: 1.0027e+00 equi: 1.3044e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.9972e-02 ...\n",
      "--- Starting trial: run-22\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 10.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 8.8894e-01 IC: 1.2452e-01 BC: 1.7580e-03 f: 7.6266e-02 Norm: 1.0035e+00 equi: 1.7039e-02\n",
      "epoch: 2 - Total: 1.5419e+03 IC: 4.6862e+02 BC: 3.4424e+00 f: 1.0698e+02 Norm: 5.4890e-01 equi: 9.3829e-03\n",
      "epoch: 3 - Total: 2.3706e-01 IC: 5.0686e-02 BC: 7.3716e-04 f: 1.8563e-02 Norm: 9.9461e-01 equi: 1.6859e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.3690e-01 ...\n",
      "--- Starting trial: run-23\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'swish', 'MSE(f) Regularization': 10.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 8.0237e-01 IC: 1.0601e-02 BC: 1.9226e-07 f: 7.9177e-02 Norm: 9.9308e-01 equi: 4.6154e-01\n",
      "epoch: 2 - Total: 3.1017e+01 IC: 7.5713e+00 BC: 1.1157e-05 f: 2.3446e+00 Norm: 1.0794e+00 equi: 2.4654e+01\n",
      "epoch: 3 - Total: 7.3438e-02 IC: 6.8708e-02 BC: 2.0195e-08 f: 4.7299e-04 Norm: 1.0041e+00 equi: 2.7688e-01\n",
      "epoch: 4 - Total: 7.8269e-02 IC: 6.8708e-02 BC: 2.3346e-05 f: 9.5381e-04 Norm: 1.0041e+00 equi: 1.9150e-02\n",
      "epoch: 5 - Total: 4.8895e+02 IC: 3.7553e+02 BC: 3.1992e-01 f: 1.1310e+01 Norm: 5.9839e-01 equi: 1.5210e-02\n",
      "epoch: 6 - Total: 5.1673e-03 IC: 3.3506e-03 BC: 3.2568e-05 f: 1.7841e-04 Norm: 9.9926e-01 equi: 1.9104e-02\n",
      "epoch: 7 - Total: 1.8138e-02 IC: 3.3506e-03 BC: 1.2647e-04 f: 1.4661e-03 Norm: 9.9926e-01 equi: 1.8346e-02\n",
      "epoch: 8 - Total: 8.6720e+02 IC: 3.8972e+02 BC: 1.2092e+00 f: 4.7627e+01 Norm: 1.3983e+00 equi: 2.2025e-02\n",
      "epoch: 9 - Total: 1.5492e-02 IC: 5.7916e-03 BC: 9.9420e-05 f: 9.6006e-04 Norm: 9.9994e-01 equi: 1.8347e-02\n",
      "epoch: 10 - Total: 3.6854e-02 IC: 5.7916e-03 BC: 2.2148e-04 f: 3.0841e-03 Norm: 9.9994e-01 equi: 1.8333e-02\n",
      "epoch: 11 - Total: 1.3262e+03 IC: 3.8665e+02 BC: 2.7920e+00 f: 9.3672e+01 Norm: 1.4016e+00 equi: 2.5724e-02\n",
      "epoch: 12 - Total: 3.4841e-02 IC: 8.8509e-03 BC: 1.8723e-04 f: 2.5803e-03 Norm: 1.0004e+00 equi: 1.8340e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL3.5268e-02 ...\n",
      "--- Starting trial: run-24\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 0.001, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 5.5516e+00 IC: 5.4630e+00 BC: 7.8287e-02 f: 1.0321e+01 Norm: 9.9559e-01 equi: 2.2629e-02\n",
      "epoch: 2 - Total: 1.6484e+03 IC: 1.6357e+03 BC: 1.2300e+01 f: 4.2920e+02 Norm: 1.8230e+00 equi: 3.3248e-02\n",
      "epoch: 3 - Total: 9.3253e-01 IC: 8.9392e-01 BC: 3.4489e-02 f: 4.1284e+00 Norm: 1.0403e+00 equi: 2.3030e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.0165e+00 ...\n",
      "--- Starting trial: run-25\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 0.001, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 2.7244e+00 IC: 2.7241e+00 BC: 4.2391e-06 f: 3.4302e-01 Norm: 1.0195e+00 equi: 2.3849e-02\n",
      "epoch: 2 - Total: 8.7227e+02 IC: 8.7227e+02 BC: 6.3958e-04 f: 5.4218e+00 Norm: 4.0964e-01 equi: 2.0903e-02\n",
      "epoch: 3 - Total: 7.4483e-01 IC: 7.4466e-01 BC: 5.9988e-07 f: 1.7096e-01 Norm: 9.7906e-01 equi: 2.3732e-02\n",
      "epoch: 4 - Total: 7.4559e-01 IC: 7.4466e-01 BC: 6.9346e-04 f: 2.3128e-01 Norm: 9.7906e-01 equi: 2.3670e-02\n",
      "epoch: 5 - Total: 9.3012e+02 IC: 9.2930e+02 BC: 7.9224e-01 f: 3.1111e+01 Norm: 1.6291e+00 equi: 3.0397e-02\n",
      "epoch: 6 - Total: 4.0038e-01 IC: 3.9900e-01 BC: 1.0696e-03 f: 3.1333e-01 Norm: 9.9663e-01 equi: 2.3792e-02\n",
      "epoch: 7 - Total: 4.0363e-01 IC: 3.9900e-01 BC: 4.1536e-03 f: 4.7553e-01 Norm: 9.9663e-01 equi: 2.3834e-02\n",
      "epoch: 8 - Total: 7.9089e+02 IC: 7.8813e+02 BC: 2.6433e+00 f: 1.1958e+02 Norm: 4.5002e-01 equi: 2.3879e-02\n",
      "epoch: 9 - Total: 3.0677e-01 IC: 3.0459e-01 BC: 1.8758e-03 f: 3.0705e-01 Norm: 9.8752e-01 equi: 2.3834e-02\n",
      "epoch: 10 - Total: 3.0919e-01 IC: 3.0459e-01 BC: 4.1787e-03 f: 4.2302e-01 Norm: 9.8752e-01 equi: 2.3406e-02\n",
      "epoch: 11 - Total: 8.7461e+02 IC: 8.6825e+02 BC: 6.1718e+00 f: 1.9547e+02 Norm: 1.6155e+00 equi: 3.3259e-02\n",
      "epoch: 12 - Total: 2.4143e-01 IC: 2.3605e-01 BC: 4.9117e-03 f: 4.6555e-01 Norm: 9.9443e-01 equi: 2.3491e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.6381e-01 ...\n",
      "--- Starting trial: run-26\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 0.1, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 9.2325e-01 IC: 8.4699e-01 BC: 3.8536e-02 f: 3.7718e-01 Norm: 1.0335e+00 equi: 2.2608e-02\n",
      "epoch: 2 - Total: 1.2728e+03 IC: 1.2364e+03 BC: 1.0061e+01 f: 2.6320e+02 Norm: 2.5093e-01 equi: 9.1657e-03\n",
      "epoch: 3 - Total: 2.1233e-01 IC: 1.4478e-01 BC: 1.1590e-02 f: 5.5962e-01 Norm: 1.0137e+00 equi: 2.2336e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL2.2454e-01 ...\n",
      "--- Starting trial: run-27\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 0.1, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 8.9122e+00 IC: 8.8648e+00 BC: 1.5811e-05 f: 4.7399e-01 Norm: 1.0654e+00 equi: 2.3259e-02\n",
      "epoch: 2 - Total: 6.0500e+02 IC: 6.0489e+02 BC: 4.6037e-04 f: 1.0840e+00 Norm: 4.9558e-01 equi: 1.3805e-02\n",
      "epoch: 3 - Total: 1.7364e+00 IC: 1.6764e+00 BC: 1.6434e-06 f: 6.0001e-01 Norm: 9.9262e-01 equi: 2.2208e-02\n",
      "epoch: 4 - Total: 1.7617e+00 IC: 1.6764e+00 BC: 1.8998e-03 f: 8.3321e-01 Norm: 9.9262e-01 equi: 2.2306e-02\n",
      "epoch: 5 - Total: 7.2863e+02 IC: 7.2615e+02 BC: 6.3335e-01 f: 1.8390e+01 Norm: 1.5663e+00 equi: 3.2405e-02\n",
      "epoch: 6 - Total: 1.0487e+00 IC: 9.9338e-01 BC: 3.1841e-03 f: 5.2134e-01 Norm: 1.0134e+00 equi: 2.2546e-02\n",
      "epoch: 7 - Total: 1.0748e+00 IC: 9.9338e-01 BC: 1.2365e-02 f: 6.9090e-01 Norm: 1.0134e+00 equi: 2.1763e-02\n",
      "epoch: 8 - Total: 4.4044e+02 IC: 4.3429e+02 BC: 1.7038e+00 f: 4.4480e+01 Norm: 5.6316e-01 equi: 1.3260e-02\n",
      "epoch: 9 - Total: 8.7406e-01 IC: 7.9808e-01 BC: 5.3708e-03 f: 7.0610e-01 Norm: 1.0008e+00 equi: 2.1543e-02\n",
      "epoch: 10 - Total: 9.0184e-01 IC: 7.9808e-01 BC: 1.1964e-02 f: 9.1795e-01 Norm: 1.0008e+00 equi: 2.0919e-02\n",
      "epoch: 11 - Total: 6.4676e+02 IC: 6.2808e+02 BC: 4.3358e+00 f: 1.4345e+02 Norm: 1.5204e+00 equi: 2.9339e-02\n",
      "epoch: 12 - Total: 6.9802e-01 IC: 6.1869e-01 BC: 1.5134e-02 f: 6.4198e-01 Norm: 1.0108e+00 equi: 2.1024e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL7.4856e-01 ...\n",
      "--- Starting trial: run-28\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 1.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 2.5370e+00 IC: 8.6673e-01 BC: 6.2045e-03 f: 1.6641e+00 Norm: 9.9980e-01 equi: 1.5596e-02\n",
      "epoch: 2 - Total: 1.9625e+03 IC: 1.5384e+03 BC: 1.0670e+01 f: 4.1343e+02 Norm: 2.2577e-01 equi: 2.0514e-02\n",
      "epoch: 3 - Total: 1.5042e+00 IC: 1.4384e+00 BC: 2.2133e-02 f: 4.3591e-02 Norm: 9.6410e-01 equi: 1.5787e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.5709e+00 ...\n",
      "--- Starting trial: run-29\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 1.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 8.1522e+01 IC: 7.6698e+01 BC: 1.2512e-04 f: 4.8237e+00 Norm: 8.6782e-01 equi: 2.4783e-02\n",
      "epoch: 2 - Total: 8.2439e+02 IC: 8.2240e+02 BC: 6.6204e-04 f: 1.9899e+00 Norm: 1.5799e+00 equi: 2.0919e-02\n",
      "epoch: 3 - Total: 9.0458e+00 IC: 7.7998e+00 BC: 1.8507e-05 f: 1.2460e+00 Norm: 1.0431e+00 equi: 2.3915e-02\n",
      "epoch: 4 - Total: 9.4114e+00 IC: 7.7998e+00 BC: 2.1395e-02 f: 1.5902e+00 Norm: 1.0431e+00 equi: 2.3158e-02\n",
      "epoch: 5 - Total: 5.5280e+02 IC: 5.5092e+02 BC: 5.0425e-01 f: 1.3723e+00 Norm: 4.5748e-01 equi: 6.9540e-03\n",
      "epoch: 6 - Total: 6.4816e+00 IC: 5.2614e+00 BC: 2.1204e-02 f: 1.1990e+00 Norm: 9.9524e-01 equi: 2.1629e-02\n",
      "epoch: 7 - Total: 8.0291e+00 IC: 5.2614e+00 BC: 8.2338e-02 f: 2.6853e+00 Norm: 9.9524e-01 equi: 2.0905e-02\n",
      "epoch: 8 - Total: 1.0478e+03 IC: 9.0248e+02 BC: 3.5444e+00 f: 1.4181e+02 Norm: 1.5749e+00 equi: 1.2167e-02\n",
      "epoch: 9 - Total: 5.2436e+00 IC: 3.9828e+00 BC: 4.0413e-02 f: 1.2203e+00 Norm: 1.0248e+00 equi: 2.0418e-02\n",
      "epoch: 10 - Total: 6.4854e+00 IC: 3.9828e+00 BC: 9.0027e-02 f: 2.4125e+00 Norm: 1.0248e+00 equi: 1.9902e-02\n",
      "epoch: 11 - Total: 5.8543e+02 IC: 4.9213e+02 BC: 2.6867e+00 f: 9.0614e+01 Norm: 5.3686e-01 equi: 8.0889e-03\n",
      "epoch: 12 - Total: 4.2540e+00 IC: 2.2328e+00 BC: 7.5957e-02 f: 1.9452e+00 Norm: 9.9398e-01 equi: 1.9039e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL4.4971e+00 ...\n",
      "--- Starting trial: run-30\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 10.0, 'Learning Type': 'standard'}\n",
      "epoch: 1 - Total: 7.3113e+01 IC: 1.5025e+01 BC: 3.4493e-01 f: 5.7743e+00 Norm: 9.1667e-01 equi: 1.4964e-02\n",
      "epoch: 2 - Total: 2.2243e+03 IC: 6.9472e+02 BC: 7.2062e+00 f: 1.5223e+02 Norm: 1.5827e+00 equi: 2.3541e-02\n",
      "epoch: 3 - Total: 1.3925e+01 IC: 1.3905e+00 BC: 5.0669e-02 f: 1.2483e+00 Norm: 9.9504e-01 equi: 1.5580e-02\n",
      "Saving ./models/MODEL_WEIGHTS_IC157_BC90_f35000_t0.08670795723907829_iter1_GridSearch_VL1.4110e+01 ...\n",
      "--- Starting trial: run-31\n",
      "{'Architecture Type': 'decreasing', 'Number of Layers': 4, 'Activation': 'tanh', 'MSE(f) Regularization': 10.0, 'Learning Type': 'transfer'}\n",
      "epoch: 1 - Total: 1.5566e+02 IC: 3.5848e+00 BC: 2.1240e-05 f: 1.5208e+01 Norm: 8.9909e-01 equi: 1.1363e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_1727/1385738351.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m                     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'--- Starting trial: %s'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mrun_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m                     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0mh\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mhparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mh\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mh\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mhparams\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m                     \u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'logs/'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mrun_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m                     \u001B[0msession_num\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_1727/1020471181.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(run_dir, hyperparams)\u001B[0m\n\u001B[1;32m     91\u001B[0m         \u001B[0mvalidation_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstandard_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhyperparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mhyperparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mHP_LEARN_TYPE\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'transfer'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 93\u001B[0;31m         \u001B[0mvalidation_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransfer_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhyperparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"LearningType:\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mhyperparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mHP_LEARN_TYPE\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_1727/1020471181.py\u001B[0m in \u001B[0;36mtransfer_train\u001B[0;34m(hyperparams)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0mpinn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprevious_pinn_weigths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0minit_params\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprevious_pinn_weigths\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m         results = scipy.optimize.minimize(fun=pinn.optimizerfunc,\n\u001B[0m\u001B[1;32m     65\u001B[0m                                           \u001B[0mx0\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minit_params\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m                                           \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/_minimize.py\u001B[0m in \u001B[0;36mminimize\u001B[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001B[0m\n\u001B[1;32m    621\u001B[0m                                   **options)\n\u001B[1;32m    622\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mmeth\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'l-bfgs-b'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 623\u001B[0;31m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001B[0m\u001B[1;32m    624\u001B[0m                                 callback=callback, **options)\n\u001B[1;32m    625\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mmeth\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'tnc'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/lbfgsb.py\u001B[0m in \u001B[0;36m_minimize_lbfgsb\u001B[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001B[0m\n\u001B[1;32m    358\u001B[0m             \u001B[0;31m# until the completion of the current minimization iteration.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    359\u001B[0m             \u001B[0;31m# Overwrite f and g:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 360\u001B[0;31m             \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc_and_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    361\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mtask_str\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mb'NEW_X'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    362\u001B[0m             \u001B[0;31m# new iteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36mfun_and_grad\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    265\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray_equal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    266\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_x_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 267\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    268\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36m_update_fun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    231\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_update_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    232\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_updated\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 233\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_fun_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    234\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_updated\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36mupdate_fun\u001B[0;34m()\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfun_wrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    138\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_fun_impl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36mfun_wrapped\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    132\u001B[0m             \u001B[0;31m# Overwriting results in undefined behaviour because\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m             \u001B[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 134\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    135\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/optimize.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, x, *args)\u001B[0m\n\u001B[1;32m     72\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0;34m\"\"\" returns the the function value \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_if_needed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/optimize/optimize.py\u001B[0m in \u001B[0;36m_compute_if_needed\u001B[0;34m(self, x, *args)\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjac\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m             \u001B[0mfg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjac\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_1727/1101286493.py\u001B[0m in \u001B[0;36moptimizerfunc\u001B[0;34m(self, parameters)\u001B[0m\n\u001B[1;32m    200\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m             \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m             total_loss, loss_ic, loss_bc, loss_f, loss_pr, loss_norm, loss_equi = self.loss(x_ic_train, p_ic_train,\n\u001B[0m\u001B[1;32m    203\u001B[0m                                                                                             x_bc_train, x_f_train)\n\u001B[1;32m    204\u001B[0m             \u001B[0mgrads\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtotal_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_1727/1101286493.py\u001B[0m in \u001B[0;36mloss\u001B[0;34m(self, x_ic, p_ic, x_bc, x_f)\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0mloss_ic\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_ic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_ic\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp_ic\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m         \u001B[0mloss_bc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_bc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_bc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m         \u001B[0mloss_f\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_pde\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_f\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m         \u001B[0mloss_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_ic\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_f\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m         \u001B[0mloss_norm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_norm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/2j/yj3x01td2lqbznzlbwcpz1pc0000gn/T/ipykernel_1727/1101286493.py\u001B[0m in \u001B[0;36mloss_pde\u001B[0;34m(self, collocation_points)\u001B[0m\n\u001B[1;32m    142\u001B[0m             \u001B[0moutput_p_collocation\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor_collocation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m             \u001B[0mp_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_p_collocation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_f\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m#inside the context bc we need it for higher derivative\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mp_t\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_p_collocation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt_f\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m         \u001B[0mp_xx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_f\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[0;32mdel\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36mgradient\u001B[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[1;32m   1079\u001B[0m                           for x in nest.flatten(output_gradients)]\n\u001B[1;32m   1080\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1081\u001B[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001B[0m\u001B[1;32m   1082\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1083\u001B[0m         \u001B[0mflat_targets\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py\u001B[0m in \u001B[0;36mimperative_grad\u001B[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[1;32m     65\u001B[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001B[0m\u001B[1;32m     68\u001B[0m       \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m       \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36m_gradient_function\u001B[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[1;32m    154\u001B[0m       \u001B[0mgradient_name_scope\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mforward_pass_name_scope\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"/\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgradient_name_scope\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 156\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    157\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py\u001B[0m in \u001B[0;36m_RealDivGrad\u001B[0;34m(op, grad)\u001B[0m\n\u001B[1;32m   1468\u001B[0m           array_ops.reshape(\n\u001B[1;32m   1469\u001B[0m               math_ops.reduce_sum(\n\u001B[0;32m-> 1470\u001B[0;31m                   grad * math_ops.realdiv(math_ops.realdiv(-x, y), y), ry), sy))  # pylint: disable=invalid-unary-operand-type\n\u001B[0m\u001B[1;32m   1471\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1472\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36mreal_div\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m   7869\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   7870\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 7871\u001B[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[1;32m   7872\u001B[0m         _ctx, \"RealDiv\", name, x, y)\n\u001B[1;32m   7873\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for archi_type in HP_ARCHI_TYPE.domain.values:\n",
    "    for num_layers in HP_NUM_LAYERS.domain.values:\n",
    "        for activ_func in HP_ACTIV_FUNC.domain.values:\n",
    "            for regu_pde_l in HP_REGU_PDE_L.domain.values:\n",
    "                for learn_type in HP_LEARN_TYPE.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_ARCHI_TYPE: archi_type,\n",
    "                        HP_NUM_LAYERS: num_layers,\n",
    "                        HP_ACTIV_FUNC: activ_func,\n",
    "                        HP_REGU_PDE_L: regu_pde_l,\n",
    "                        HP_LEARN_TYPE: learn_type\n",
    "                    }\n",
    "                    run_name = \"run-%d\" % session_num\n",
    "                    print('--- Starting trial: %s' % run_name)\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('logs/' + run_name, hparams)\n",
    "                    session_num += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
