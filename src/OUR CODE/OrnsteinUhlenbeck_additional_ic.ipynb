{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGNG42Iq3mZ-",
    "outputId": "4d3db688-cdd8-4c5c-f7bb-e66b111d553a"
   },
   "outputs": [],
   "source": [
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPPr5oba3j88",
    "outputId": "5cb21a1e-8894-4ebd-e658-2494be28417f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "import datetime, os\n",
    "#hide tf logs \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'} \n",
    "#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs,\n",
    "# and 3 to additionally filter out ERROR logs\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "#import seaborn as sns\n",
    "#import codecs, json\n",
    "import pandas as pd\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNpIlNHaRQG-"
   },
   "source": [
    "$\\frac{\\partial p(x,t)}{\\partial t} = \\frac{\\partial}{\\partial x}\n",
    "\\left[\\frac{m\\omega^2}{\\gamma} x \\,p(x,t)+ D \\frac{\\partial p(x,t)}{\\partial x}\\right] \\qquad A= \\frac{m\\omega^2}{\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88G3Lt8xn-Oo"
   },
   "source": [
    "# Data Prep\n",
    "\n",
    "The training and Testing data is prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC_Gn7mu3j9B"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# Defining the domain\n",
    "x_lower = -1\n",
    "x_upper = 1\n",
    "x = np.linspace(x_lower, x_upper, 256)\n",
    "dx = (x_upper - x_lower) / 256\n",
    "t_lower = 0\n",
    "t_upper = 0.1\n",
    "t = np.linspace(t_lower, t_upper, 100)\n",
    "dt = (t_upper - t_lower) / 100\n",
    "psol=np.zeros((256,100)) #NOT COMPUTED YET ############CHECK\n",
    "psol[:,0]=norm.pdf(x,0.5,0.05).T #INITIAL CONDITION\n",
    "\n",
    "# All domain points\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "A=10\n",
    "D=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyGxyaOAcqpi"
   },
   "source": [
    "## *Test Data*\n",
    "\n",
    "We take the numerical solutions of the ```fplanck``` package as the test data to compare against the solution produced by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yddknKA2Xohp"
   },
   "outputs": [],
   "source": [
    "''' X_u_test = [X[i],T[i]] [25600,2] for interpolation'''\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None])) # Need numerical solution here\n",
    "\n",
    "# Domain bounds\n",
    "low_bound = np.array([x_lower, t_lower]) \n",
    "up_bound = np.array([x_upper, t_upper])\n",
    "\n",
    "'''\n",
    "stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   p =  [25600x1]\n",
    "'''\n",
    "p = psol.flatten('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training Data*\n",
    "\n",
    "The boundary and initial conditions serve as the training data for the PINN. We also select the collocation points\n",
    "using *Latin Hypercube Sampling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UVJmvZbXjXb"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_bc, N_ic, N_f):\n",
    "    \"\"\" Initial Condition\"\"\"\n",
    "\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "    all_ic_x = np.vstack((X[0,:], T[0,:])).T\n",
    "    all_ic_p = psol[:,0].reshape(len(psol[:,0]),1)\n",
    "     \n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.vstack((X[:,0], T[:,0])).T\n",
    "    #bottomedge_p = psol[-1,:].reshape(len(psol[-1,:]),1)\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.vstack((X[:,-1], T[:,0])).T\n",
    "    #topedge_p = psol[0,:].reshape(len(psol[0,:]),1)\n",
    "\n",
    "\n",
    "    all_bc_x=np.vstack([bottomedge_x, topedge_x])\n",
    "    # Reflecting conditions do not use the value of p\n",
    "    #all_bc_p_train = np.vstack([ bottomedge_p, topedge_p])  \n",
    "\n",
    "    #choose random N_bc and N:ic points for training\n",
    "    index_bc = np.random.choice(all_bc_x.shape[0], N_bc, replace=False) \n",
    "    index_ic = np.random.choice(all_ic_x.shape[0], N_ic, replace=False)\n",
    "\n",
    "    x_bc_train = all_bc_x[index_bc, :] \n",
    "    x_ic_train = all_ic_x[index_ic, :] \n",
    "    p_ic_train = all_ic_p[index_ic,:]    \n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x_f_train = low_bound + (up_bound-low_bound)*lhs(2,N_f)\n",
    "    # Do we select boundary and initial points also for f calculation?\n",
    "    #x_f_train = np.vstack((x_f_train, x_bc_train, x_ic_train))\n",
    "    \n",
    "    '''Normalization Instants'''\n",
    "    x_norm_instant = np.vstack((X[50,:],\n",
    "                                T[50,:])).T\n",
    "    \n",
    "    return x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp4nc2S7bwzz"
   },
   "source": [
    "# **PINN**\n",
    "\n",
    "Generate a **PINN** of L hidden layers, each with n neurons. \n",
    "\n",
    "Initialization: ***Xavier***\n",
    "\n",
    "Activation: *tanh (x)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ivj5SRpG3j9F"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(tf.Module): \n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 low_bound, up_bound,\n",
    "                 x_ic_train, p_ic_train,\n",
    "                 x_bc_train,\n",
    "                 x_f_train,\n",
    "                 x_norm_instants,\n",
    "                 name = \"FP-PINN\",\n",
    "                 A = 10,\n",
    "                 D = 0.1,\n",
    "                 doPrint = True,\n",
    "                 additional_constraints = ()):\n",
    "       \n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0 #total number of parameters\n",
    "        self.epoch = 0\n",
    "        self.doPrint = doPrint\n",
    "        self.history = {\"epoch\": [],\n",
    "                        \"Total loss\": [],\n",
    "                        \"IC loss\": [],\n",
    "                        \"BC loss\": [],\n",
    "                        \"f loss\": [],\n",
    "                        \"Pr loss\": [],\n",
    "                        \"Norm loss\": [],\n",
    "                        \"Equi loss\": []}\n",
    "\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i+1]\n",
    "            \n",
    "            #Xavier standard deviation \n",
    "            std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
    "\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype = 'float64') * std_dv\n",
    "                       \n",
    "            w = tf.Variable(w, trainable=True, name = 'w' + str(i+1))\n",
    "\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype = 'float64'), trainable = True, name = 'b' + str(i+1))\n",
    "                    \n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "            \n",
    "            self.parameters +=  input_dim * output_dim + output_dim\n",
    "    \n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        \n",
    "        x = (x-low_bound)/(up_bound-low_bound) # Normalization\n",
    "        \n",
    "        a = x\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = tf.add(tf.matmul(a, self.W[2*i]), self.W[2*i+1])\n",
    "            a = tf.nn.tanh(z)\n",
    "            \n",
    "        a = tf.add(tf.matmul(a, self.W[-2]), self.W[-1]) # For regression, no activation to last layer\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def get_weights(self):\n",
    "\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "            \n",
    "            w_1d = tf.reshape(self.W[2*i],[-1])   #flatten weights \n",
    "            b_1d = tf.reshape(self.W[2*i+1],[-1]) #flatten biases\n",
    "            \n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0) #concat weights \n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0) #concat biases\n",
    "        \n",
    "        return parameters_1d\n",
    "        \n",
    "        \n",
    "    def set_weights(self,parameters):\n",
    "                \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            shape_w = tf.shape(self.W[2*i]).numpy() # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2*i]).numpy() #size of the weight tensor \n",
    "            \n",
    "            shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2*i+1]).numpy() #size of the bias tensor \n",
    "                        \n",
    "            pick_w = parameters[0:size_w] #pick the weights \n",
    "            self.W[2*i].assign(tf.reshape(pick_w,shape_w)) # assign  \n",
    "            parameters = np.delete(parameters,np.arange(size_w),0) #delete \n",
    "            \n",
    "            pick_b = parameters[0:size_b] #pick the biases \n",
    "            self.W[2*i+1].assign(tf.reshape(pick_b,shape_b)) # assign \n",
    "            parameters = np.delete(parameters,np.arange(size_b),0) #delete \n",
    "\n",
    "\n",
    "    def get_training_history(self):\n",
    "        return self.history\n",
    "\n",
    "    # Satisfy the IC\n",
    "    def loss_IC(self, ic_points, p_ic_train):\n",
    "        return tf.reduce_mean(tf.square(p_ic_train - self.evaluate(ic_points))) # MSE_ic\n",
    "\n",
    "\n",
    "    # Reflecting boundary\n",
    "    def loss_BC(self, bc_points):\n",
    "        variable_bc = tf.Variable(bc_points, dtype = 'float64', trainable = False)\n",
    "\n",
    "        x_bc = variable_bc[:,0:1]\n",
    "        t_bc = variable_bc[:,1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            tape.watch(x_bc)\n",
    "            tape.watch(t_bc)\n",
    "\n",
    "            tensor_bc = tf.stack([x_bc[:,0], t_bc[:,0]], axis=1)   \n",
    "\n",
    "            output_p_bc = self.evaluate(tensor_bc)\n",
    "        p_x = tape.gradient(output_p_bc,x_bc)  #more efficient out of the context\n",
    "\n",
    "        del tape\n",
    "\n",
    "        J = -1 * (A * x_bc * output_p_bc + D * p_x)\n",
    "\n",
    "        return tf.reduce_mean(tf.square(J)) # MSE_bc\n",
    "\n",
    "    # Satisfy the PDE at the collocation points\n",
    "    def loss_PDE(self, collocation_points):\n",
    "    \n",
    "        variable_collocation = tf.Variable(collocation_points, dtype = 'float64', trainable = False)\n",
    "\n",
    "        x_f = variable_collocation[:,0:1]\n",
    "        t_f = variable_collocation[:,1:2]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            tape.watch(x_f)\n",
    "            tape.watch(t_f)\n",
    "\n",
    "            tensor_collocation = tf.stack([x_f[:,0], t_f[:,0]], axis=1)   \n",
    "\n",
    "            output_p_collocation = self.evaluate(tensor_collocation)\n",
    "            p_x = tape.gradient(output_p_collocation,x_f) #inside the context bc we need it for higher derivative\n",
    "        p_t = tape.gradient(output_p_collocation,t_f)    \n",
    "        p_xx = tape.gradient(p_x, x_f)  \n",
    "\n",
    "        del tape\n",
    "\n",
    "        f = p_t - A * output_p_collocation - A * x_f * p_x - D * p_xx\n",
    "\n",
    "        return tf.reduce_mean(tf.square(f)) # MSE_f\n",
    "\n",
    "\n",
    "    # Satisfy probabilty norm at some instants\n",
    "    def loss_NORM(self, instants):\n",
    "        o = self.evaluate(instants)\n",
    "        return tf.abs(tf.reduce_sum(o) * dx - 1.0) # ME |norm - 1|\n",
    "\n",
    "    # Must be Boltzmann at t >> 1 with ÃŸ * m * w ** 2 = A / D\n",
    "    def loss_EQUI(self):\n",
    "        t_large = (1 / A) * 10 * np.ones(256).reshape(256,1) # Typical time is 1 / A.\n",
    "        x_domain = np.linspace(low_bound[0], up_bound[0], 256).reshape(256, 1)\n",
    "        x_at_large_t = tf.stack([x_domain[:,0], t_large[:,0]], axis=1)\n",
    "        output = self.evaluate(x_at_large_t)\n",
    "        boltzmann_dist = tf.exp((-A/D) * x_domain ** 2)\n",
    "        z = tf.reduce_sum(boltzmann_dist) * dx\n",
    "        boltzmann_dist = boltzmann_dist / z\n",
    "        # L2 norm (Boltzmann_dist - output)\n",
    "        return tf.reduce_mean(tf.square(boltzmann_dist - output) * dx)\n",
    "\n",
    "\n",
    "    # Satisfy p > 0 at IC and the collocation points\n",
    "    def loss_PROB(self, ic_points, collocation_points):\n",
    "        o1 = self.evaluate(ic_points)\n",
    "        o2 = self.evaluate(collocation_points)\n",
    "        negatives = tf.where(tf.greater_equal(o1, 0.),\n",
    "                             tf.zeros_like(o1),\n",
    "                             o1)\n",
    "        loss_pr = tf.abs(tf.reduce_mean(negatives)) # MSE (p < 0) at IC\n",
    "        negatives = tf.where(tf.greater_equal(o2, 0.),\n",
    "                             tf.zeros_like(o2),\n",
    "                             o2)\n",
    "        loss_pr = loss_pr + tf.abs(tf.reduce_mean(negatives)) # MSE (p < 0) at collocation\n",
    "        return loss_pr\n",
    "\n",
    "\n",
    "    def loss(self):\n",
    "        loss_ic = self.loss_IC(x_ic_train, p_ic_train)\n",
    "        loss_bc = self.loss_BC(x_bc_train)\n",
    "        loss_f = self.loss_PDE(x_f_train)\n",
    "        loss_prob = self.loss_PROB(x_ic_train, x_f_train)\n",
    "        loss_norm = self.loss_NORM(x_norm_instants)\n",
    "        loss_equi = self.loss_EQUI()\n",
    "        loss = loss_ic + loss_bc + loss_f\n",
    "        if \"prob\" in additional_constraints:\n",
    "            loss = loss + loss_prob\n",
    "        if \"norm\" in additional_constraints:\n",
    "            loss = loss + loss_norm\n",
    "        if \"equi\" in additional_constraints:\n",
    "            loss = loss + loss_equi\n",
    "        return loss, loss_ic, loss_bc, loss_f, loss_prob, loss_norm, loss_equi\n",
    "    \n",
    "    \n",
    "    def optimizerfunc(self,parameters):\n",
    "        \n",
    "        self.set_weights(parameters)\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            \n",
    "            total_loss, loss_ic, loss_bc, loss_f, loss_pr, loss_norm, loss_equi = self.loss()\n",
    "            grads = tape.gradient(total_loss,self.trainable_variables)\n",
    "        if self.doPrint:\n",
    "            self.epoch += 1\n",
    "            tf.print(f\"epoch: {self.epoch}\", f\"- Total: {total_loss:5.4e}\", f\"IC: {loss_ic:5.4e}\",\n",
    "                     f\"BC: {loss_bc:5.4e}\", f\"f: {loss_f:5.4e}\", f\"Norm: {loss_norm:5.4e}\", f\"equi: {loss_equi:5.4e}\")\n",
    "                \n",
    "        del tape\n",
    "        \n",
    "        grads_1d = [ ] #flatten grads \n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            grads_w_1d = tf.reshape(grads[2*i],[-1]) #flatten weights \n",
    "            grads_b_1d = tf.reshape(grads[2*i+1],[-1]) #flatten biases\n",
    "\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0) #concat grad_weights \n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0) #concat grad_biases\n",
    "\n",
    "        self.history[\"epoch\"].append(self.epoch)\n",
    "        self.history[\"Total loss\"].append(float(total_loss))\n",
    "        self.history[\"IC loss\"].append(float(loss_ic))\n",
    "        self.history[\"BC loss\"].append(float(loss_bc))\n",
    "        self.history[\"f loss\"].append(float(loss_f))\n",
    "        self.history[\"Pr loss\"].append(float(loss_pr))\n",
    "        self.history[\"Norm loss\"].append(float(loss_norm))\n",
    "        self.history[\"Equi loss\"].append(float(loss_equi))\n",
    "\n",
    "        return total_loss.numpy(), grads_1d.numpy()\n",
    "    \n",
    "    def optimizer_callback(self,parameters):\n",
    "               return None\n",
    "        # Not optimized to call loss() twice per epoch\n",
    "        #total_loss, loss_ic, loss_bc, loss_f = self.loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWqNuRMLhg4m"
   },
   "outputs": [],
   "source": [
    "def solutionplot(p_pred,X_p_train):#,p_train):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.axis('off')\n",
    "\n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.05, bottom=1-1/3, left=0.1, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "    h = ax.imshow(p_pred, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[T.min(), T.max(), X.min(), X.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "    \n",
    "    #ax.plot(X_p_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "    #line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    #ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$p(x,t)$', fontsize = 10)\n",
    "\n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n",
    "    '''\n",
    "    \n",
    "    ####### Row 1: u(t,x) slices ##################\n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(top=2/3-1/5, bottom=0.05, left=0.2, right=0.8, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    #ax.plot(x,usol.T[0,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,p_pred.T[0,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(x,t)$')    \n",
    "    ax.set_title('$t = 0.s$', fontsize = 10)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-0.1,9])\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    #ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,p_pred.T[50,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(x,t)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-0.1,9])\n",
    "    ax.set_title('$t = 0.05s$', fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 2])\n",
    "    #ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,p_pred.T[75,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-0.1,9])    \n",
    "    ax.set_title('$t = 0.075s$', fontsize = 10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('Ornstein-Uhlenbeck.png',dpi = 500)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRuuEXx-eeWa"
   },
   "source": [
    "# *Model Training and Testing*\n",
    "\n",
    "A function '**model**' is defined to generate a NN as per the input set of hyperparameters, which is then trained and tested. The L2 Norm of the solution error is returned as a comparison metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyH2oLRH3j9K"
   },
   "outputs": [],
   "source": [
    "N_bc = 100 #Total number of boundary conditions points\n",
    "N_ic = 100 #Total number of initial condition points\n",
    "N_f = 1000 #Total number of collocation points\n",
    "\n",
    "# Training data\n",
    "x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instants = trainingdata(N_bc, N_ic, N_f)\n",
    "\n",
    "layers = np.array([2,20,20,20,20,20,20,20,20,1]) #8 hidden layers\n",
    "\n",
    "# Put 'norm', 'prob' or 'equi' on the following tuple to add to the total loss the normalization loss,\n",
    "#   negative probability loss, or Boltzmann equilibrium distribution loss respectively.\n",
    "additional_constraints = [\"equi\"]\n",
    "\n",
    "PINN = Sequentialmodel(layers, low_bound, up_bound,\n",
    "                       x_ic_train, p_ic_train,\n",
    "                       x_bc_train,\n",
    "                       x_f_train,\n",
    "                       x_norm_instants,\n",
    "                       additional_constraints=additional_constraints)\n",
    "\n",
    "init_params = PINN.get_weights().numpy()\n",
    "\n",
    "start_time = time.time() \n",
    "# train the model with Scipy L-BFGS optimizer\n",
    "#tfp.optimizer.lbfgs_minimize\n",
    "maxiter = 5000\n",
    "results = scipy.optimize.minimize(fun = PINN.optimizerfunc, \n",
    "                                  x0 = init_params, \n",
    "                                  args=(), \n",
    "                                  method='L-BFGS-B', \n",
    "                                  jac= True,        # If jac is True, fun is assumed to return the gradient along with the objective function\n",
    "                                  #callback = PINN.optimizer_callback, \n",
    "                                  options = {'disp': None,\n",
    "                                            'maxcor': 200, \n",
    "                                            'ftol': 1 * np.finfo(float).eps,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                            'gtol': 5e-8,\n",
    "                                            'maxfun':  50000, \n",
    "                                            'maxiter': maxiter,\n",
    "                                            'iprint': -1,   #print update every 50 iterations\n",
    "                                            'maxls': 50})\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % elapsed)\n",
    "\n",
    "print(results)\n",
    "\n",
    "PINN.set_weights(results.x)\n",
    "\n",
    "''' Model Accuracy '''\n",
    "p_pred = PINN.evaluate(X_p_test)\n",
    "\n",
    "\n",
    "p_pred = np.reshape(p_pred,(256,100),order='F')                        # Fortran Style ,stacked column wise!\n",
    "\n",
    "''' Solution Plot '''\n",
    "solutionplot(p_pred,X_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save train history\n",
    "TRAIN_HISTORY = PINN.get_training_history()\n",
    "data_filename = \"./data/TRAIN_HISTORY_\"\n",
    "if len(additional_constraints) != 0:\n",
    "    for constrain in additional_constraints:\n",
    "        data_filename = data_filename + constrain + \"_\"\n",
    "data_filename = data_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper}_iter{maxiter}\"\n",
    "pd.DataFrame(TRAIN_HISTORY).to_csv(data_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Comparison with numerical solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for large t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "zbd1nLwqKYJw",
    "outputId": "1e6172fb-f9d1-4a30-c2d8-98159db4f2e7"
   },
   "outputs": [],
   "source": [
    "x_lower = -1\n",
    "x_upper = 1\n",
    "x = np.linspace(x_lower, x_upper, 256)\n",
    "t_lower = 0\n",
    "t_upper = 10\n",
    "t = np.linspace(t_lower, t_upper, 1000)\n",
    "psol=np.zeros((256,1000)) #NOT COMPUTED YET\n",
    "psol[:,0]=norm.pdf(x,0.5,0.05).T #INITIAL CONDITION\n",
    "\n",
    "#collocation points for every position and every time\n",
    "X, T = np.meshgrid(x,t)\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "p_pred = PINN.evaluate(X_p_test)\n",
    "\n",
    "\n",
    "p_pred = np.reshape(p_pred,(256,1000),order='F')\n",
    "solutionplot(p_pred,X_p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_filename = \"./data/TRAIN_HISTORY_equi_IC100_BC100_f1000_t0.1_iter5000\"\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Training Loss\")\n",
    "df = pd.read_csv(data_filename)\n",
    "plt.plot(df[\"epoch\"], df[\"Total loss\"], label=\"Total Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"BC loss\"], label=\"$MSE_{bc}$\")\n",
    "plt.plot(df[\"epoch\"], df[\"f loss\"], label=\"$MSE_f$\")\n",
    "plt.plot(df[\"epoch\"], df[\"IC loss\"], label=\"$MSE_{ic}$\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Training Norm Loss (not trained for)\")\n",
    "#plt.plot(df[\"epoch\"], df[\"Pr loss\"], label = \"Probability Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"Norm loss\"], label = \"Norm Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Probability Loss (not trained for)\")\n",
    "plt.plot(df[\"epoch\"], df[\"Pr loss\"], label = \"Probability Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim((10 ** -14, 10 ** 1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Equilibrium Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"Norm loss\"], label = \"Equilibrium Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OrnsteinUhlenbeck_lateral_ic.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a6b1c1e0b4cc2c7b94758dc48ca43f4e1f573c11345aa471dd73649661b4f98a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}