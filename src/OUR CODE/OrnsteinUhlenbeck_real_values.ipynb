{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPPr5oba3j88",
    "outputId": "5cb21a1e-8894-4ebd-e658-2494be28417f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Hide tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'} \n",
    "# 0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs,\n",
    "#     and 3 to additionally filter out ERROR logs.\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "from pyDOE import lhs  #Latin Hypercube Sampling\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNpIlNHaRQG-"
   },
   "source": [
    "$\\frac{\\partial p(x,t)}{\\partial t} =\n",
    "\\frac{\\partial}{\\partial x}\\left[\\frac{m\\omega^2}{\\gamma} x \\,p(x,t) + D \\frac{\\partial p(x,t)}{\\partial x}\\right]\n",
    "\\qquad A= \\frac{m\\omega^2}{\\gamma}= \\frac{k}{\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88G3Lt8xn-Oo"
   },
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC_Gn7mu3j9B"
   },
   "outputs": [],
   "source": [
    "from scipy import constants\n",
    "\n",
    "Temperature = 300  # K\n",
    "viscosity = 1e-3  # Kg / (m * s)\n",
    "radius = 20e-10  # m\n",
    "gamma = 6 * np.pi * viscosity * radius  # Kg / s\n",
    "k = 3e-7  # Kg / s^2; 10GHz for frequency and 3e-26 Kg for mass\n",
    "A = k / gamma  # s^-1\n",
    "D = Temperature * constants.k / gamma  # m^2 / s\n",
    "print(f\"{gamma = } [Kg/s]\")\n",
    "print(f\"{k = } [Kg/s^2]\")\n",
    "print(f\"kB = {constants.k} [J/K]\")\n",
    "print(f\"{A = :5.4e} [s^-1],\\t{D = :4.4e} [m^2/s]\")\n",
    "# Change of units m -> µm\n",
    "#                 s -> ms\n",
    "# such that A ~ O(1), D ~ O(0.1)\n",
    "A = A * 1e-3  # s^-1 -> ms^-1\n",
    "D = D * 1e-3 * 1e+12  # m^2/s -> µm^2/ms\n",
    "print(f\"{A = :4.4e} [ms^-1],\\t{D = :4.4e} [µm^2/ms]\")\n",
    "# Collocation points for every position and every time\n",
    "x_lower = -0.1\n",
    "x_upper = 0.25\n",
    "x_steps = 350\n",
    "dx = (x_upper - x_lower) / x_steps\n",
    "x = np.linspace(x_lower, x_upper, x_steps)  # µm line length\n",
    "t_lower = 0\n",
    "t_upper = 0.69 / A  # Typical Ornstein-Uhlenbeck time is ln(2) / A\n",
    "N_steps = 200\n",
    "t = np.linspace(t_lower, t_upper, N_steps)  # ms time interval\n",
    "X, T = np.meshgrid(x, t)\n",
    "# Initial Condition from imported numerical solution\n",
    "real = pickle.load(open('./simulations/numerical_solution.sav', 'rb'))\n",
    "psol=np.zeros((len(x),len(t)))\n",
    "psol[:,0]=real[0]\n",
    "# Max epochs in training\n",
    "maxiter = 10000\n",
    "# Total number of boundary conditions points\n",
    "N_bc = 100\n",
    "# Total number of initial condition points\n",
    "N_ic = 100\n",
    "# Total number of collocation points\n",
    "N_f = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyGxyaOAcqpi"
   },
   "source": [
    "## *Test Data*\n",
    "We take the numerical solutions of the ```fplanck``` package as the test data to compare against the solution produced\n",
    "by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yddknKA2Xohp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' X_u_test = [X[i],T[i]] [x_steps * N_steps, 2] for interpolation'''\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "# Domain bounds\n",
    "low_bound = np.array([x_lower, t_lower])\n",
    "up_bound = np.array([x_upper, t_upper])\n",
    "\n",
    "'''\n",
    "stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [x_steps * N_steps, 1]\n",
    "'''\n",
    "p = psol.flatten('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UVJmvZbXjXb",
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *Training Data*\n",
    "\n",
    "The boundary and initial conditions serve as the training data for the PINN. We also select the collocation points\n",
    "using *Latin Hypercube Sampling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainingdata(n_bc, n_ic, n_f):\n",
    "    \"\"\" Initial Condition\"\"\"\n",
    "\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "    all_ic_x = np.vstack((X[0, :], T[0, :])).T\n",
    "    all_ic_p = psol[:, 0].reshape(len(psol[:, 0]), 1)\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.vstack((X[:, 0], T[:, 0])).T\n",
    "    #bottomedge_p = psol[-1, :].reshape(len(psol[-1, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.vstack((X[:, -1], T[:, 0])).T\n",
    "    #topedge_p = psol[0, :].reshape(len(psol[0, :]), 1) # Not needed in reflecting boundaries\n",
    "\n",
    "    all_bc_x = np.vstack([bottomedge_x, topedge_x])\n",
    "    # Reflecting conditions do not use the value of p\n",
    "    #all_bc_p_train = np.vstack([ bottomedge_p, topedge_p])  \n",
    "\n",
    "    #choose random N_bc and N:ic points for training\n",
    "    index_bc = np.random.choice(all_bc_x.shape[0], n_bc, replace=False)\n",
    "    index_ic = np.random.choice(all_ic_x.shape[0], n_ic, replace=False)\n",
    "\n",
    "    _x_bc_train = all_bc_x[index_bc, :]\n",
    "    _x_ic_train = all_ic_x[index_ic, :]\n",
    "    _p_ic_train = all_ic_p[index_ic, :]\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    _x_f_train = low_bound + (up_bound - low_bound) * lhs(2, n_f)\n",
    "    # Do we select boundary and initial points also for f calculation?\n",
    "    #   Only god knows \n",
    "    #x_f_train = np.vstack((x_f_train, x_bc_train, x_ic_train))\n",
    "\n",
    "    '''Normalization Instants'''\n",
    "    # N_steps is the global variable for the time steps of the domain\n",
    "    _x_norm_instant = np.vstack((X[int(N_steps / 2), :],\n",
    "                                T[int(N_steps / 2), :])).T\n",
    "\n",
    "    return _x_f_train, _x_bc_train, _x_ic_train, _p_ic_train, _x_norm_instant"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp4nc2S7bwzz"
   },
   "source": [
    "# **PINN implementation**\n",
    "\n",
    "Generate a **PINN** of L hidden layers, each with n neurons. \n",
    "\n",
    "Initialization: ***Xavier***\n",
    "\n",
    "Activation: *tanh (x)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_features = [\"xLower-0.1\", \"xUpper0.25\", \"micrometers_miliseconds\", \"k3e-7\", \"r1e-20\"]\n",
    "print(\"Training with extra features:\", extra_features)\n",
    "additional_constraints = []\n",
    "print(\"Training with extra constraints:\", additional_constraints)\n",
    "\n",
    "\n",
    "def save_train_history(neural_network, name=None):\n",
    "    history_filename = \"./data/TRAIN_HISTORY_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            history_filename = history_filename + constrain + \"_\"\n",
    "    history_filename = history_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper:4.4f}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            history_filename = history_filename + \"_\" + feat\n",
    "    if name is not None:\n",
    "        history_filename = history_filename + \"_\" + name\n",
    "    print(\"Saving\", history_filename, \"...\")\n",
    "    neural_network.history[\"path\"] = history_filename\n",
    "    history = neural_network.get_training_history()\n",
    "    pd.DataFrame(history).to_csv(history_filename + \".csv\")\n",
    "\n",
    "\n",
    "def save_model_weights(neural_network, name=None):\n",
    "    model_filename = \"./models/MODEL_WEIGHTS_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            model_filename = model_filename + constrain + \"_\"\n",
    "    model_filename = model_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper:4.4f}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            model_filename = model_filename + \"_\" + feat\n",
    "    if name is not None:\n",
    "        model_filename = model_filename + \"_\" + name\n",
    "    print(\"Saving\", model_filename, \"...\")\n",
    "    np.savetxt(model_filename + \".txt\", neural_network.get_weights().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ivj5SRpG3j9F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PINN(tf.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 do_print=True,\n",
    "                 _additional_constraints=(),\n",
    "                 a=10,\n",
    "                 d=0.1,\n",
    "                 f_regularization=1.0,\n",
    "                 activation_function='tanh'):\n",
    "        # The IC, BC, collocation points as well as p(IC points) and the domain upper/lower bounds are global variables,\n",
    "        #     because they define the domain of the learning problem for our model,\n",
    "        #     therefore they are required for our PINN class to be instantiated.\n",
    "        for _global_variable in ['x_f_train', 'x_bc_train', 'x_ic_train', 'p_ic_train', 'x_norm_instants', 'low_bound',\n",
    "                                'up_bound']:\n",
    "            if _global_variable not in globals():\n",
    "                raise ValueError(\"MissingGlobalVariable: \" + _global_variable)\n",
    "        super(PINN, self).__init__(name=\"PINN\")\n",
    "        self.layers = layers\n",
    "        self.additional_constraints = _additional_constraints\n",
    "        if activation_function == 'tanh':\n",
    "            self.activation_function = tf.nn.tanh\n",
    "        elif activation_function == 'swish':\n",
    "            self.activation_function = tf.nn.swish\n",
    "        else:\n",
    "            raise ValueError(\"ActivationFunction:\" + activation_function)\n",
    "        self.a = a\n",
    "        self.d = d\n",
    "        self.f_regularization = f_regularization\n",
    "        self.epoch = 0\n",
    "        self.do_print = do_print\n",
    "        self.save_training_after_n = 1000\n",
    "        self.history = {\"path\": \"\",\n",
    "                        \"epoch\": [],\n",
    "                        \"Total loss\": [],\n",
    "                        \"IC loss\": [],\n",
    "                        \"BC loss\": [],\n",
    "                        \"f loss\": [],\n",
    "                        \"Pr loss\": [],\n",
    "                        \"Norm loss\": [],\n",
    "                        \"Equi loss\": []}\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0  #total number of parameters\n",
    "        for i in range(len(layers) - 1):\n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i + 1]\n",
    "            #Xavier standard deviation\n",
    "            std_dv = np.sqrt((2.0 / (input_dim + output_dim)))\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype='float64') * std_dv\n",
    "            w = tf.Variable(w, trainable=True, name='w' + str(i + 1))\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype='float64'), trainable=True, name='b' + str(i + 1))\n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "            self.parameters += input_dim * output_dim + output_dim\n",
    "\n",
    "    def evaluate(self, subset):\n",
    "        layer_input = (subset - low_bound) / (up_bound - low_bound)  # Normalization\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            layer_output = tf.add(tf.matmul(layer_input, self.W[2 * i]), self.W[2 * i + 1])\n",
    "            layer_input = self.activation_function(layer_output)\n",
    "        layer_output = tf.add(tf.matmul(layer_input, self.W[-2]), self.W[-1])  # Regression: no activation to last layer\n",
    "        return layer_output\n",
    "\n",
    "    def get_weights(self):\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            w_1d = tf.reshape(self.W[2 * i], [-1])  #flatten weights\n",
    "            b_1d = tf.reshape(self.W[2 * i + 1], [-1])  #flatten biases\n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0)  #concat weights\n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0)  #concat biases\n",
    "        return parameters_1d\n",
    "\n",
    "    def set_weights(self, parameters):\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            shape_w = tf.shape(self.W[2 * i]).numpy()  # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2 * i]).numpy()  #size of the weight tensor\n",
    "            shape_b = tf.shape(self.W[2 * i + 1]).numpy()  # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2 * i + 1]).numpy()  #size of the bias tensor\n",
    "            pick_w = parameters[0:size_w]  #pick the weights\n",
    "            self.W[2 * i].assign(tf.reshape(pick_w, shape_w))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_w), 0)  #delete\n",
    "            pick_b = parameters[0:size_b]  #pick the biases\n",
    "            self.W[2 * i + 1].assign(tf.reshape(pick_b, shape_b))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_b), 0)  #delete\n",
    "\n",
    "    def set_training_history(self, path):\n",
    "        history = pd.read_csv(path)\n",
    "        self.history[\"epoch\"] = list(history[\"epoch\"])\n",
    "        if self.history[\"epoch\"] is not []:\n",
    "            self.epoch = self.history[\"epoch\"]\n",
    "        self.history[\"Total loss\"] = list(history[\"Total loss\"])\n",
    "        self.history[\"IC loss\"] = list(history[\"IC loss\"])\n",
    "        self.history[\"BC loss\"] = list(history[\"BC loss\"])\n",
    "        self.history[\"f loss\"] = list(history[\"f loss\"])\n",
    "        self.history[\"Pr loss\"] = list(history[\"Pr loss\"])\n",
    "        self.history[\"Norm loss\"] = list(history[\"Norm loss\"])\n",
    "        self.history[\"Equi loss\"] = list(history[\"Equi loss\"])\n",
    "\n",
    "    def get_training_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_epoch(self):\n",
    "        return self.epoch\n",
    "\n",
    "    def set_pde_params(self, a, d):\n",
    "        self.a = a\n",
    "        self.d = d\n",
    "\n",
    "    # Satisfy the IC\n",
    "    def loss_ic(self, x_ic, p_ic):\n",
    "        # Relative MSE\n",
    "        return tf.reduce_mean(tf.square(p_ic - self.evaluate(x_ic))) / tf.reduce_sum(tf.square(p_ic))\n",
    "\n",
    "    # Satisfy the reflecting boundary\n",
    "    def loss_bc(self, boundary_points):\n",
    "        variable_bc = tf.Variable(boundary_points, dtype='float64', trainable=False)\n",
    "        x_bc = variable_bc[:, 0:1]\n",
    "        t_bc = variable_bc[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_bc)\n",
    "            tape.watch(t_bc)\n",
    "            tensor_bc = tf.stack([x_bc[:, 0], t_bc[:, 0]], axis=1)\n",
    "            output_p_bc = self.evaluate(tensor_bc)\n",
    "        p_x = tape.gradient(output_p_bc, x_bc)  #more efficient out of the context\n",
    "        del tape\n",
    "        flux = -1 * (self.a * x_bc * output_p_bc + self.d * p_x)\n",
    "        return tf.reduce_mean(tf.square(flux))  # MSE_bc\n",
    "\n",
    "    # Satisfy the PDE at the collocation points\n",
    "    def loss_pde(self, collocation_points):\n",
    "        variable_collocation = tf.Variable(collocation_points, dtype='float64', trainable=False)\n",
    "        x_f = variable_collocation[:, 0:1]\n",
    "        t_f = variable_collocation[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_f)\n",
    "            tape.watch(t_f)\n",
    "            tensor_collocation = tf.stack([x_f[:, 0], t_f[:, 0]], axis=1)\n",
    "            output_p_collocation = self.evaluate(tensor_collocation)\n",
    "            p_x = tape.gradient(output_p_collocation, x_f)  #inside the context bc we need it for higher derivative\n",
    "        p_t = tape.gradient(output_p_collocation, t_f)\n",
    "        p_xx = tape.gradient(p_x, x_f)\n",
    "        del tape\n",
    "        f = p_t - self.a * output_p_collocation - self.a * x_f * p_x - self.d * p_xx\n",
    "        return tf.reduce_mean(tf.square(f))  # MSE_f\n",
    "\n",
    "    # Satisfy |probability| = 1 at some instants\n",
    "    def loss_norm(self):\n",
    "        o = self.evaluate(x_norm_instants)\n",
    "        return tf.abs(tf.reduce_sum(o) * dx - 1.0)  # ME |norm - 1|\n",
    "\n",
    "    # Must be Boltzmann distributed at t >> 1 with ß * m * w ^ 2 = A / D\n",
    "    def loss_equi(self):\n",
    "        # Typical time is ln(2) / A ≈ 0.69 / A\n",
    "        t_large = 10 * (0.69 / self.a) * np.ones(256).reshape(256, 1)\n",
    "        x_domain = np.linspace(low_bound[0], up_bound[0], 256).reshape(256, 1)\n",
    "        x_at_large_t = tf.stack([x_domain[:, 0], t_large[:, 0]], axis=1)\n",
    "        output = self.evaluate(x_at_large_t)\n",
    "        boltzmann_dist = tf.exp(-1 * (self.a / (2 * self.d)) * x_domain ** 2)\n",
    "        z = tf.reduce_sum(boltzmann_dist) * dx\n",
    "        boltzmann_dist = boltzmann_dist / z\n",
    "        # L2 norm (Boltzmann_dist - output)\n",
    "        return tf.reduce_mean(tf.square(boltzmann_dist - output) * dx)\n",
    "\n",
    "    # Satisfy p > 0 at IC and the collocation points\n",
    "    def loss_prob(self, x_ic, x_f):\n",
    "        o1 = self.evaluate(x_ic)\n",
    "        o2 = self.evaluate(x_f)\n",
    "        negatives = tf.where(tf.greater_equal(o1, 0.),\n",
    "                             tf.zeros_like(o1),\n",
    "                             o1)\n",
    "        loss_pr = tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at IC\n",
    "        negatives = tf.where(tf.greater_equal(o2, 0.),\n",
    "                             tf.zeros_like(o2),\n",
    "                             o2)\n",
    "        loss_pr = loss_pr + tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at collocation\n",
    "        return loss_pr\n",
    "\n",
    "    def loss(self, x_ic, p_ic, x_bc, x_f):\n",
    "        loss_ic = self.loss_ic(x_ic, p_ic)\n",
    "        loss_bc = self.loss_bc(x_bc)\n",
    "        loss_f = self.loss_pde(x_f)\n",
    "        loss_prob = self.loss_prob(x_ic, x_f)\n",
    "        loss_norm = self.loss_norm()\n",
    "        loss_equi = self.loss_equi()\n",
    "        loss = loss_ic + loss_bc + self.f_regularization * loss_f\n",
    "        if \"prob\" in self.additional_constraints:\n",
    "            loss = loss + loss_prob\n",
    "        if \"norm\" in self.additional_constraints:\n",
    "            loss = loss + loss_norm\n",
    "        if \"equi\" in self.additional_constraints:\n",
    "            loss = loss + loss_equi\n",
    "        return loss, loss_ic, loss_bc, loss_f, loss_prob, loss_norm, loss_equi\n",
    "\n",
    "    def optimizerfunc(self, parameters):\n",
    "        self.set_weights(parameters)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            total_loss, loss_ic, loss_bc, loss_f, loss_pr, loss_norm, loss_equi = self.loss(x_ic_train, p_ic_train,\n",
    "                                                                                            x_bc_train, x_f_train)\n",
    "            grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.epoch += 1 # This has been an iteration of the training process.\n",
    "        if self.do_print:\n",
    "            tf.print(f\"epoch: {self.epoch}\", f\"- Total: {total_loss:5.4e}\", f\"IC: {loss_ic:5.4e}\",\n",
    "                     f\"BC: {loss_bc:5.4e}\", f\"f: {loss_f:5.4e}\", f\"Norm: {loss_norm:5.4e}\", f\"equi: {loss_equi:5.4e}\")\n",
    "        del tape\n",
    "        grads_1d = []  #flatten grads\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            grads_w_1d = tf.reshape(grads[2 * i], [-1])  #flatten weights\n",
    "            grads_b_1d = tf.reshape(grads[2 * i + 1], [-1])  #flatten biases\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0)  #concat grad_weights\n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0)  #concat grad_biases\n",
    "        self.history[\"epoch\"].append(self.epoch)\n",
    "        self.history[\"Total loss\"].append(float(total_loss))\n",
    "        self.history[\"IC loss\"].append(float(loss_ic))\n",
    "        self.history[\"BC loss\"].append(float(loss_bc))\n",
    "        self.history[\"f loss\"].append(float(loss_f))\n",
    "        self.history[\"Pr loss\"].append(float(loss_pr))\n",
    "        self.history[\"Norm loss\"].append(float(loss_norm))\n",
    "        self.history[\"Equi loss\"].append(float(loss_equi))\n",
    "        if self.epoch % self.save_training_after_n == 0:\n",
    "            save_train_history(self)\n",
    "            save_model_weights(self)\n",
    "        return total_loss.numpy(), grads_1d.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOjuHdzAhib-"
   },
   "source": [
    "# *Model Training and Testing*\n",
    "\n",
    "A function '**model**' is defined to generate a NN as per the input set of hyperparameters, which is then trained and\n",
    "tested.\n",
    "\n",
    "## Solution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWqNuRMLhg4m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solutionplot(u_pred):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "\n",
    "    gs0 = gridspec.GridSpec(2, 3)\n",
    "    gs0.update(top=1, bottom=0, left=0.1, right=2, wspace=0.3, hspace=0.4)\n",
    "    ax = plt.subplot(gs0[0, :])\n",
    "\n",
    "    h = ax.imshow(u_pred, interpolation='nearest', cmap='rainbow',\n",
    "                  extent=[T.min(), T.max(), X.min(), X.max()],\n",
    "                  origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "\n",
    "    #ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4,\n",
    "    # clip_on = False)\n",
    "\n",
    "    #line = np.linspace(x.min(), x.max(), 2)[:, None]\n",
    "    #ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.set_title('$u(x,t)$', fontsize=10)\n",
    "\n",
    "    ''' \n",
    "    Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n",
    "    '''\n",
    "\n",
    "    ####### Row 1: u(t,x) slices ##################\n",
    "    #gs1 = gridspec.GridSpec(1, 3)\n",
    "    #gs1.update(top=0.3, bottom=-0.1, left=0.1, right=2, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 0])\n",
    "    #ax.plot(x,usol.T[0,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x, u_pred.T[0, :], 'r', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    ax.set_title('$t = 0.s$', fontsize=10)\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([-0.1, 9])\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 1])\n",
    "    #ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x, u_pred.T[int(0.5 * len(t)), :], 'r', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([-0.1, 9])\n",
    "    ax.set_title('$t = {}s$'.format(int(0.5 * len(t)) / 100), fontsize=10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 2])\n",
    "    #ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x, u_pred.T[int(0.75 * len(t)), :], 'r', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([-0.1, 9])\n",
    "    ax.set_title('$t = {}s$'.format(int(0.75 * len(t)) / 100), fontsize=10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig('Ornstein-Uhlenbeck.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRuuEXx-eeWa",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyH2oLRH3j9K",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instants = trainingdata(N_bc, N_ic, N_f)\n",
    "\n",
    "our_layers = np.array([2, 20, 20, 20, 20, 20, 20, 20, 20, 1])  #8 hidden layers\n",
    "pinn = PINN(our_layers,\n",
    "            a=A,\n",
    "            d=D,\n",
    "            _additional_constraints=additional_constraints)\n",
    "save_train_history(pinn)\n",
    "save_model_weights(pinn)\n",
    "# If starting a new training:\n",
    "init_params = pinn.get_weights().numpy()\n",
    "# If need to manually restart the training at some point due to Kernel death:\n",
    "#init_params = np.loadtxt(pinn.history[\"path\"].replace(\"./data/TRAIN_HISTORY\", \"./models/MODEL_WEIGHTS\") + \".txt\")\n",
    "#pinn.set_weights(init_params)\n",
    "#pinn.set_training_history(pinn.history[\"path\"] + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# train the model with Scipy L-BFGS optimizer\n",
    "results = scipy.optimize.minimize(fun=pinn.optimizerfunc,\n",
    "                                  x0=init_params,\n",
    "                                  args=(),\n",
    "                                  method='L-BFGS-B',\n",
    "                                  jac=True, #fun is assumed to return the gradient along with the objective function\n",
    "                                  options={'disp': None,\n",
    "                                           'maxcor': 200,\n",
    "                                           'ftol': 1 * np.finfo(float).eps,\n",
    "                                           #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                           'gtol': 5e-8,\n",
    "                                           'maxfun': 50000,\n",
    "                                           'maxiter': max(maxiter - pinn.get_epoch(), 0),\n",
    "                                           'iprint': -1,  #print update every 50 iterations\n",
    "                                           'maxls': 50})\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.2f' % elapsed)\n",
    "print(results)\n",
    "pinn.set_weights(results.x)\n",
    "save_train_history(pinn)\n",
    "save_model_weights(pinn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model Accuracy '''\n",
    "p_pred = pinn.evaluate(X_p_test)\n",
    "p_pred = np.reshape(p_pred, (x_steps, N_steps), order='F')  # Fortran Style ,stacked column wise!\n",
    "''' Solution Plot '''\n",
    "solutionplot(p_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with numerical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def differenceplot(u_pred, numerical):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "\n",
    "    gs0 = gridspec.GridSpec(2, 3)\n",
    "    gs0.update(top=1, bottom=0, left=0.1, right=2, wspace=0.3, hspace =0.4)\n",
    "    ax = plt.subplot(gs0[0, :])\n",
    "    rel_err = np.absolute(u_pred-numerical)/np.max(numerical)\n",
    "    rel_err[rel_err > 1] = 1\n",
    "    h = ax.imshow(rel_err, interpolation='nearest', cmap='rainbow',\n",
    "                extent=[T.min(), T.max(), X.min(), X.max()],\n",
    "                origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "\n",
    "    #ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4,\n",
    "    # clip_on = False)\n",
    "\n",
    "    #line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    #ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$u(x,t)$', fontsize = 10)\n",
    "\n",
    "    '''\n",
    "    Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n",
    "    '''\n",
    "\n",
    "    ####### Row 1: u(t,x) slices ##################\n",
    "    #gs1 = gridspec.GridSpec(1, 3)\n",
    "    #gs1.update(top=0.3, bottom=-0.1, left=0.1, right=2, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 0])\n",
    "    #ax.plot(x,usol.T[0,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x, u_pred.T[0,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.plot(x, numerical.T[0,:], 'b', linewidth = 2, label = 'Real')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    ax.set_title('$t = 0.s$', fontsize = 10)\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower ,x_upper])\n",
    "    ax.set_ylim([0.0, 0.015])\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 1])\n",
    "    #ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x,u_pred.T[int(0.15*len(t)),:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.plot(x,numerical.T[int(0.15*len(t)),:], 'b', linewidth = 2, label = 'Real')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([0.0, 0.015])\n",
    "    ax.set_title('$t = {}ms$'.format(int(0.5*len(t))/2000), fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 2])\n",
    "    #ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x,u_pred.T[int(0.75*len(t)),:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.plot(x,numerical.T[int(0.75*len(t)),:], 'b', linewidth = 2, label = 'Real')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([0.0, 0.015])\n",
    "    ax.set_title('$t = {}ms$'.format(int(0.75*len(t))/2000), fontsize = 10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig('Ornstein-Uhlenbeck.png',dpi = 500)\n",
    "\n",
    "p_pred = pinn.evaluate(X_p_test)\n",
    "p_pred = np.reshape(p_pred,(x_steps, N_steps), order='F')\n",
    "''' Comparison Plot '''\n",
    "differenceplot(p_pred ,real.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction for large t"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "zbd1nLwqKYJw",
    "outputId": "1e6172fb-f9d1-4a30-c2d8-98159db4f2e7"
   },
   "outputs": [],
   "source": [
    "t_upper = 10 * t_upper\n",
    "t = np.linspace(t_lower, t_upper, N_steps * 10)\n",
    "X, T = np.meshgrid(x, t)\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "p_pred = pinn.evaluate(X_p_test)\n",
    "p_pred = np.reshape(p_pred, (x_steps, N_steps * 10), order='F')\n",
    "solutionplot(p_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_filename = pinn.history[\"path\"] + \".csv\" # PUT PERTINENT TRAIN HISTORY HERE\n",
    "plt.style.use(\"dark_background\")\n",
    "# MSEf + MSEbc + MSEic\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Training Loss\")\n",
    "df = pd.read_csv(data_filename)\n",
    "plt.plot(df[\"epoch\"], df[\"Total loss\"], label=\"Total Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"BC loss\"], label=\"$MSE_{bc}$\")\n",
    "plt.plot(df[\"epoch\"], df[\"f loss\"], label=\"$MSE_f$\")\n",
    "plt.plot(df[\"epoch\"], df[\"IC loss\"], label=\"$MSE_{ic}$\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Norm loss\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Training Norm Loss (not trained for)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"Norm loss\"], label=\"Norm Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Probability loss\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Probability Loss (not trained for)\")\n",
    "plt.plot(df[\"epoch\"], df[\"Pr loss\"], label=\"Probability Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylim((0, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OrnsteinUhlenbeck_lateral_ic.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a6b1c1e0b4cc2c7b94758dc48ca43f4e1f573c11345aa471dd73649661b4f98a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}