{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPPr5oba3j88",
    "outputId": "5cb21a1e-8894-4ebd-e658-2494be28417f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "#hide tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'} \n",
    "#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from pyDOE import lhs  #Latin Hypercube Sampling\n",
    "import seaborn as sns\n",
    "import codecs, json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNpIlNHaRQG-"
   },
   "source": [
    "$\\frac{\\partial p(x,t)}{\\partial t} = \\frac{\\partial}{\\partial x}\\left[\\frac{m\\omega^2}{\\gamma} x \\,p(x,t) + D \\frac{\\partial p(x,t)}{\\partial x}\\right] \\qquad A= \\frac{m\\omega^2}{\\gamma}= \\frac{k}{\\gamma}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88G3Lt8xn-Oo"
   },
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC_Gn7mu3j9B"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy import constants\n",
    "\n",
    "Temperature = 300  # K\n",
    "viscosity = 1e-3  # Kg / (m * s)\n",
    "radius = 20e-10  # m\n",
    "gamma = 6 * np.pi * viscosity * radius  # Kg / s\n",
    "k = 3e-7  # Kg / s^2; 10GHz for frequency and 3e-26 Kg for mass\n",
    "A = k / gamma  # s^-1\n",
    "D = Temperature * constants.k / gamma  # m^2 / s\n",
    "print(f\"{gamma = } [Kg/s]\")\n",
    "print(f\"{k = } [Kg/s^2]\")\n",
    "print(f\"kB = {constants.k} [J/K]\")\n",
    "print(f\"{A = :5.4e} [s^-1],\\t{D = :4.4e} [m^2/s]\")\n",
    "# Change of units m -> µm\n",
    "#                 s -> ms\n",
    "# such that A ~ O(1), D ~ O(0.1)\n",
    "A = A * 1e-3  # s^-1 -> ms^-1\n",
    "D = D * 1e-3 * 1e+12  # m^2/s -> µm^2/ms\n",
    "print(f\"{A = :4.4e} [ms^-1],\\t{D = :4.4e} [µm^2/ms]\")\n",
    "# Collocation points for every position and every time\n",
    "x_lower = -0.1\n",
    "x_upper = 0.25\n",
    "x_steps = 2560\n",
    "dx = (x_upper - x_lower) / x_steps\n",
    "x = np.linspace(x_lower, x_upper, x_steps)  # µm line length\n",
    "t_lower = 0\n",
    "t_upper = 0.69 / A  # Typical Ornstein-Uhlenbeck time is ln(2) / A\n",
    "N_steps = 200\n",
    "t = np.linspace(t_lower, t_upper, N_steps)  # ms time interval\n",
    "X, T = np.meshgrid(x, t)\n",
    "# Initial Condition from imported numerical solution\n",
    "real = pickle.load(open('./simulations/numerical_solution.sav', 'rb'))\n",
    "psol=np.zeros((len(x),len(t)))\n",
    "psol[:,0]=real[0]\n",
    "# Max epochs in training\n",
    "maxiter = 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyGxyaOAcqpi"
   },
   "source": [
    "## *Test Data*\n",
    "We take the numerical solutions of the ```fplanck``` package as the test data to compare against the solution produced\n",
    "by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yddknKA2Xohp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' X_u_test = [X[i],T[i]] [x_steps * N_steps, 2] for interpolation'''\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "\n",
    "# Domain bounds\n",
    "low_bound = np.array([x_lower, t_lower])\n",
    "up_bound = np.array([x_upper, t_upper])\n",
    "\n",
    "'''\n",
    "stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [x_steps * N_steps, 1]\n",
    "'''\n",
    "p = psol.flatten('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UVJmvZbXjXb",
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *Training Data*\n",
    "\n",
    "The boundary and initial conditions serve as the training data for the PINN. We also select the collocation points\n",
    "using *Latin Hypercube Sampling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trainingdata(N_bc, N_ic, N_f):\n",
    "    \"\"\" Initial Condition\"\"\"\n",
    "\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "    all_ic_x = np.vstack((X[0, :], T[0, :])).T\n",
    "    all_ic_p = psol[:, 0].reshape(len(psol[:, 0]), 1)\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.vstack((X[:, 0], T[:, 0])).T\n",
    "    bottomedge_p = psol[-1, :].reshape(len(psol[-1, :]), 1)\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.vstack((X[:, -1], T[:, 0])).T\n",
    "    topedge_p = psol[0, :].reshape(len(psol[0, :]), 1)\n",
    "\n",
    "    all_bc_x = np.vstack([bottomedge_x, topedge_x])\n",
    "    # Reflecting conditions do not use the value of p\n",
    "    #all_bc_p_train = np.vstack([ bottomedge_p, topedge_p])  \n",
    "\n",
    "    #choose random N_bc and N:ic points for training\n",
    "    index_bc = np.random.choice(all_bc_x.shape[0], N_bc, replace=False)\n",
    "    index_ic = np.random.choice(all_ic_x.shape[0], N_ic, replace=False)\n",
    "\n",
    "    x_bc_train = all_bc_x[index_bc, :]\n",
    "    x_ic_train = all_ic_x[index_ic, :]\n",
    "    p_ic_train = all_ic_p[index_ic, :]\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x_f_train = low_bound + (up_bound - low_bound) * lhs(2, N_f)\n",
    "    # Do we select boundary and initial points also for f calculation?\n",
    "    #   Only god knows \n",
    "    #x_f_train = np.vstack((x_f_train, x_bc_train, x_ic_train))\n",
    "\n",
    "    '''Normalization Instants'''\n",
    "    x_norm_instant = np.vstack((X[int(N_steps / 2), :],\n",
    "                                T[int(N_steps / 2), :])).T\n",
    "\n",
    "    return x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instant"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp4nc2S7bwzz"
   },
   "source": [
    "# **PINN implementation**\n",
    "\n",
    "Generate a **PINN** of L hidden layers, each with n neurons. \n",
    "\n",
    "Initialization: ***Xavier***\n",
    "\n",
    "Activation: *tanh (x)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_features = [\"xLower-0.1\", \"xUpper0.25\", \"micrometers_miliseconds\", \"k1e-7\", \"r1e-20\"]\n",
    "print(\"Training with extra features:\", extra_features)\n",
    "additional_constraints = []\n",
    "print(\"Training with extra constraints:\", additional_constraints)\n",
    "\n",
    "\n",
    "def save_train_history(neural_network):\n",
    "    history_filename = \"./data/TRAIN_HISTORY_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            history_filename = history_filename + constrain + \"_\"\n",
    "    history_filename = history_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            history_filename = history_filename + \"_\" + feat\n",
    "    print(\"Saving\", history_filename, \"...\")\n",
    "    neural_network.history[\"path\"] = history_filename\n",
    "    history = neural_network.get_training_history()\n",
    "    pd.DataFrame(history).to_csv(history_filename + \".csv\")\n",
    "\n",
    "\n",
    "def save_model_weights(neural_network):\n",
    "    model_filename = \"./models/MODEL_WEIGHTS_\"\n",
    "    if len(additional_constraints) != 0:\n",
    "        for constrain in additional_constraints:\n",
    "            model_filename = model_filename + constrain + \"_\"\n",
    "    model_filename = model_filename + f\"IC{N_ic}_BC{N_bc}_f{N_f}_t{t_upper}_iter{maxiter}\"\n",
    "    if len(extra_features) != 0:\n",
    "        for feat in extra_features:\n",
    "            model_filename = model_filename + \"_\" + feat\n",
    "    print(\"Saving\", model_filename, \"...\")\n",
    "    np.savetxt(model_filename + \".txt\", neural_network.get_weights().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ivj5SRpG3j9F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(tf.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 low_bound, up_bound,\n",
    "                 x_ic_train, p_ic_train,\n",
    "                 x_bc_train,\n",
    "                 x_f_train,\n",
    "                 x_norm_instants,\n",
    "                 name=\"FP-PINN\",\n",
    "                 A=10,\n",
    "                 D=0.1,\n",
    "                 doPrint=True,\n",
    "                 additional_constraints=()):\n",
    "\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0  #total number of parameters\n",
    "        self.epoch = 0\n",
    "        self.doPrint = doPrint\n",
    "        self.callback_after_n = 1000\n",
    "        self.history = {\"path\": \"\",\n",
    "                        \"epoch\": [],\n",
    "                        \"Total loss\": [],\n",
    "                        \"IC loss\": [],\n",
    "                        \"BC loss\": [],\n",
    "                        \"f loss\": [],\n",
    "                        \"Pr loss\": [],\n",
    "                        \"Norm loss\": [],\n",
    "                        \"Equi loss\": []}\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i + 1]\n",
    "\n",
    "            #Xavier standard deviation \n",
    "            std_dv = np.sqrt((2.0 / (input_dim + output_dim)))\n",
    "\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype='float64') * std_dv\n",
    "\n",
    "            w = tf.Variable(w, trainable=True, name='w' + str(i + 1))\n",
    "\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype='float64'), trainable=True, name='b' + str(i + 1))\n",
    "\n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "\n",
    "            self.parameters += input_dim * output_dim + output_dim\n",
    "\n",
    "    def evaluate(self, x):\n",
    "\n",
    "        x = (x - low_bound) / (up_bound - low_bound)  # Normalization\n",
    "        a = x\n",
    "\n",
    "        for i in range(len(layers) - 2):\n",
    "            z = tf.add(tf.matmul(a, self.W[2 * i]), self.W[2 * i + 1])\n",
    "            a = tf.nn.tanh(z)\n",
    "\n",
    "        a = tf.add(tf.matmul(a, self.W[-2]), self.W[-1])  # For regression, no activation to last layer\n",
    "        return a\n",
    "\n",
    "    def get_weights(self):\n",
    "\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            w_1d = tf.reshape(self.W[2 * i], [-1])  #flatten weights\n",
    "            b_1d = tf.reshape(self.W[2 * i + 1], [-1])  #flatten biases\n",
    "\n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0)  #concat weights\n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0)  #concat biases\n",
    "\n",
    "        return parameters_1d\n",
    "\n",
    "    def set_weights(self, parameters):\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            shape_w = tf.shape(self.W[2 * i]).numpy()  # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2 * i]).numpy()  #size of the weight tensor\n",
    "\n",
    "            shape_b = tf.shape(self.W[2 * i + 1]).numpy()  # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2 * i + 1]).numpy()  #size of the bias tensor\n",
    "\n",
    "            pick_w = parameters[0:size_w]  #pick the weights\n",
    "            self.W[2 * i].assign(tf.reshape(pick_w, shape_w))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_w), 0)  #delete\n",
    "\n",
    "            pick_b = parameters[0:size_b]  #pick the biases\n",
    "            self.W[2 * i + 1].assign(tf.reshape(pick_b, shape_b))  # assign\n",
    "            parameters = np.delete(parameters, np.arange(size_b), 0)  #delete\n",
    "\n",
    "    def set_training_history(self, path):\n",
    "        history = pd.read_csv(path)\n",
    "        self.history[\"epoch\"] = list(history[\"epoch\"])\n",
    "        self.epoch = self.history[\"epoch\"][-1]\n",
    "        self.history[\"Total loss\"] = list(history[\"Total loss\"])\n",
    "        self.history[\"IC loss\"] = list(history[\"IC loss\"])\n",
    "        self.history[\"BC loss\"] = list(history[\"BC loss\"])\n",
    "        self.history[\"f loss\"] = list(history[\"f loss\"])\n",
    "        self.history[\"Pr loss\"] = list(history[\"Pr loss\"])\n",
    "        self.history[\"Norm loss\"] = list(history[\"Norm loss\"])\n",
    "        self.history[\"Equi loss\"] = list(history[\"Equi loss\"])\n",
    "\n",
    "    def get_training_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_epoch(self):\n",
    "        return self.epoch\n",
    "\n",
    "    # Satisfy the IC\n",
    "    def loss_IC(self, ic_points, p_ic_train):\n",
    "        return tf.reduce_mean(tf.square(p_ic_train - self.evaluate(ic_points)))  # MSE_ic\n",
    "\n",
    "    # Reflecting boundary\n",
    "    def loss_BC(self, bc_points):\n",
    "        variable_bc = tf.Variable(bc_points, dtype='float64', trainable=False)\n",
    "\n",
    "        x_bc = variable_bc[:, 0:1]\n",
    "        t_bc = variable_bc[:, 1:2]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_bc)\n",
    "            tape.watch(t_bc)\n",
    "\n",
    "            tensor_bc = tf.stack([x_bc[:, 0], t_bc[:, 0]], axis=1)\n",
    "\n",
    "            output_p_bc = self.evaluate(tensor_bc)\n",
    "        p_x = tape.gradient(output_p_bc, x_bc)  #more efficient out of the context\n",
    "\n",
    "        del tape\n",
    "\n",
    "        flux = -1 * (A * x_bc * output_p_bc + D * p_x)\n",
    "\n",
    "        return tf.reduce_mean(tf.square(flux))  # MSE_bc\n",
    "\n",
    "    # Satisfy the PDE at the collocation points\n",
    "    def loss_PDE(self, collocation_points):\n",
    "\n",
    "        variable_collocation = tf.Variable(collocation_points, dtype='float64', trainable=False)\n",
    "\n",
    "        x_f = variable_collocation[:, 0:1]\n",
    "        t_f = variable_collocation[:, 1:2]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_f)\n",
    "            tape.watch(t_f)\n",
    "\n",
    "            tensor_collocation = tf.stack([x_f[:, 0], t_f[:, 0]], axis=1)\n",
    "\n",
    "            output_p_collocation = self.evaluate(tensor_collocation)\n",
    "            p_x = tape.gradient(output_p_collocation, x_f)  #inside the context bc we need it for higher derivative\n",
    "        p_t = tape.gradient(output_p_collocation, t_f)\n",
    "        p_xx = tape.gradient(p_x, x_f)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        f = p_t - A * output_p_collocation - A * x_f * p_x - D * p_xx\n",
    "\n",
    "        return tf.reduce_mean(tf.square(f))  # MSE_f\n",
    "\n",
    "    # Satisfy probabilty norm at some instants\n",
    "    def loss_NORM(self, instants):\n",
    "        o = self.evaluate(instants)\n",
    "        return tf.abs(tf.reduce_sum(o) * dx - 1.0)  # ME |norm - 1|\n",
    "\n",
    "    # Must be Boltzmann at t >> 1 with ß * m * w ** 2 = A / D\n",
    "    def loss_EQUI(self):\n",
    "        # Typical time is ln(2) / A ≈ 0.69 / A\n",
    "        t_large = 10 * (0.69 / A) * np.ones(256).reshape(256, 1)\n",
    "        x_domain = np.linspace(low_bound[0], up_bound[0], 256).reshape(256, 1)\n",
    "        x_at_large_t = tf.stack([x_domain[:, 0], t_large[:, 0]], axis=1)\n",
    "        output = self.evaluate(x_at_large_t)\n",
    "        boltzmann_dist = tf.exp(-1 * (A / (2 * D)) * x_domain ** 2)\n",
    "        z = tf.reduce_sum(boltzmann_dist) * dx\n",
    "        boltzmann_dist = boltzmann_dist / z\n",
    "        # L2 norm (Boltzmann_dist - output)\n",
    "        return tf.reduce_mean(tf.square(boltzmann_dist - output) * dx)\n",
    "\n",
    "    # Satisfy p > 0 at IC and the collocation points\n",
    "    def loss_PROB(self, ic_points, collocation_points):\n",
    "        o1 = self.evaluate(ic_points)\n",
    "        o2 = self.evaluate(collocation_points)\n",
    "        negatives = tf.where(tf.greater_equal(o1, 0.),\n",
    "                             tf.zeros_like(o1),\n",
    "                             o1)\n",
    "        loss_pr = tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at IC\n",
    "        negatives = tf.where(tf.greater_equal(o2, 0.),\n",
    "                             tf.zeros_like(o2),\n",
    "                             o2)\n",
    "        loss_pr = loss_pr + tf.abs(tf.reduce_mean(negatives))  # MSE (p < 0) at collocation\n",
    "        return loss_pr\n",
    "\n",
    "    def loss(self):\n",
    "        loss_ic = self.loss_IC(x_ic_train, p_ic_train)\n",
    "        loss_bc = self.loss_BC(x_bc_train)\n",
    "        loss_f = self.loss_PDE(x_f_train)\n",
    "        loss_prob = self.loss_PROB(x_ic_train, x_f_train)\n",
    "        loss_norm = self.loss_NORM(x_norm_instants)\n",
    "        loss_equi = self.loss_EQUI()\n",
    "        loss = loss_ic + loss_bc + loss_f\n",
    "        if \"prob\" in additional_constraints:\n",
    "            loss = loss + loss_prob\n",
    "        if \"norm\" in additional_constraints:\n",
    "            loss = loss + loss_norm\n",
    "        if \"equi\" in additional_constraints:\n",
    "            loss = loss + loss_equi\n",
    "        return loss, loss_ic, loss_bc, loss_f, loss_prob, loss_norm, loss_equi\n",
    "\n",
    "    def optimizerfunc(self, parameters):\n",
    "\n",
    "        self.set_weights(parameters)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "\n",
    "            total_loss, loss_ic, loss_bc, loss_f, loss_pr, loss_norm, loss_equi = self.loss()\n",
    "            grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "\n",
    "        self.epoch += 1\n",
    "        if self.doPrint:\n",
    "            tf.print(f\"epoch: {self.epoch}\", f\"- Total: {total_loss:5.4e}\", f\"IC: {loss_ic:5.4e}\",\n",
    "                     f\"BC: {loss_bc:5.4e}\", f\"f: {loss_f:5.4e}\", f\"Norm: {loss_norm:5.4e}\", f\"equi: {loss_equi:5.4e}\")\n",
    "\n",
    "        del tape\n",
    "\n",
    "        grads_1d = []  #flatten grads\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            grads_w_1d = tf.reshape(grads[2 * i], [-1])  #flatten weights\n",
    "            grads_b_1d = tf.reshape(grads[2 * i + 1], [-1])  #flatten biases\n",
    "\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0)  #concat grad_weights\n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0)  #concat grad_biases\n",
    "\n",
    "        self.history[\"epoch\"].append(self.epoch)\n",
    "        self.history[\"Total loss\"].append(float(total_loss))\n",
    "        self.history[\"IC loss\"].append(float(loss_ic))\n",
    "        self.history[\"BC loss\"].append(float(loss_bc))\n",
    "        self.history[\"f loss\"].append(float(loss_f))\n",
    "        self.history[\"Pr loss\"].append(float(loss_pr))\n",
    "        self.history[\"Norm loss\"].append(float(loss_norm))\n",
    "        self.history[\"Equi loss\"].append(float(loss_equi))\n",
    "\n",
    "        return total_loss.numpy(), grads_1d.numpy()\n",
    "\n",
    "    def optimizer_callback(self, parameters):\n",
    "        if self.epoch % self.callback_after_n == 0:\n",
    "            save_train_history(self)\n",
    "            save_model_weights(self)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOjuHdzAhib-"
   },
   "source": [
    "# *Model Training and Testing*\n",
    "\n",
    "A function '**model**' is defined to generate a NN as per the input set of hyperparameters, which is then trained and\n",
    "tested.\n",
    "\n",
    "## Solution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWqNuRMLhg4m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solutionplot(u_pred, X_u_train):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "\n",
    "    gs0 = gridspec.GridSpec(2, 3)\n",
    "    gs0.update(top=1, bottom=0, left=0.1, right=2, wspace=0.3, hspace=0.4)\n",
    "    ax = plt.subplot(gs0[0, :])\n",
    "\n",
    "    h = ax.imshow(u_pred, interpolation='nearest', cmap='rainbow',\n",
    "                  extent=[T.min(), T.max(), X.min(), X.max()],\n",
    "                  origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "\n",
    "    #ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "    line = np.linspace(x.min(), x.max(), 2)[:, None]\n",
    "    #ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc='best')\n",
    "    ax.set_title('$u(x,t)$', fontsize=10)\n",
    "\n",
    "    ''' \n",
    "    Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n",
    "    '''\n",
    "\n",
    "    ####### Row 1: u(t,x) slices ##################\n",
    "    #gs1 = gridspec.GridSpec(1, 3)\n",
    "    #gs1.update(top=0.3, bottom=-0.1, left=0.1, right=2, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 0])\n",
    "    #ax.plot(x,usol.T[0,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x, u_pred.T[0, :], 'r', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    ax.set_title('$t = 0.s$', fontsize=10)\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([-0.1, 9])\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 1])\n",
    "    #ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x, u_pred.T[int(0.5 * len(t)), :], 'r', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([-0.1, 9])\n",
    "    ax.set_title('$t = {}s$'.format(int(0.5 * len(t)) / 100), fontsize=10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 2])\n",
    "    #ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x, u_pred.T[int(0.75 * len(t)), :], 'r', linewidth=2, label='Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([-0.1, 9])\n",
    "    ax.set_title('$t = {}s$'.format(int(0.75 * len(t)) / 100), fontsize=10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig('Ornstein-Uhlenbeck.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRuuEXx-eeWa",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyH2oLRH3j9K",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_bc = 100  #Total number of boundary conditions points\n",
    "N_ic = 100  #Total number of initial condition points\n",
    "N_f = 1000  #Total number of collocation points\n",
    "\n",
    "# Training data\n",
    "x_f_train, x_bc_train, x_ic_train, p_ic_train, x_norm_instants = trainingdata(N_bc, N_ic, N_f)\n",
    "\n",
    "layers = np.array([2, 20, 20, 20, 20, 20, 20, 20, 20, 1])  #8 hidden layers\n",
    "\n",
    "PINN = Sequentialmodel(layers, low_bound, up_bound,\n",
    "                       x_ic_train, p_ic_train,\n",
    "                       x_bc_train,\n",
    "                       x_f_train,\n",
    "                       x_norm_instants,\n",
    "                       A=A,\n",
    "                       D=D)\n",
    "\n",
    "init_params = PINN.get_weights().numpy()\n",
    "#init_params = np.loadtxt(\"./models/MODEL_WEIGHTS_IC100_BC100_f1000_t0.2197371130248822_iter25000_xLower-0.1_xUpper0.25_micrometers_miliseconds_k1e-7_r1e-20.txt\")\n",
    "#PINN.set_weights(init_params)\n",
    "#PINN.set_training_history(\"./data/TRAIN_HISTORY_IC100_BC100_f1000_t0.2197371130248822_iter25000_xLower-0.1_xUpper0.25_micrometers_miliseconds_k1e-7_r1e-20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# train the model with Scipy L-BFGS optimizer\n",
    "results = scipy.optimize.minimize(fun=PINN.optimizerfunc,\n",
    "                                  x0=init_params,\n",
    "                                  args=(),\n",
    "                                  method='L-BFGS-B',\n",
    "                                  jac=True,\n",
    "                                  # If jac is True, fun is assumed to return the gradient along with the objective function\n",
    "                                  callback=PINN.optimizer_callback,\n",
    "                                  options={'disp': None,\n",
    "                                           'maxcor': 200,\n",
    "                                           'ftol': 1 * np.finfo(float).eps,\n",
    "                                           #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                           'gtol': 5e-8,\n",
    "                                           'maxfun': 50000,\n",
    "                                           'maxiter': max(maxiter - PINN.get_epoch(), 0),\n",
    "                                           'iprint': -1,  #print update every 50 iterations\n",
    "                                           'maxls': 50})\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.2f' % elapsed)\n",
    "\n",
    "print(results)\n",
    "\n",
    "PINN.set_weights(results.x)\n",
    "\n",
    "save_train_history(PINN)\n",
    "save_model_weights(PINN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model Accuracy '''\n",
    "\n",
    "p_pred = PINN.evaluate(X_p_test)\n",
    "p_pred = np.reshape(p_pred, (x_steps, N_steps), order='F')  # Fortran Style ,stacked column wise!\n",
    "\n",
    "''' Solution Plot '''\n",
    "solutionplot(p_pred, X_p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with numerical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def differenceplot(u_pred, real):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "\n",
    "    gs0 = gridspec.GridSpec(2, 3)\n",
    "    gs0.update(top=1, bottom=0, left=0.1, right=2, wspace=0.3, hspace =0.4)\n",
    "    ax = plt.subplot(gs0[0, :])\n",
    "    rel_err = np.absolute(u_pred-real)/np.max(real)\n",
    "    rel_err[rel_err > 1] = 1\n",
    "    h = ax.imshow(rel_err, interpolation='nearest', cmap='rainbow',\n",
    "                extent=[T.min(), T.max(), X.min(), X.max()],\n",
    "                origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "\n",
    "    #ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    #ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$u(x,t)$', fontsize = 10)\n",
    "\n",
    "    '''\n",
    "    Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n",
    "    '''\n",
    "\n",
    "    ####### Row 1: u(t,x) slices ##################\n",
    "    #gs1 = gridspec.GridSpec(1, 3)\n",
    "    #gs1.update(top=0.3, bottom=-0.1, left=0.1, right=2, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 0])\n",
    "    #ax.plot(x,usol.T[0,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x, u_pred.T[0,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.plot(x, real.T[0,:], 'b', linewidth = 2, label = 'Real')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    ax.set_title('$t = 0.s$', fontsize = 10)\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower ,x_upper])\n",
    "    ax.set_ylim([0.0, 0.015])\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 1])\n",
    "    #ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x,u_pred.T[int(0.15*len(t)),:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.plot(x,real.T[int(0.15*len(t)),:], 'b', linewidth = 2, label = 'Real')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([0.0, 0.015])\n",
    "    ax.set_title('$t = {}ms$'.format(int(0.5*len(t))/2000), fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 2])\n",
    "    #ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x,u_pred.T[int(0.75*len(t)),:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.plot(x,real.T[int(0.75*len(t)),:], 'b', linewidth = 2, label = 'Real')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    ax.set_ylim([0.0, 0.015])\n",
    "    ax.set_title('$t = {}ms$'.format(int(0.75*len(t))/2000), fontsize = 10)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig('Ornstein-Uhlenbeck.png',dpi = 500)\n",
    "\n",
    "\n",
    "p_pred = PINN.evaluate(X_p_test)\n",
    "p_pred = np.reshape(p_pred,(x_steps, N_steps), order='F')\n",
    "''' Comparison Plot '''\n",
    "differenceplot(p_pred ,real.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction for large t"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "zbd1nLwqKYJw",
    "outputId": "1e6172fb-f9d1-4a30-c2d8-98159db4f2e7"
   },
   "outputs": [],
   "source": [
    "t_upper = 10 * t_upper\n",
    "t = np.linspace(t_lower, t_upper, N_steps * 10)\n",
    "# Collocation points for every position and every time\n",
    "X, T = np.meshgrid(x, t)\n",
    "X_p_test = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "p_pred = PINN.evaluate(X_p_test)\n",
    "p_pred = np.reshape(p_pred, (x_steps, N_steps * 10), order='F')\n",
    "solutionplot(p_pred, X_p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_filename =  PINN.history[\"path\"] # PUT PERTINENT TRAIN HISTORY HERE\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Training Loss\")\n",
    "df = pd.read_csv(data_filename)\n",
    "plt.plot(df[\"epoch\"], df[\"Total loss\"], label=\"Total Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"BC loss\"], label=\"$MSE_{bc}$\")\n",
    "plt.plot(df[\"epoch\"], df[\"f loss\"], label=\"$MSE_f$\")\n",
    "plt.plot(df[\"epoch\"], df[\"IC loss\"], label=\"$MSE_{ic}$\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Training Norm Loss (not trained for)\")\n",
    "#plt.plot(df[\"epoch\"], df[\"Pr loss\"], label = \"Probability Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"Norm loss\"], label=\"Norm Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Probability Loss (not trained for)\")\n",
    "plt.plot(df[\"epoch\"], df[\"Pr loss\"], label=\"Probability Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim((10 ** -14, 10 ** 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OrnsteinUhlenbeck_lateral_ic.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a6b1c1e0b4cc2c7b94758dc48ca43f4e1f573c11345aa471dd73649661b4f98a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}