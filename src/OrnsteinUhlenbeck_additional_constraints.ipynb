{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2630,
     "status": "ok",
     "timestamp": 1652611858676,
     "user": {
      "displayName": "alessio giorlandino",
      "userId": "14669683435891599642"
     },
     "user_tz": -120
    },
    "id": "BGNG42Iq3mZ-",
    "outputId": "5564318e-51ac-47f0-9dd3-7d5003ff5ccd"
   },
   "outputs": [],
   "source": [
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1652611858676,
     "user": {
      "displayName": "alessio giorlandino",
      "userId": "14669683435891599642"
     },
     "user_tz": -120
    },
    "id": "qPPr5oba3j88",
    "outputId": "658042e9-dd78-4208-8c86-44df6f833480"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "#hide tf logs \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'} \n",
    "#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import seaborn as sns \n",
    "import codecs, json\n",
    "import pandas as pd\n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88G3Lt8xn-Oo"
   },
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1652611858677,
     "user": {
      "displayName": "alessio giorlandino",
      "userId": "14669683435891599642"
     },
     "user_tz": -120
    },
    "id": "HC_Gn7mu3j9B"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "#getting collocation points\n",
    "x_init = -1\n",
    "x_final = 1\n",
    "dx = 1 / 128\n",
    "x = np.linspace(x_init, x_final, int((x_final - x_init) / dx))               # 256 points between -1 and 1 [256x1]\n",
    "t_init = 0\n",
    "t_final = 0.2\n",
    "dt = 2e-4\n",
    "t = np.linspace(t_init, t_final, int((t_final - t_init) / dt))    # 100 time points between 0 and 1 [100x1]\n",
    "usol=np.zeros((256,1000))\n",
    "usol[:,0][:]=norm.pdf(x,0.5,0.05).T\n",
    "\n",
    "#collocation points for every position and every time\n",
    "X, T = np.meshgrid(x,t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyGxyaOAcqpi"
   },
   "source": [
    "# *Test Data*\n",
    "\n",
    "We prepare the test data to compare against the solution produced by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1652611858677,
     "user": {
      "displayName": "alessio giorlandino",
      "userId": "14669683435891599642"
     },
     "user_tz": -120
    },
    "id": "yddknKA2Xohp"
   },
   "outputs": [],
   "source": [
    "''' X_u_test = [X[i],T[i]] [25600,2] for interpolation'''\n",
    "X_u_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "''' X_instants'''\n",
    "norm_collocation_instants = (100, 500)\n",
    "dtc = 250\n",
    "X_instants = np.hstack((X.flatten()[:, None],\n",
    "                        T.flatten()[:, None]))\n",
    "#TODO: HOW TO LUMP IN BLOCKS OF 1000.\n",
    "X_instants = X_instants[100000:100999, :]\n",
    "# Domain bounds\n",
    "lb = X_u_test[0]  # [-1. 0.]\n",
    "ub = X_u_test[-1] # [1.  0.99]\n",
    "\n",
    "'''\n",
    "   Fortran Style ('F') flatten,stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [25600x1] \n",
    "'''\n",
    "u = usol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ5oBRtEXnyu"
   },
   "source": [
    "# *Training Data*\n",
    "\n",
    "The boundary conditions serve as the test data for the PINN and the collocation points are generated using **Latin Hypercube Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1652611858677,
     "user": {
      "displayName": "alessio giorlandino",
      "userId": "14669683435891599642"
     },
     "user_tz": -120
    },
    "id": "8UVJmvZbXjXb"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "    leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "    leftedge_u = usol[:,0][:,None]\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "    bottomedge_u = usol[-1,:][:,None]\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "    topedge_u = usol[0,:][:,None]\n",
    "\n",
    "    all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n",
    "    all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) \n",
    "\n",
    "    X_u_train = all_X_u_train[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    X_f_train = lb + (ub-lb)*lhs(2,N_f) \n",
    "    X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
    "\n",
    "    return X_f_train, X_u_train, u_train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp4nc2S7bwzz"
   },
   "source": [
    "# **PINN**\n",
    "\n",
    "Generate a **PINN** of L hidden layers, each with n neurons. \n",
    "\n",
    "Initialization: ***Xavier***\n",
    "\n",
    "Activation: *tanh (x)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1652611859054,
     "user": {
      "displayName": "alessio giorlandino",
      "userId": "14669683435891599642"
     },
     "user_tz": -120
    },
    "id": "Ivj5SRpG3j9F"
   },
   "outputs": [],
   "source": [
    "TRAIN_HISTORY = {\"epoch\": [], \"Total loss\": [], \"BC loss\": [], \"f loss\": [], \"Pr loss\": [], \"Norm loss\": []}\n",
    "class Sequentialmodel(tf.Module): \n",
    "    def __init__(self, layers, name=None):\n",
    "        self.itera = 1\n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0 #total number of parameters\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i+1]\n",
    "            \n",
    "            #Xavier standard deviation \n",
    "            std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
    "\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype = 'float64') * std_dv\n",
    "                       \n",
    "            w = tf.Variable(w, trainable=True, name = 'w' + str(i+1))\n",
    "\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype = 'float64'), trainable = True, name = 'b' + str(i+1))\n",
    "                    \n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "            \n",
    "            self.parameters +=  input_dim * output_dim + output_dim\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        \n",
    "        x = (x-lb)/(ub-lb)\n",
    "        \n",
    "        a = x\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = tf.add(tf.matmul(a, self.W[2*i]), self.W[2*i+1])\n",
    "            a = tf.nn.tanh(z)\n",
    "            \n",
    "        a = tf.add(tf.matmul(a, self.W[-2]), self.W[-1]) # For regression, no activation to last layer\n",
    "        return a\n",
    "    \n",
    "    def get_weights(self):\n",
    "\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "            \n",
    "            w_1d = tf.reshape(self.W[2*i],[-1])   #flatten weights \n",
    "            b_1d = tf.reshape(self.W[2*i+1],[-1]) #flatten biases\n",
    "            \n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0) #concat weights \n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0) #concat biases\n",
    "        \n",
    "        return parameters_1d\n",
    "        \n",
    "    def set_weights(self,parameters):\n",
    "                \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            shape_w = tf.shape(self.W[2*i]).numpy() # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2*i]).numpy() #size of the weight tensor \n",
    "            \n",
    "            shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2*i+1]).numpy() #size of the bias tensor \n",
    "                        \n",
    "            pick_w = parameters[0:size_w] #pick the weights \n",
    "            self.W[2*i].assign(tf.reshape(pick_w,shape_w)) # assign  \n",
    "            parameters = np.delete(parameters,np.arange(size_w),0) #delete \n",
    "            \n",
    "            pick_b = parameters[0:size_b] #pick the biases \n",
    "            self.W[2*i+1].assign(tf.reshape(pick_b,shape_b)) # assign \n",
    "            parameters = np.delete(parameters,np.arange(size_b),0) #delete \n",
    "\n",
    "            \n",
    "    def loss_BC(self,x,y):\n",
    "\n",
    "        loss_u = tf.reduce_mean(tf.square(y-self.evaluate(x)))\n",
    "        return loss_u\n",
    "\n",
    "    def loss_PDE(self, x_to_train_f):\n",
    "    \n",
    "        g = tf.Variable(x_to_train_f, dtype = 'float64', trainable = False)\n",
    "    \n",
    "        cost=10 \n",
    "        sigma2=0.25\n",
    "\n",
    "        x_f = g[:,0:1]\n",
    "        t_f = g[:,1:2]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            tape.watch(x_f)\n",
    "            tape.watch(t_f)\n",
    "\n",
    "            g = tf.stack([x_f[:,0], t_f[:,0]], axis=1)   \n",
    "\n",
    "            z = self.evaluate(g)\n",
    "            p_x = tape.gradient(z,x_f)\n",
    "\n",
    "        p_t = tape.gradient(z,t_f)    \n",
    "        p_xx = tape.gradient(p_x, x_f)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        p=self.evaluate(g)\n",
    "\n",
    "        f = p_t - cost * p - cost * x_f * p_x - sigma2/2*p_xx\n",
    "\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "\n",
    "        return loss_f\n",
    "\n",
    "    def loss_PROB(self,x,g):\n",
    "        o1 = self.evaluate(x)\n",
    "        o2 = self.evaluate(g)\n",
    "        negatives = tf.where(tf.greater_equal(o1, 0.),\n",
    "                             tf.zeros_like(o1),\n",
    "                             o1)\n",
    "        loss_pr = tf.abs(tf.reduce_mean(negatives))\n",
    "        negatives = tf.where(tf.greater_equal(o2, 0.),\n",
    "                             tf.zeros_like(o2),\n",
    "                             o2)\n",
    "        loss_pr = loss_pr + tf.abs(tf.reduce_mean(negatives))\n",
    "        return loss_pr\n",
    "\n",
    "    def loss_NORM(self):\n",
    "        o = self.evaluate(X_instants)\n",
    "        tf.print(tf.abs(tf.reduce_sum(o) * dx))\n",
    "        loss_norm = tf.abs(tf.reduce_sum(o) * dx - 1)\\\n",
    "                    / int((norm_collocation_instants[1] - norm_collocation_instants[0]) / dtc)\n",
    "        return loss_norm\n",
    "\n",
    "    def loss(self,x,y,g):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f = self.loss_PDE(g)\n",
    "        loss_pr = self.loss_PROB(x,g)\n",
    "        loss_norm = self.loss_NORM()\n",
    "\n",
    "        loss = loss_u + loss_f + loss_norm\n",
    "\n",
    "        return loss, loss_u, loss_f, loss_norm\n",
    "    \n",
    "    def optimizerfunc(self,parameters):\n",
    "        \n",
    "        self.set_weights(parameters)\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            \n",
    "            loss_val, loss_u, loss_f, loss_pr = self.loss(X_u_train, u_train, X_f_train)\n",
    "            \n",
    "        grads = tape.gradient(loss_val,self.trainable_variables)\n",
    "                \n",
    "        del tape\n",
    "        \n",
    "        grads_1d = [ ] #flatten grads \n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            grads_w_1d = tf.reshape(grads[2*i],[-1]) #flatten weights \n",
    "            grads_b_1d = tf.reshape(grads[2*i+1],[-1]) #flatten biases\n",
    "\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0) #concat grad_weights \n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0) #concat grad_biases\n",
    "\n",
    "        return loss_val.numpy(), grads_1d.numpy()\n",
    "    \n",
    "    def optimizer_callback(self,parameters):\n",
    "               \n",
    "        loss_value, loss_u, loss_f, loss_norm = self.loss(X_u_train, u_train, X_f_train)\n",
    "        \n",
    "        #u_pred = self.evaluate(X_u_test)\n",
    "        #error_vec = np.linalg.norm((u-u_pred),2)/np.linalg.norm(u,2)\n",
    "        tf.print(\"epoch:\", self.itera, \"Loss:\",  loss_value, \"BC:\", loss_u, \"f:\", loss_f, \"Norm:\", loss_norm)\n",
    "\n",
    "        self.itera += 1\n",
    "        TRAIN_HISTORY[\"epoch\"].append(self.itera)\n",
    "        TRAIN_HISTORY[\"Total loss\"].append(float(loss_value))\n",
    "        TRAIN_HISTORY[\"BC loss\"].append(float(loss_f))\n",
    "        TRAIN_HISTORY[\"f loss\"].append(float(loss_u))\n",
    "        TRAIN_HISTORY[\"Norm loss\"].append(float(loss_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOjuHdzAhib-"
   },
   "source": [
    "# *Solution Plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1652611859054,
     "user": {
      "displayName": "alessio giorlandino",
      "userId": "14669683435891599642"
     },
     "user_tz": -120
    },
    "id": "UWqNuRMLhg4m"
   },
   "outputs": [],
   "source": [
    "def solutionplot(u_pred,X_u_train,u_train):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "\n",
    "    gs0 = gridspec.GridSpec(2, 3)\n",
    "    gs0.update(top=1, bottom=0, left=0.1, right=2, wspace=0.3, hspace =0.4)\n",
    "    ax = plt.subplot(gs0[0, :])\n",
    "\n",
    "    h = ax.imshow(u_pred, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[T.min(), T.max(), X.min(), X.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "    \n",
    "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    #ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    #ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$u(x,t)$', fontsize = 10)\n",
    "    \n",
    "    ''' \n",
    "    Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n",
    "    '''\n",
    "    \n",
    "    ####### Row 1: u(t,x) slices ##################\n",
    "    #gs1 = gridspec.GridSpec(1, 3)\n",
    "    #gs1.update(top=0.3, bottom=-0.1, left=0.1, right=2, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 0])\n",
    "    #ax.plot(x,usol.T[0,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,u_pred.T[0,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(x,t)$')    \n",
    "    ax.set_title('$t = 0.s$', fontsize = 10)\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-0.1,9])\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 1])\n",
    "    #ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,u_pred.T[500,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-0.1,9])\n",
    "    ax.set_title('$t = 0.1s$', fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "    ax = plt.subplot(gs0[1, 2])\n",
    "    #ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,u_pred.T[750,:], 'r', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(x,t)$')\n",
    "    #ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-0.1,9])    \n",
    "    ax.set_title('$t = 0.15s$', fontsize = 10)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig('Ornstein-Uhlenbeck.png',dpi = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRuuEXx-eeWa"
   },
   "source": [
    "# *Model Training and Testing*\n",
    "\n",
    "A function '**model**' is defined to generate a NN as per the input set of hyperparameters, which is then trained and tested. The L2 Norm of the solution error is returned as a comparison metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyH2oLRH3j9K",
    "outputId": "2a6e482f-0246-4bcc-f61a-b2eda8c48a7b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_u = 100 #Total number of data points for 'u'\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "# Training data\n",
    "X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n",
    "\n",
    "layers = np.array([2,20,20,20,20,20,20,20,20,1]) #8 hidden layers\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "\n",
    "init_params = PINN.get_weights().numpy()\n",
    "\n",
    "start_time = time.time() \n",
    "method = 'L-BFGS-B'\n",
    "maxiter = 5000\n",
    "# train the model with Scipy L-BFGS optimizer\n",
    "results = scipy.optimize.minimize(fun = PINN.optimizerfunc, \n",
    "                                  x0 = init_params, \n",
    "                                  args=(), \n",
    "                                  method=method,\n",
    "                                  jac= True,        # If jac is True, fun is assumed to return the gradient along with the objective function\n",
    "                                  callback = PINN.optimizer_callback,\n",
    "                                  options = {'disp': None,\n",
    "                                            'maxcor': 200, \n",
    "                                            'ftol': 1 * np.finfo(float).eps,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                            'gtol': 5e-8,\n",
    "                                            'maxfun':  50000, \n",
    "                                            'maxiter': maxiter,\n",
    "                                            'iprint': -1,   #print update every 50 iterations\n",
    "                                            'maxls': 50})\n",
    "\n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "print(results)\n",
    "\n",
    "PINN.set_weights(results.x)\n",
    "\n",
    "''' Model Accuracy ''' \n",
    "u_pred = PINN.evaluate(X_u_test)\n",
    "\n",
    "error_vec = np.linalg.norm((u-u_pred),2)/np.linalg.norm(u,2)        # Relative L2 Norm of the error (Vector)\n",
    "print('Test Error: %.5f'  % (error_vec))\n",
    "\n",
    "u_pred = np.reshape(u_pred,(256,1000),order='F')                        # Fortran Style ,stacked column wise!\n",
    "\n",
    "''' Solution Plot '''\n",
    "#solutionplot(u_pred,X_u_train,u_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'models/ornstein_uhlenbeck_lateral_ic.sav'\n",
    "pickle.dump(results, open(filename, 'wb')) #dump model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbd1nLwqKYJw"
   },
   "outputs": [],
   "source": [
    "solutionplot(u_pred,X_u_train,u_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SWqKJsg3j9L"
   },
   "source": [
    "# Plot of collocation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Sk4oL6R3j9L"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_f_train, X_u_train, u_train = trainingdata(N_u,N_f)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "plt.plot(X_u_train[:,1], X_u_train[:,0], '*', color = 'red', markersize = 5, label = 'Boundary collocation = 1000')\n",
    "plt.plot(X_f_train[:,1], X_f_train[:,0], 'o', markersize = 0.5, label = 'PDE collocation = 10000')\n",
    "\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('Collocation points')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('collocation_points_Burgers.png', dpi = 500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWlfaghgKJB3"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 256)                     # 256 points between -1 and 1 [256x1]\n",
    "t = np.linspace(0, 0.1, 1000)                     # 100 time points between 0 and 1 [100x1] \n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "X_u_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_pred=PINN.evaluate(X_u_test)\n",
    "u_pred = np.reshape(u_pred,(256,1000),order='F')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOd4NoUaDyjH"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, usol[:,0])\n",
    "plt.plot(x, u_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import pickle\n",
    "filename = 'models/ornstein_uhlenbeck_lateral_ic.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb')) #load model\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "PINN.set_weights(loaded_model.x)\n",
    "u_pred = PINN.evaluate(X_u_test)\n",
    "u_pred = np.reshape(u_pred,(256,1000),order='F')  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inter_pred(time):\n",
    "    time = int(time * 10000)\n",
    "    plt.plot(x, u_pred[:,time])\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(0, 8)\n",
    "    plt.grid()\n",
    "        \n",
    "        \n",
    "ipywidgets.interact(inter_pred, time=(0, T.max() - T.max()/1000, T.max()/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(TRAIN_HISTORY[\"epoch\"], TRAIN_HISTORY[\"Total loss\"])\n",
    "plt.plot(TRAIN_HISTORY[\"epoch\"], TRAIN_HISTORY[\"BC loss\"])\n",
    "plt.plot(TRAIN_HISTORY[\"epoch\"], TRAIN_HISTORY[\"f loss\"])\n",
    "plt.plot(TRAIN_HISTORY[\"epoch\"], TRAIN_HISTORY[\"Pr loss\"])\n",
    "plt.xscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(TRAIN_HISTORY).to_csv(f\"./data/TRAIN_HISTORY_additional_{N_u}_{N_f}_{method}_{maxiter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Training Loss\")\n",
    "df = pd.read_csv(f\"./data/TRAIN_HISTORY_additional_{N_u}_{N_f}_{method}_{maxiter}\")\n",
    "plt.plot(df[\"epoch\"], df[\"Total loss\"], label=\"Total Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"BC loss\"], label=\"$MSE_u$\")\n",
    "plt.plot(df[\"epoch\"], df[\"f loss\"], label=\"$MSE_f$\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Training BC 'Probability' Loss\")\n",
    "plt.plot(df[\"epoch\"], df[\"Pr loss\"], label = \"$$\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OrnsteinUhlenbeck_lateral_ic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}